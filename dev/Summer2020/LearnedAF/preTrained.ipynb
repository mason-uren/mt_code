{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pdb\n",
    "import math\n",
    "\n",
    "def get_random_crop(image, crop_height, crop_width):\n",
    "\n",
    "    max_x = image.shape[1] - crop_width\n",
    "    max_y = image.shape[0] - crop_height\n",
    "\n",
    "    x = np.random.randint(0, max_x)\n",
    "    y = np.random.randint(0, max_y)\n",
    "\n",
    "    crop = image[y: y + crop_height, x: x + crop_width]\n",
    "\n",
    "    return crop\n",
    "\n",
    "def searchForFocus(filename, substring):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read()\n",
    "        location = data.find(substring)\n",
    "        croppedStr = data[location+len(substring):]\n",
    "        # Split at spaces and find first number\n",
    "        for word in croppedStr.split(): # Split at spaces\n",
    "            # Delete any commas    \n",
    "            word = word.replace(',', \"\")\n",
    "            try:\n",
    "                focusPosition = int(word)\n",
    "                return focusPosition\n",
    "            except ValueError:\n",
    "                continue\n",
    "    file.close()\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # ids indicates what subfolders (samples) to access\n",
    "    def __init__(self, foldername, subfolderPrefix, ids):\n",
    "        self.foldername = foldername\n",
    "        self.subfolderPrefix = subfolderPrefix\n",
    "        self.ids = ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sampleFoldername = self.foldername + '/' + self.subfolderPrefix + str(index)\n",
    "        \n",
    "        # Load in image\n",
    "        image = cv2.imread(sampleFoldername + '/before' + str(index) + '.tif', 0)\n",
    "        \n",
    "        # Convert to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Shift it so each channel is from [0,1]\n",
    "        image = image * 1 / 255\n",
    "        \n",
    "        image = get_random_crop(image, 400, 400)\n",
    "\n",
    "        # pyTorch expects C X H X W\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        # Randomly crop to a subset of image (400 X 400) for faster training\n",
    "        #crop = transforms.RandomCrop(400)\n",
    "        #image = crop(image)\n",
    "        \n",
    "        X = torch.from_numpy(image)\n",
    "                \n",
    "        # Normalize according to expect input distribution of resnet\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        \n",
    "        # Randomly flip the image as data augmentation approach, p = 0.5 default\n",
    "        #hFlip = transforms.RandomHorizontalFlip()\n",
    "        #vFlip = transforms.RandomVerticalFlip()\n",
    "        \n",
    "        #totalTransform = transforms.Compose([normalize, crop])\n",
    "        \n",
    "        #X = totalTransform(X)\n",
    "        \n",
    "        X = normalize(X)\n",
    "        \n",
    "        # Get the label\n",
    "        beforeFocus = searchForFocus(sampleFoldername + '/focusInfo.txt', 'before focus: ')\n",
    "        afterFocus = searchForFocus(sampleFoldername + '/focusInfo.txt', 'after focus: ')\n",
    "        \n",
    "        y = afterFocus - beforeFocus\n",
    "        \n",
    "        \n",
    "        # Moved to separate createAF file        \n",
    "#         # Load in image as [0,1] array\n",
    "#         image = cv2.imread(sampleFoldername + '/before' + str(index) + '.tif', 0) * 1 / 255.0\n",
    "        \n",
    "#         # Shift it so is from [-1,1]\n",
    "#         image *= 2\n",
    "#         image -= 1\n",
    "#         X = torch.from_numpy(image)\n",
    "#         X = X.unsqueeze(0) # Add fake first dimension to specify 1-channel\n",
    "\n",
    "#         # Get the label\n",
    "#         beforeFocus = searchForFocus(sampleFoldername + '/focusInfo.txt', 'before focus: ')\n",
    "#         afterFocus = searchForFocus(sampleFoldername + '/focusInfo.txt', 'after focus: ')\n",
    "        \n",
    "#         y = afterFocus - beforeFocus\n",
    "        \n",
    "        # X = torch.load(sampleFoldername + '/X')\n",
    "        # y = torch.load(sampleFoldername + '/y')\n",
    "        \n",
    "        return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 20,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "# Randomly partition the full list into a training set and validation set\n",
    "numSamples = 150 # total number of samples collected\n",
    "frac = 1/5 # fraction to be validation\n",
    "np.random.seed(0)\n",
    "permutedIds = np.random.permutation(range(numSamples))\n",
    "splitPoint = int((1-frac) * len(permutedIds))\n",
    "trainingIds = permutedIds[:splitPoint]\n",
    "valIds = permutedIds[splitPoint:]\n",
    "\n",
    "training_set = Dataset('C:/Users/aofeldman/Desktop/AFdataCollection', 'sample', trainingIds)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset('C:/Users/aofeldman/Desktop/AFdataCollection', 'sample', valIds)\n",
    "#validation_generator = torch.utils.data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = training_set.__getitem__(1)\n",
    "\n",
    "# def imshow(img,wait):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = np.squeeze(img.numpy())\n",
    "#     width = int(0.15 * npimg.shape[1])\n",
    "#     height = int(0.15 * npimg.shape[0])\n",
    "#     cv2.imshow(\"Hi\",cv2.resize(npimg, (width, height)))\n",
    "#     cv2.waitKey(wait)\n",
    "#     cv2.destroyAllWindows()\n",
    "# imshow(X, 1000)\n",
    "# print(X.shape)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/aofeldman/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbb6bf893754d69a673092330c4cac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider placing dropout layers after conv2d layers (conv2d -> batchnorm2d -> leakyReLU -> dropout(p=0.1))\n",
    "# And also place after fully connected layers (linear -> leakyReLU -> dropout(p=0.3))\n",
    "# TODO: Should actually figure out appropriate amount of padding for layers \n",
    "\n",
    "fc = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, 100),\n",
    "    nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "    nn.Linear(100, 10),\n",
    "    nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "    nn.Linear(10, 1)\n",
    ")\n",
    "\n",
    "net = resnet18\n",
    "\n",
    "net.fc = fc\n",
    "\n",
    "# net = nn.Sequential(\n",
    "#     # First, let's try to \"downsample\" by using a large stride with a one-channel convolution\n",
    "#     nn.Conv2d(1, 1, kernel_size=7, stride=3),\n",
    "#     nn.Conv2d(1, 4, kernel_size=5, stride=1),\n",
    "#     nn.BatchNorm2d(4),\n",
    "#     nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "#     nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "#     nn.Conv2d(4, 4, kernel_size=5, stride=1, padding=1),\n",
    "#     nn.BatchNorm2d(4),\n",
    "#     nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "#     nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "#     nn.Conv2d(4, 4, kernel_size=5, stride=1, padding=1),\n",
    "#     nn.BatchNorm2d(4),\n",
    "#     nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "#     nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "#     nn.Conv2d(4, 4, kernel_size=5, stride=1, padding=1),\n",
    "#     nn.BatchNorm2d(4),\n",
    "#     nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "#     nn.BatchNorm2d(4),\n",
    "#     nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(252, 100),\n",
    "#     nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "#     nn.Linear(100, 10),\n",
    "#     nn.LeakyReLU(negative_slope = 0.1, inplace=True),\n",
    "#     nn.Linear(10, 1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64, 64, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64, 64, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64, 64, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64, 64, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([64])\n",
      "Turned gradient off\n",
      "torch.Size([128, 64, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128, 128, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128, 64, 1, 1])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128, 128, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128, 128, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([128])\n",
      "Turned gradient off\n",
      "torch.Size([256, 128, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256, 256, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256, 128, 1, 1])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256, 256, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256, 256, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([256])\n",
      "Turned gradient off\n",
      "torch.Size([512, 256, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512, 512, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512, 256, 1, 1])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512, 512, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512, 512, 3, 3])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([512])\n",
      "Turned gradient off\n",
      "torch.Size([100, 512])\n",
      "Kept gradient turned on\n",
      "torch.Size([100])\n",
      "Kept gradient turned on\n",
      "torch.Size([10, 100])\n",
      "Kept gradient turned on\n",
      "torch.Size([10])\n",
      "Kept gradient turned on\n",
      "torch.Size([1, 10])\n",
      "Kept gradient turned on\n",
      "torch.Size([1])\n",
      "Kept gradient turned on\n"
     ]
    }
   ],
   "source": [
    "fcLength = len([module for module in fc.modules() if type(module) != nn.Sequential])\n",
    "netLength = len([module for module in net.modules() if type(module) != nn.Sequential])\n",
    "\n",
    "layerInd = 0\n",
    "for p in net.parameters():\n",
    "    print(p.data.shape)    \n",
    "    if layerInd < netLength - fcLength:\n",
    "        p.requires_grad = False\n",
    "        print('Turned gradient off')\n",
    "    else:\n",
    "        p.requires_grad = True\n",
    "        print('Kept gradient turned on')\n",
    "    layerInd += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03,  ...,  5.6615e-02,\n",
      "            1.7083e-02, -1.2694e-02],\n",
      "          [ 1.1083e-02,  9.5276e-03, -1.0993e-01,  ..., -2.7124e-01,\n",
      "           -1.2907e-01,  3.7424e-03],\n",
      "          [-6.9434e-03,  5.9089e-02,  2.9548e-01,  ...,  5.1972e-01,\n",
      "            2.5632e-01,  6.3573e-02],\n",
      "          ...,\n",
      "          [-2.7535e-02,  1.6045e-02,  7.2595e-02,  ..., -3.3285e-01,\n",
      "           -4.2058e-01, -2.5781e-01],\n",
      "          [ 3.0613e-02,  4.0960e-02,  6.2850e-02,  ...,  4.1384e-01,\n",
      "            3.9359e-01,  1.6606e-01],\n",
      "          [-1.3736e-02, -3.6746e-03, -2.4084e-02,  ..., -1.5070e-01,\n",
      "           -8.2230e-02, -5.7828e-03]],\n",
      "\n",
      "         [[-1.1397e-02, -2.6619e-02, -3.4641e-02,  ...,  3.2521e-02,\n",
      "            6.6221e-04, -2.5743e-02],\n",
      "          [ 4.5687e-02,  3.3603e-02, -1.0453e-01,  ..., -3.1253e-01,\n",
      "           -1.6051e-01, -1.2826e-03],\n",
      "          [-8.3730e-04,  9.8420e-02,  4.0210e-01,  ...,  7.0789e-01,\n",
      "            3.6887e-01,  1.2455e-01],\n",
      "          ...,\n",
      "          [-5.5926e-02, -5.2239e-03,  2.7081e-02,  ..., -4.6178e-01,\n",
      "           -5.7080e-01, -3.6552e-01],\n",
      "          [ 3.2860e-02,  5.5574e-02,  9.9670e-02,  ...,  5.4636e-01,\n",
      "            4.8276e-01,  1.9867e-01],\n",
      "          [ 5.3051e-03,  6.6938e-03, -1.7254e-02,  ..., -1.4822e-01,\n",
      "           -7.7248e-02,  7.2183e-04]],\n",
      "\n",
      "         [[-2.0315e-03, -9.1617e-03,  2.1209e-02,  ...,  8.9177e-02,\n",
      "            3.3655e-02, -2.0102e-02],\n",
      "          [ 1.5398e-02, -1.8648e-02, -1.2591e-01,  ..., -2.5342e-01,\n",
      "           -1.2980e-01, -2.7975e-02],\n",
      "          [ 9.8454e-03,  4.9047e-02,  2.1699e-01,  ...,  3.4872e-01,\n",
      "            1.0433e-01,  1.8413e-02],\n",
      "          ...,\n",
      "          [-2.8356e-02,  1.8404e-02,  9.8647e-02,  ..., -1.1740e-01,\n",
      "           -2.5760e-01, -1.5451e-01],\n",
      "          [ 2.0766e-02, -2.6286e-03, -3.7825e-02,  ...,  2.4141e-01,\n",
      "            2.4345e-01,  1.1796e-01],\n",
      "          [ 7.4684e-04,  7.7677e-04, -1.0050e-02,  ..., -1.4865e-01,\n",
      "           -1.1754e-01, -3.8350e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.4154e-03, -4.0645e-03,  3.1589e-03,  ..., -3.7026e-02,\n",
      "           -2.5158e-02, -4.7945e-02],\n",
      "          [ 5.1310e-02,  5.3402e-02,  8.0436e-02,  ...,  1.4480e-01,\n",
      "            1.4287e-01,  1.2312e-01],\n",
      "          [-7.3337e-03,  2.1755e-03,  3.7580e-02,  ...,  6.1517e-02,\n",
      "            8.0324e-02,  1.1715e-01],\n",
      "          ...,\n",
      "          [-2.6754e-02, -1.2297e-01, -1.3653e-01,  ..., -1.4068e-01,\n",
      "           -1.1155e-01, -4.9556e-02],\n",
      "          [ 2.3524e-02, -1.7288e-02, -1.1122e-02,  ..., -1.8826e-02,\n",
      "           -2.3320e-02, -2.9474e-02],\n",
      "          [ 2.8689e-02,  2.1659e-02,  4.7888e-02,  ...,  2.5498e-02,\n",
      "            3.5346e-02,  1.1280e-02]],\n",
      "\n",
      "         [[ 4.6919e-04,  1.2153e-02,  4.2035e-02,  ...,  4.6403e-02,\n",
      "            4.0423e-02, -1.4439e-02],\n",
      "          [ 4.3463e-02,  6.8779e-02,  1.3268e-01,  ...,  2.8606e-01,\n",
      "            2.6905e-01,  2.0935e-01],\n",
      "          [-5.7621e-02, -2.2642e-02,  3.0547e-02,  ...,  1.3763e-01,\n",
      "            1.6538e-01,  1.7946e-01],\n",
      "          ...,\n",
      "          [-1.0816e-01, -2.5227e-01, -2.9742e-01,  ..., -2.8503e-01,\n",
      "           -2.1493e-01, -1.0320e-01],\n",
      "          [ 4.0709e-02, -3.2771e-02, -6.3450e-02,  ..., -9.2360e-02,\n",
      "           -6.9876e-02, -4.9841e-02],\n",
      "          [ 8.2942e-02,  8.7580e-02,  1.0111e-01,  ...,  5.2714e-02,\n",
      "            6.0968e-02,  4.1198e-02]],\n",
      "\n",
      "         [[-1.6391e-02, -1.3870e-02,  5.2810e-03,  ...,  4.3698e-02,\n",
      "            2.2707e-02, -4.5983e-02],\n",
      "          [ 3.3202e-02,  4.2014e-02,  9.3500e-02,  ...,  2.6162e-01,\n",
      "            2.2970e-01,  1.6694e-01],\n",
      "          [-4.5987e-02, -1.6365e-02,  2.6811e-02,  ...,  1.4951e-01,\n",
      "            1.3216e-01,  1.3579e-01],\n",
      "          ...,\n",
      "          [-7.2129e-02, -1.8902e-01, -2.3389e-01,  ..., -1.9038e-01,\n",
      "           -1.5609e-01, -7.5974e-02],\n",
      "          [ 5.1161e-02, -2.5815e-02, -6.9357e-02,  ..., -5.8999e-02,\n",
      "           -6.1550e-02, -4.4555e-02],\n",
      "          [ 1.1174e-01,  7.8979e-02,  6.5849e-02,  ...,  3.1617e-02,\n",
      "            2.5221e-02,  7.4257e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.0826e-08, -6.4306e-08, -7.3806e-08,  ..., -9.8000e-08,\n",
      "           -1.0905e-07, -8.3421e-08],\n",
      "          [-6.1125e-09,  2.0613e-09, -8.0922e-09,  ..., -4.9840e-08,\n",
      "           -4.3836e-08, -3.0538e-09],\n",
      "          [ 7.1953e-08,  7.5616e-08,  5.9282e-08,  ..., -9.7509e-09,\n",
      "           -1.0951e-09,  4.2442e-08],\n",
      "          ...,\n",
      "          [ 9.5889e-08,  1.0039e-07,  7.9817e-08,  ..., -1.7491e-08,\n",
      "           -4.7666e-08, -1.3265e-08],\n",
      "          [ 1.2904e-07,  1.4762e-07,  1.7477e-07,  ...,  1.3233e-07,\n",
      "            1.0628e-07,  9.3316e-08],\n",
      "          [ 1.2558e-07,  1.3644e-07,  1.8431e-07,  ...,  2.1399e-07,\n",
      "            1.7710e-07,  1.7166e-07]],\n",
      "\n",
      "         [[-1.2690e-07, -9.6139e-08, -1.0372e-07,  ..., -1.1808e-07,\n",
      "           -1.3309e-07, -1.0820e-07],\n",
      "          [-5.7412e-08, -2.5055e-08, -3.0115e-08,  ..., -7.2922e-08,\n",
      "           -6.7022e-08, -2.2574e-08],\n",
      "          [ 2.1813e-08,  4.8608e-08,  3.1222e-08,  ..., -1.8694e-08,\n",
      "           -7.9591e-09,  3.9750e-08],\n",
      "          ...,\n",
      "          [ 5.6013e-08,  7.5526e-08,  4.4496e-08,  ..., -4.4128e-08,\n",
      "           -5.9930e-08, -1.8247e-08],\n",
      "          [ 7.7614e-08,  9.8348e-08,  1.0455e-07,  ...,  6.3272e-08,\n",
      "            4.1781e-08,  4.5901e-08],\n",
      "          [ 5.9834e-08,  7.1006e-08,  9.0437e-08,  ...,  1.1654e-07,\n",
      "            8.7550e-08,  9.8837e-08]],\n",
      "\n",
      "         [[-4.3810e-08,  1.3270e-08,  7.8275e-09,  ..., -5.8804e-09,\n",
      "           -2.6217e-08, -1.5649e-08],\n",
      "          [ 4.1700e-08,  1.0778e-07,  1.0946e-07,  ...,  7.6403e-08,\n",
      "            7.1450e-08,  9.7615e-08],\n",
      "          [ 1.0436e-07,  1.6586e-07,  1.5933e-07,  ...,  1.3517e-07,\n",
      "            1.3487e-07,  1.6449e-07],\n",
      "          ...,\n",
      "          [ 9.8763e-08,  1.5072e-07,  1.2547e-07,  ...,  6.8316e-08,\n",
      "            6.8382e-08,  1.1367e-07],\n",
      "          [ 9.1435e-08,  1.3576e-07,  1.3793e-07,  ...,  1.1678e-07,\n",
      "            1.1723e-07,  1.4394e-07],\n",
      "          [ 6.2183e-08,  8.8184e-08,  1.0456e-07,  ...,  1.3941e-07,\n",
      "            1.3333e-07,  1.5844e-07]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.1896e-02, -3.0206e-02,  1.9225e-02,  ...,  4.3665e-02,\n",
      "           -2.2114e-02, -4.2214e-02],\n",
      "          [-3.8061e-02,  6.0774e-03,  4.5797e-02,  ...,  9.6029e-02,\n",
      "            5.9254e-02,  2.9958e-02],\n",
      "          [-2.9672e-02,  2.7766e-03,  2.0457e-02,  ...,  5.9828e-02,\n",
      "            4.1422e-02,  2.3134e-02],\n",
      "          ...,\n",
      "          [ 1.1916e-02,  4.5701e-02,  4.4892e-02,  ...,  4.7419e-02,\n",
      "            2.2274e-02, -5.4993e-03],\n",
      "          [-3.2468e-02, -1.2210e-02,  2.2023e-02,  ...,  5.8061e-02,\n",
      "           -7.5033e-03, -5.9736e-02],\n",
      "          [-4.3314e-02, -2.8162e-02, -5.9126e-03,  ...,  8.8460e-02,\n",
      "            8.4406e-03, -5.0019e-02]],\n",
      "\n",
      "         [[-6.1292e-02, -1.4004e-02,  1.7229e-02,  ...,  1.8349e-02,\n",
      "           -3.2708e-02, -4.1060e-02],\n",
      "          [-3.1506e-02,  2.4460e-02,  4.5516e-02,  ...,  6.6806e-02,\n",
      "            4.6687e-02,  3.3248e-02],\n",
      "          [-3.2216e-02,  2.0718e-02,  2.3343e-02,  ...,  3.5265e-02,\n",
      "            3.6478e-02,  3.1291e-02],\n",
      "          ...,\n",
      "          [ 1.7739e-02,  6.1040e-02,  4.8247e-02,  ...,  3.7785e-02,\n",
      "            2.8894e-02,  1.3984e-02],\n",
      "          [-1.0890e-02,  2.2079e-02,  4.2737e-02,  ...,  6.0247e-02,\n",
      "            1.6197e-02, -1.2493e-02],\n",
      "          [-2.2284e-02,  1.3220e-02,  3.0897e-02,  ...,  1.0403e-01,\n",
      "            4.0119e-02, -5.3310e-03]],\n",
      "\n",
      "         [[-8.5322e-02, -4.2603e-02,  6.8145e-03,  ...,  3.0751e-02,\n",
      "           -3.4818e-02, -4.9945e-02],\n",
      "          [-2.9215e-02,  1.8165e-02,  5.1092e-02,  ...,  9.0200e-02,\n",
      "            5.3438e-02,  4.0169e-02],\n",
      "          [-3.9932e-02, -1.1100e-03,  9.6176e-03,  ...,  2.4114e-02,\n",
      "            2.6298e-02,  2.5489e-02],\n",
      "          ...,\n",
      "          [-3.1890e-03,  3.0454e-02,  1.6316e-02,  ...,  5.5054e-03,\n",
      "           -6.2689e-03, -8.4638e-03],\n",
      "          [-2.2995e-02, -2.8211e-03,  2.3203e-02,  ...,  3.5888e-02,\n",
      "           -1.4296e-02, -3.2419e-02],\n",
      "          [-9.8894e-03,  7.0542e-03,  1.0659e-02,  ...,  7.0495e-02,\n",
      "            1.2996e-02, -8.3417e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.8699e-03,  1.9911e-02,  3.4208e-02,  ...,  2.8694e-02,\n",
      "            1.2820e-02,  1.8142e-02],\n",
      "          [ 8.7942e-03, -3.2875e-02, -3.5713e-02,  ...,  7.2533e-02,\n",
      "            4.5889e-02,  5.2383e-02],\n",
      "          [-3.6122e-02, -1.1878e-01, -1.3767e-01,  ...,  3.3811e-02,\n",
      "            3.7806e-02,  2.6944e-02],\n",
      "          ...,\n",
      "          [ 1.7322e-02,  3.9589e-03, -8.2269e-03,  ...,  2.7543e-03,\n",
      "            1.8313e-02,  1.6057e-02],\n",
      "          [-9.5007e-04,  1.6428e-02,  1.7156e-02,  ...,  3.3672e-03,\n",
      "            2.2857e-02,  6.5783e-04],\n",
      "          [ 6.1727e-03,  2.7145e-02,  1.4340e-02,  ...,  7.5867e-03,\n",
      "            1.8770e-02,  1.5624e-02]],\n",
      "\n",
      "         [[-1.3423e-02, -5.0696e-04,  8.0959e-03,  ..., -6.0963e-03,\n",
      "            9.2341e-03,  1.5751e-02],\n",
      "          [-1.8343e-02, -6.7982e-02, -7.0685e-02,  ...,  2.9855e-02,\n",
      "            2.6264e-02,  2.3773e-02],\n",
      "          [-5.4359e-02, -1.4663e-01, -1.6211e-01,  ...,  1.1781e-02,\n",
      "            3.2477e-02,  1.1980e-02],\n",
      "          ...,\n",
      "          [ 8.3686e-04, -1.7564e-02, -1.9535e-02,  ..., -4.1382e-03,\n",
      "            2.4658e-02,  1.2893e-02],\n",
      "          [-6.3183e-04,  1.1788e-02,  2.4810e-02,  ...,  6.1105e-03,\n",
      "            3.9210e-02,  9.6696e-03],\n",
      "          [-7.1831e-03,  6.6918e-03,  5.2723e-03,  ..., -7.6077e-03,\n",
      "            2.7253e-02,  1.7735e-02]],\n",
      "\n",
      "         [[-2.3753e-04, -4.9343e-03,  2.2991e-03,  ..., -4.7958e-02,\n",
      "           -2.6154e-02, -2.3525e-02],\n",
      "          [-3.3053e-04, -5.1502e-02, -5.9977e-02,  ..., -1.7369e-02,\n",
      "           -2.3337e-02, -3.7312e-02],\n",
      "          [-2.2674e-02, -9.9412e-02, -1.1176e-01,  ..., -1.1725e-02,\n",
      "           -8.3744e-03, -4.0615e-02],\n",
      "          ...,\n",
      "          [ 1.1437e-02, -8.0313e-03, -1.4955e-03,  ..., -3.4133e-02,\n",
      "           -8.7267e-03, -2.3526e-02],\n",
      "          [ 2.9522e-03,  6.7770e-04,  1.9933e-02,  ..., -2.2002e-02,\n",
      "            1.4814e-02, -1.4487e-02],\n",
      "          [-1.9085e-02, -2.9430e-02, -2.3284e-02,  ..., -4.8587e-02,\n",
      "           -1.3049e-02, -2.4368e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.6296e-02,  7.1996e-03,  1.9100e-02,  ...,  1.9602e-02,\n",
      "            1.4870e-02, -1.7298e-02],\n",
      "          [-1.1061e-02,  8.5665e-02,  1.2667e-01,  ...,  1.3744e-02,\n",
      "           -5.5036e-05, -3.0162e-02],\n",
      "          [ 1.1322e-01,  1.8634e-01,  5.0658e-02,  ..., -1.7333e-01,\n",
      "           -7.2041e-02, -6.2474e-02],\n",
      "          ...,\n",
      "          [-5.3062e-02, -2.5781e-01, -2.6747e-01,  ...,  2.6781e-01,\n",
      "            1.4344e-01,  5.5145e-02],\n",
      "          [-2.1009e-02, -2.9969e-02,  1.0245e-01,  ...,  2.0843e-01,\n",
      "           -4.1518e-03, -3.8118e-02],\n",
      "          [-2.2155e-02,  1.2380e-02,  8.4302e-02,  ..., -4.4992e-02,\n",
      "           -1.4687e-01, -9.0890e-02]],\n",
      "\n",
      "         [[-5.3969e-03,  3.2799e-02,  1.5486e-02,  ..., -7.7451e-03,\n",
      "            3.0229e-03,  1.1216e-03],\n",
      "          [ 6.1723e-02,  1.4899e-01,  1.4645e-01,  ..., -2.8897e-02,\n",
      "           -2.0227e-02, -9.1878e-03],\n",
      "          [ 1.6146e-01,  2.0886e-01, -2.5589e-02,  ..., -2.7278e-01,\n",
      "           -1.0735e-01, -6.2971e-02],\n",
      "          ...,\n",
      "          [-1.3723e-01, -4.0863e-01, -3.8551e-01,  ...,  4.0846e-01,\n",
      "            2.6202e-01,  1.3491e-01],\n",
      "          [-5.9388e-02, -6.1187e-02,  1.4197e-01,  ...,  3.5780e-01,\n",
      "            9.0893e-02, -1.7392e-03],\n",
      "          [ 7.8613e-03,  5.8403e-02,  1.5339e-01,  ...,  4.7045e-02,\n",
      "           -1.0095e-01, -9.7920e-02]],\n",
      "\n",
      "         [[-5.6799e-03,  1.3425e-02, -2.6461e-02,  ...,  4.4881e-03,\n",
      "            2.0666e-03,  1.3902e-02],\n",
      "          [ 6.5943e-03,  4.5181e-02,  6.0260e-02,  ...,  1.4368e-02,\n",
      "           -5.0725e-03,  4.0505e-03],\n",
      "          [ 5.5257e-02,  1.2397e-01,  4.3193e-02,  ..., -1.4486e-01,\n",
      "           -7.4489e-02, -5.7533e-02],\n",
      "          ...,\n",
      "          [-3.1513e-02, -1.6334e-01, -1.5795e-01,  ...,  2.2904e-01,\n",
      "            1.2017e-01,  7.1998e-02],\n",
      "          [-1.0456e-02, -1.1248e-03,  8.4582e-02,  ...,  1.5748e-01,\n",
      "            2.2142e-02, -1.0083e-02],\n",
      "          [-4.8639e-03, -5.0065e-03,  3.6341e-02,  ..., -2.4361e-02,\n",
      "           -7.1195e-02, -6.6788e-02]]]])\n",
      "tensor([ 2.3487e-01,  2.6626e-01, -5.1096e-08,  5.1870e-01,  3.4404e-09,\n",
      "         2.2239e-01,  4.2289e-01,  1.3153e-07,  2.5093e-01,  1.5152e-06,\n",
      "         3.1687e-01,  2.5049e-01,  3.7893e-01,  1.0862e-05,  2.7526e-01,\n",
      "         2.3674e-01,  2.4202e-01,  3.9531e-01,  4.6935e-01,  2.9090e-01,\n",
      "         2.7268e-01,  2.7803e-01,  2.9069e-01,  2.0693e-01,  2.5899e-01,\n",
      "         2.7871e-01,  2.9115e-01,  3.1601e-01,  3.8889e-01,  3.0411e-01,\n",
      "         2.6776e-01,  2.1093e-01,  2.8708e-01,  3.3243e-01,  4.2673e-01,\n",
      "         3.7326e-01,  7.4804e-08,  1.9068e-01,  1.4740e-08,  2.2303e-01,\n",
      "         1.7908e-01,  2.4860e-01,  2.7400e-01,  2.5923e-01,  2.9420e-01,\n",
      "         2.9924e-01,  2.2369e-01,  2.6280e-01,  2.2001e-08,  2.6610e-01,\n",
      "         2.2089e-01,  2.8429e-01,  3.3072e-01,  2.2681e-01,  3.6538e-01,\n",
      "         2.1230e-01,  2.3965e-01,  2.4950e-01,  5.2583e-01,  2.4825e-01,\n",
      "         2.9565e-01,  2.5878e-01,  4.8326e-01,  2.6670e-01])\n",
      "tensor([ 2.3072e-01,  2.5382e-01, -1.0543e-06, -6.6439e-01, -1.6571e-08,\n",
      "         1.6152e-01,  4.5450e-01, -4.3020e-07,  3.0051e-01, -8.0052e-06,\n",
      "         3.4942e-01,  3.1148e-01, -2.4953e-01, -3.4749e-05,  1.0773e-01,\n",
      "         2.1897e-01,  3.8141e-01, -5.2988e-01, -6.2864e-01,  5.7140e-01,\n",
      "         2.9985e-01,  5.8430e-01,  4.8202e-01,  3.2853e-01,  1.9672e-01,\n",
      "         1.9496e-01,  1.5215e-01,  8.5522e-02,  5.1314e-01,  1.5237e-02,\n",
      "         1.6644e-01,  3.3239e-01,  2.4921e-01,  4.4337e-01, -2.8017e-01,\n",
      "        -2.0385e-02, -2.4507e-07,  3.2134e-01, -4.9152e-08,  2.3777e-01,\n",
      "         2.3291e-01,  3.1527e-01,  4.2776e-01,  2.9313e-01,  2.6379e-01,\n",
      "         6.7598e-01,  4.2910e-01,  3.4566e-01, -8.6909e-08,  2.4729e-01,\n",
      "         3.0316e-01,  6.1577e-01,  3.9835e-01,  3.3207e-01, -4.1219e-01,\n",
      "         3.7807e-01,  1.7895e-01,  2.5748e-01, -4.4908e-01,  2.1306e-01,\n",
      "         5.6934e-01,  5.7274e-01, -4.0238e-01,  2.3406e-01])\n",
      "tensor([[[[ 5.7593e-02, -9.5114e-02, -2.0272e-02],\n",
      "          [-7.4556e-02, -7.9931e-01, -2.1284e-01],\n",
      "          [ 6.5571e-02, -9.6534e-02, -1.2111e-02]],\n",
      "\n",
      "         [[-6.9944e-03,  1.4266e-02,  5.5824e-04],\n",
      "          [ 4.1238e-02, -1.6125e-01, -2.3208e-02],\n",
      "          [ 3.2887e-03,  7.1779e-03,  7.1686e-02]],\n",
      "\n",
      "         [[-2.3627e-09, -3.9270e-08, -3.2971e-08],\n",
      "          [ 2.1737e-08,  8.3299e-09,  1.2543e-08],\n",
      "          [ 1.1382e-08,  8.8096e-09,  1.5506e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6921e-02,  1.8294e-02, -2.9358e-02],\n",
      "          [-9.8615e-02, -4.3645e-02, -5.2717e-02],\n",
      "          [-7.9635e-02,  2.9396e-02,  4.1479e-03]],\n",
      "\n",
      "         [[ 1.6948e-02,  1.3978e-02,  9.6727e-03],\n",
      "          [ 1.4297e-02, -6.6985e-04, -2.2077e-02],\n",
      "          [ 1.2398e-02,  3.5454e-02, -2.2320e-02]],\n",
      "\n",
      "         [[-2.2600e-02, -2.5331e-02, -2.3548e-02],\n",
      "          [ 6.0860e-02, -9.6779e-02,  2.4057e-02],\n",
      "          [-1.2750e-02,  9.2237e-02,  4.0152e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2160e-02,  4.2177e-02, -1.6428e-02],\n",
      "          [-2.9667e-02,  5.6865e-02,  2.5486e-02],\n",
      "          [ 4.3847e-03,  5.1188e-02,  1.0436e-02]],\n",
      "\n",
      "         [[ 2.5342e-02,  5.4374e-02,  5.3888e-02],\n",
      "          [-2.8334e-02, -2.0139e-01, -5.6358e-02],\n",
      "          [ 5.6774e-02,  7.4188e-02,  2.1585e-02]],\n",
      "\n",
      "         [[-3.1458e-08,  3.5335e-08,  5.3791e-08],\n",
      "          [-2.6896e-08,  5.1530e-08,  5.4480e-08],\n",
      "          [-3.8487e-08, -1.1234e-08, -7.5787e-09]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2754e-01,  4.3552e-02, -6.5607e-02],\n",
      "          [-6.0462e-02,  1.5989e-01, -7.7070e-03],\n",
      "          [-9.4202e-02,  5.0750e-02, -7.8154e-02]],\n",
      "\n",
      "         [[-3.3309e-02,  1.6631e-03, -8.8497e-03],\n",
      "          [ 1.5553e-02, -5.8277e-02, -2.7437e-02],\n",
      "          [ 1.3126e-02, -3.0268e-02, -2.1661e-03]],\n",
      "\n",
      "         [[-4.2313e-03,  3.4517e-02,  3.8193e-03],\n",
      "          [ 5.4317e-02, -1.2457e-02,  3.2900e-02],\n",
      "          [ 2.2000e-04,  1.6040e-02,  1.2764e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.5247e-02,  8.0748e-03,  2.0353e-02],\n",
      "          [ 1.7344e-02, -2.4320e-02, -1.5511e-04],\n",
      "          [-2.7634e-04,  2.8024e-02, -2.3777e-03]],\n",
      "\n",
      "         [[-2.3741e-02, -3.2057e-03, -5.7059e-03],\n",
      "          [-1.1582e-02,  1.7200e-03,  2.1067e-02],\n",
      "          [ 4.3606e-03, -4.6459e-02, -7.2954e-02]],\n",
      "\n",
      "         [[ 3.1002e-08,  5.3568e-08,  3.1873e-08],\n",
      "          [-1.6063e-08, -1.8072e-08, -1.9508e-09],\n",
      "          [-5.8339e-08, -4.5366e-08, -1.2395e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9689e-03, -2.6809e-02, -4.3760e-02],\n",
      "          [ 2.4518e-02, -2.8396e-02, -3.5896e-02],\n",
      "          [-1.7883e-04, -2.4661e-02, -2.0085e-02]],\n",
      "\n",
      "         [[ 2.1551e-02,  2.2789e-03, -2.5823e-02],\n",
      "          [ 2.3272e-02, -7.9333e-03, -2.0814e-03],\n",
      "          [-5.7062e-03, -2.6934e-02, -1.4421e-02]],\n",
      "\n",
      "         [[-1.9674e-02,  2.7914e-02, -2.0025e-02],\n",
      "          [ 6.3222e-02, -3.9077e-02, -3.3220e-03],\n",
      "          [-2.7434e-02,  1.1390e-02, -3.1608e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.3440e-03, -7.6970e-03, -6.4950e-02],\n",
      "          [ 1.3846e-02, -2.2803e-02, -4.6478e-02],\n",
      "          [ 2.7776e-02,  1.6080e-02, -1.3363e-02]],\n",
      "\n",
      "         [[ 4.7379e-02, -2.4982e-02, -2.7605e-02],\n",
      "          [ 7.0091e-02,  4.2084e-03, -1.0805e-01],\n",
      "          [ 1.7526e-02,  4.5647e-02,  7.8810e-03]],\n",
      "\n",
      "         [[ 2.6680e-09,  2.7671e-08,  2.4702e-08],\n",
      "          [ 6.3905e-09,  4.1020e-08,  3.3631e-08],\n",
      "          [ 5.8335e-09,  1.3334e-08,  9.6604e-09]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.5900e-03,  4.7084e-02, -8.6949e-03],\n",
      "          [-6.3011e-03,  5.9585e-02,  5.8667e-03],\n",
      "          [-2.0255e-02,  4.3285e-02,  4.5094e-03]],\n",
      "\n",
      "         [[ 1.1253e-03, -5.7461e-03, -6.8411e-03],\n",
      "          [ 6.0616e-03,  7.3295e-03, -1.1784e-02],\n",
      "          [-1.1455e-03,  5.1868e-03, -1.9867e-02]],\n",
      "\n",
      "         [[ 1.7529e-02,  4.4606e-02, -2.6595e-02],\n",
      "          [ 2.2102e-02,  4.5857e-02,  2.3347e-02],\n",
      "          [ 1.8052e-02,  5.9689e-02,  1.7129e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.9112e-02,  3.4242e-03, -1.7523e-02],\n",
      "          [-2.3682e-02,  2.2716e-02, -3.8301e-02],\n",
      "          [-1.0308e-02, -4.3802e-03, -2.3582e-02]],\n",
      "\n",
      "         [[-4.9607e-02, -3.2724e-03, -1.5345e-02],\n",
      "          [-1.3524e-02,  5.4842e-02,  1.1187e-02],\n",
      "          [-2.3549e-02, -2.8495e-02, -6.6371e-02]],\n",
      "\n",
      "         [[-4.9804e-08, -2.8211e-08, -2.0583e-08],\n",
      "          [-5.2389e-08, -2.8522e-08, -3.5099e-08],\n",
      "          [-3.2171e-08, -3.4110e-08, -4.3153e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4487e-03,  2.6532e-02, -1.1202e-02],\n",
      "          [ 7.0925e-03,  3.7903e-02, -3.2481e-02],\n",
      "          [ 4.1381e-02,  3.2329e-02,  2.8309e-03]],\n",
      "\n",
      "         [[-6.5955e-03,  1.6476e-02,  2.1810e-02],\n",
      "          [-1.2293e-02,  2.2310e-02,  1.2645e-02],\n",
      "          [-8.9897e-03,  1.1948e-03, -5.2390e-03]],\n",
      "\n",
      "         [[-2.5295e-03,  7.2689e-02, -7.8046e-03],\n",
      "          [-4.2221e-02,  7.9756e-02, -2.7738e-02],\n",
      "          [ 4.6716e-03, -5.6596e-02, -8.2261e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2235e-02,  3.5231e-03, -3.3131e-02],\n",
      "          [ 3.1048e-02,  1.6193e-02,  1.7283e-02],\n",
      "          [ 1.4446e-02,  2.4302e-02, -1.9689e-03]],\n",
      "\n",
      "         [[-2.4717e-02,  8.3009e-03, -6.1336e-02],\n",
      "          [-1.6134e-02,  5.5323e-02, -6.5029e-02],\n",
      "          [-2.4715e-02,  1.0030e-03,  3.2437e-02]],\n",
      "\n",
      "         [[ 1.8496e-08,  5.2798e-09,  4.1820e-08],\n",
      "          [ 3.7489e-08,  2.5450e-08,  3.0419e-08],\n",
      "          [ 1.1246e-08, -5.6956e-09, -2.0008e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.1194e-03, -4.1052e-02, -1.0002e-02],\n",
      "          [ 2.5924e-02, -6.3819e-02,  1.3366e-02],\n",
      "          [ 2.9751e-02, -7.9476e-03,  1.4007e-02]],\n",
      "\n",
      "         [[-2.5166e-03,  2.2051e-02, -1.9967e-02],\n",
      "          [-5.9436e-02,  4.3872e-02,  2.6832e-02],\n",
      "          [-1.7509e-02,  2.4625e-02,  2.4822e-02]],\n",
      "\n",
      "         [[ 3.5832e-02, -7.0357e-02,  3.9452e-03],\n",
      "          [-2.9835e-02,  9.2727e-02,  1.9336e-02],\n",
      "          [-2.9145e-02, -9.7087e-03, -7.3388e-02]]]])\n",
      "tensor([0.3090, 0.2147, 0.2366, 0.4259, 0.5137, 0.2181, 0.2204, 0.2300, 0.2640,\n",
      "        0.2695, 0.2138, 0.4602, 0.2661, 0.2319, 0.3900, 0.2389, 0.2660, 0.3634,\n",
      "        0.3474, 0.2477, 0.3285, 0.5349, 0.6440, 0.2275, 0.4482, 0.3078, 0.2604,\n",
      "        0.4651, 0.2179, 0.2858, 0.3426, 0.4420, 0.4450, 0.4500, 0.5516, 0.5092,\n",
      "        0.2564, 0.2634, 0.5664, 0.6410, 0.2228, 0.1986, 0.2460, 0.2242, 0.2143,\n",
      "        0.1982, 0.6368, 0.3106, 0.5049, 0.2403, 0.3065, 0.3760, 0.3794, 0.4281,\n",
      "        0.2991, 0.3326, 0.2596, 0.3345, 0.2006, 0.4351, 0.1683, 0.5149, 0.2629,\n",
      "        0.3254])\n",
      "tensor([ 0.1657,  0.2420,  0.1780, -0.0431, -0.2053,  0.1598,  0.2929,  0.0912,\n",
      "         0.1116,  0.0884,  0.1104, -0.2035,  0.1539,  0.0857, -0.1094,  0.0654,\n",
      "         0.0766, -0.2067, -0.0212,  0.1396,  0.0401, -0.2827, -0.3257, -0.0035,\n",
      "        -0.4373, -0.1248,  0.1282, -0.0874,  0.1199, -0.0829, -0.5315, -0.0780,\n",
      "        -0.3876, -0.0547, -0.1816, -0.1888,  0.1320,  0.0031, -0.2697, -0.2984,\n",
      "         0.1394,  0.2597,  0.1372,  0.0053,  0.0132,  0.3295, -0.2715, -0.0187,\n",
      "        -0.2467,  0.1579,  0.0165, -0.0890, -0.1903, -0.0787,  0.1700, -0.4832,\n",
      "         0.0619, -0.0677,  0.3125, -0.5064,  0.3138, -0.2617, -0.1545,  0.0063])\n",
      "tensor([[[[ 2.5947e-02, -1.0458e-01, -4.7712e-03],\n",
      "          [-8.6223e-02, -3.3021e-01, -1.0275e-01],\n",
      "          [-5.7426e-02, -1.9074e-01, -5.4646e-02]],\n",
      "\n",
      "         [[-1.6951e-02,  2.1384e-02, -2.1074e-03],\n",
      "          [-3.2983e-03,  4.5014e-02, -1.1510e-02],\n",
      "          [-5.9602e-02,  6.4942e-03,  2.9080e-03]],\n",
      "\n",
      "         [[-4.4903e-03,  1.9637e-02,  1.3167e-02],\n",
      "          [ 1.3050e-02, -7.7471e-03,  1.1931e-02],\n",
      "          [ 1.3454e-02,  1.1103e-02,  5.5145e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2706e-03, -7.7438e-03,  2.0753e-02],\n",
      "          [-4.0024e-02, -4.0383e-02, -3.4821e-02],\n",
      "          [-2.0251e-02, -9.5164e-03,  1.3954e-02]],\n",
      "\n",
      "         [[-2.3430e-03,  3.2303e-02, -4.3342e-03],\n",
      "          [ 8.6194e-03,  1.0553e-02,  1.8074e-03],\n",
      "          [-1.2760e-02, -1.0232e-02,  4.5711e-03]],\n",
      "\n",
      "         [[ 1.5302e-02,  2.1361e-02, -7.0908e-03],\n",
      "          [-1.4221e-02,  4.5979e-02,  2.1369e-02],\n",
      "          [ 3.1312e-02,  6.6428e-02,  2.1465e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3422e-02,  4.0515e-02,  9.6680e-03],\n",
      "          [ 3.2884e-02, -2.3474e-02,  3.4642e-02],\n",
      "          [-1.2861e-02,  5.0066e-02,  5.4579e-02]],\n",
      "\n",
      "         [[ 2.8764e-02,  4.3431e-02,  2.8258e-02],\n",
      "          [ 2.8734e-02, -3.5459e-02, -5.2788e-02],\n",
      "          [-5.5119e-02, -7.1813e-02, -8.2970e-02]],\n",
      "\n",
      "         [[ 9.5293e-02,  1.2549e-01, -6.4001e-02],\n",
      "          [-4.1166e-02, -9.0480e-04,  5.1387e-02],\n",
      "          [-1.1311e-01, -7.9823e-02,  1.4373e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.6924e-03,  2.0647e-02,  1.9521e-02],\n",
      "          [-6.7352e-03,  1.2601e-04,  4.8309e-03],\n",
      "          [-6.2405e-03, -9.2119e-03, -2.5806e-04]],\n",
      "\n",
      "         [[-2.6153e-02, -2.4641e-02,  4.0970e-02],\n",
      "          [-1.9164e-02, -1.0160e-02,  3.3163e-02],\n",
      "          [ 5.4200e-03,  9.0485e-04,  6.7799e-04]],\n",
      "\n",
      "         [[ 7.7762e-03,  2.6447e-02,  6.3650e-02],\n",
      "          [-3.0608e-02,  2.4959e-02,  1.2951e-02],\n",
      "          [-2.0938e-02, -7.7342e-03, -3.8790e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0893e-02, -1.4409e-02,  1.5730e-02],\n",
      "          [ 1.6655e-02,  4.4535e-02,  6.3212e-02],\n",
      "          [ 3.4121e-02,  7.3135e-02,  5.9203e-02]],\n",
      "\n",
      "         [[ 2.3195e-03,  7.7598e-03,  2.0308e-02],\n",
      "          [ 2.0457e-02,  4.0029e-02,  3.4744e-02],\n",
      "          [-4.7356e-02, -3.7286e-02,  1.4542e-02]],\n",
      "\n",
      "         [[-2.2742e-02, -1.9000e-02, -8.4317e-03],\n",
      "          [-9.8759e-04,  2.1510e-02,  6.3959e-03],\n",
      "          [-9.4558e-03,  2.6833e-03, -3.1136e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.5787e-03, -1.6056e-02, -6.4204e-04],\n",
      "          [-5.5104e-03,  1.4252e-02,  4.5000e-02],\n",
      "          [-9.2800e-03,  2.2351e-02,  4.1728e-02]],\n",
      "\n",
      "         [[ 2.5705e-02,  4.8207e-02,  7.9145e-02],\n",
      "          [-4.4350e-03,  3.8872e-03,  4.1694e-02],\n",
      "          [ 8.0536e-04, -1.0601e-02,  9.2706e-03]],\n",
      "\n",
      "         [[-3.3892e-02,  9.3543e-03,  4.1746e-02],\n",
      "          [-1.6470e-02,  3.9542e-03,  6.2438e-02],\n",
      "          [-3.1055e-02, -3.6302e-03,  7.0817e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.1044e-05, -9.0020e-03, -2.6998e-03],\n",
      "          [ 3.0072e-03,  1.1579e-02,  1.5214e-02],\n",
      "          [ 3.4832e-03,  1.1353e-05,  1.6320e-02]],\n",
      "\n",
      "         [[-2.6334e-02,  2.1967e-02, -6.0039e-02],\n",
      "          [ 4.4519e-02,  1.3203e-01, -9.1163e-03],\n",
      "          [ 5.4242e-02,  1.3726e-01,  2.7454e-02]],\n",
      "\n",
      "         [[ 1.7122e-02,  3.7646e-03,  1.4872e-02],\n",
      "          [ 1.2092e-02,  1.1319e-02,  3.4667e-02],\n",
      "          [ 8.1790e-03, -2.0805e-02,  2.7143e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0111e-02, -1.0526e-02,  2.8394e-02],\n",
      "          [-2.5112e-02, -2.2196e-02,  3.7229e-02],\n",
      "          [-3.8220e-02, -4.6644e-02,  1.5660e-02]],\n",
      "\n",
      "         [[-2.5913e-03, -2.4307e-02,  1.0611e-02],\n",
      "          [-2.1730e-02, -4.3938e-02, -7.1536e-03],\n",
      "          [-2.5171e-02, -5.9467e-02, -2.5577e-02]],\n",
      "\n",
      "         [[ 2.8652e-02,  2.5850e-04,  1.1416e-03],\n",
      "          [ 3.7812e-02, -1.1271e-03,  9.6027e-03],\n",
      "          [ 3.9350e-02,  1.0134e-02,  1.0449e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.9305e-03,  7.0872e-03,  2.1412e-02],\n",
      "          [-6.0065e-02,  1.4147e-03,  9.7281e-02],\n",
      "          [-6.0130e-02, -2.1725e-02,  3.6863e-02]],\n",
      "\n",
      "         [[ 2.8024e-02,  2.6183e-02, -2.3027e-02],\n",
      "          [ 5.1900e-02, -2.0588e-03, -1.0940e-01],\n",
      "          [-3.2729e-02, -6.2752e-03,  8.0630e-03]],\n",
      "\n",
      "         [[-1.8062e-02, -1.9510e-02,  4.3163e-02],\n",
      "          [ 4.6080e-02,  2.9494e-02,  4.0844e-02],\n",
      "          [ 5.9607e-03, -6.5891e-03, -6.4623e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2193e-02,  8.4653e-03,  3.6764e-03],\n",
      "          [ 1.7549e-02,  2.1971e-02, -4.5108e-03],\n",
      "          [ 2.1124e-02,  3.4591e-02, -1.6310e-02]],\n",
      "\n",
      "         [[ 3.8144e-02,  4.8395e-02, -9.5556e-02],\n",
      "          [ 1.8923e-02,  1.1341e-02, -7.6311e-02],\n",
      "          [ 4.7358e-03,  3.2138e-02, -7.4777e-02]],\n",
      "\n",
      "         [[-1.9031e-02, -3.2568e-02, -3.8251e-02],\n",
      "          [ 1.0705e-02,  2.3121e-03, -7.5078e-02],\n",
      "          [ 3.3316e-02,  3.5515e-02, -2.1023e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3330e-01,  7.4683e-02, -3.8624e-03],\n",
      "          [ 9.1377e-02,  8.2415e-02,  3.9469e-02],\n",
      "          [-1.8265e-02, -5.9943e-02,  8.9354e-02]],\n",
      "\n",
      "         [[ 1.5566e-02, -4.1716e-02,  1.0633e-02],\n",
      "          [ 7.2644e-03,  3.1934e-02,  1.2732e-03],\n",
      "          [-2.0851e-02, -3.7593e-03, -7.0170e-02]],\n",
      "\n",
      "         [[-6.6139e-02,  1.0627e-01,  1.9590e-02],\n",
      "          [ 5.4987e-02, -1.5552e-01, -1.8819e-02],\n",
      "          [-4.2554e-03,  4.4964e-02, -2.4632e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.1691e-02, -4.5531e-02, -9.1721e-03],\n",
      "          [ 4.3995e-02,  4.5703e-02, -7.0108e-02],\n",
      "          [ 1.1388e-02,  4.4678e-02, -4.5953e-02]],\n",
      "\n",
      "         [[ 4.3432e-03,  2.3194e-02, -2.1895e-02],\n",
      "          [-8.0216e-02, -5.7606e-02, -9.8455e-03],\n",
      "          [-3.3285e-02, -1.1468e-01, -2.3779e-02]],\n",
      "\n",
      "         [[-6.3785e-02, -2.4485e-02, -4.9061e-02],\n",
      "          [-6.1594e-02,  1.0328e-01,  5.9685e-03],\n",
      "          [ 8.1863e-02, -3.0314e-02, -4.6373e-03]]]])\n",
      "tensor([0.2496, 0.2198, 0.2756, 0.6073, 0.2654, 0.2942, 0.1136, 0.4425, 0.2868,\n",
      "        0.2974, 0.2506, 0.4103, 0.4855, 0.3383, 0.4670, 0.1772, 0.2171, 0.5025,\n",
      "        0.2263, 0.3667, 0.4867, 0.4586, 0.4652, 0.2200, 0.1510, 0.2761, 0.3813,\n",
      "        0.2803, 0.2382, 0.3953, 0.3032, 0.3163, 0.2025, 0.2323, 0.2003, 0.1661,\n",
      "        0.4690, 0.3476, 0.3414, 0.2274, 0.2485, 0.2356, 0.2726, 0.4657, 0.3429,\n",
      "        0.2465, 0.4674, 0.2812, 0.6241, 0.4152, 0.3403, 0.4218, 0.1152, 0.2985,\n",
      "        0.5802, 0.2795, 0.4706, 0.4517, 0.4303, 0.2749, 0.3427, 0.1137, 0.5069,\n",
      "        0.4370])\n",
      "tensor([ 2.2752e-01,  8.6747e-03, -6.7346e-02, -6.8779e-02,  3.5977e-01,\n",
      "        -2.0167e-01, -4.8431e-05,  2.3735e-02,  3.9549e-01,  3.7079e-02,\n",
      "         6.8793e-03,  2.7578e-01, -7.0272e-02, -2.3970e-01, -8.1753e-02,\n",
      "        -9.4132e-02, -1.4544e-01,  3.7301e-02, -3.6174e-01, -3.9561e-01,\n",
      "        -4.0789e-01,  3.5559e-03, -2.7878e-01, -3.5299e-02, -7.0281e-02,\n",
      "         2.1005e-01, -4.6362e-03, -1.9665e-01, -2.8066e-01, -1.6540e-02,\n",
      "         2.6452e-01, -8.9359e-02, -2.1046e-01, -1.3026e-01,  1.7215e-01,\n",
      "         5.3403e-02, -2.2295e-01, -4.8033e-02,  2.4572e-01,  2.0950e-01,\n",
      "         1.6220e-01,  1.1370e-01,  1.1457e-01, -1.4870e-01, -3.2150e-02,\n",
      "        -3.0549e-01,  4.9125e-01,  1.0873e-01,  1.2779e-02,  1.0044e-01,\n",
      "         4.1553e-01, -1.4710e-02,  2.3922e-02,  9.9812e-02, -1.7273e-01,\n",
      "         1.0078e-01, -1.4564e-01, -2.2735e-01,  1.3637e-01,  2.0127e-01,\n",
      "        -5.7430e-02,  2.3530e-01, -1.1299e-01,  3.0933e-01])\n",
      "tensor([[[[ 1.9712e-02, -5.2562e-03, -3.7619e-03],\n",
      "          [-1.9635e-02, -1.2336e-02, -3.5196e-02],\n",
      "          [ 5.0761e-02,  7.5668e-02,  4.3344e-02]],\n",
      "\n",
      "         [[ 1.4160e-02, -8.6094e-03, -1.0541e-02],\n",
      "          [-4.2586e-02, -2.3814e-02, -5.4694e-02],\n",
      "          [-1.4018e-03,  4.6720e-02,  5.0898e-02]],\n",
      "\n",
      "         [[ 2.1559e-02,  4.1633e-03, -9.7118e-03],\n",
      "          [-9.3201e-03, -2.5432e-02, -2.8274e-02],\n",
      "          [-3.0107e-02, -4.8230e-02, -2.6001e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.4300e-03,  9.1875e-02,  3.1938e-03],\n",
      "          [-1.7945e-02,  5.7266e-02, -8.4098e-03],\n",
      "          [-3.4961e-02, -2.3296e-02, -3.5089e-02]],\n",
      "\n",
      "         [[ 2.5603e-02, -3.1689e-02, -5.4160e-02],\n",
      "          [ 6.9736e-02, -1.0716e-02, -6.8034e-02],\n",
      "          [ 3.5578e-02,  3.4749e-02, -1.9334e-02]],\n",
      "\n",
      "         [[-6.5420e-02, -4.6427e-03, -2.3362e-02],\n",
      "          [ 7.5833e-02,  9.1174e-03, -4.9701e-02],\n",
      "          [ 6.2944e-02, -9.8735e-02,  3.3158e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.0557e-03, -3.0753e-02,  1.1953e-02],\n",
      "          [-3.2539e-02, -6.2846e-03, -2.0235e-02],\n",
      "          [ 4.7996e-03, -2.1462e-02, -4.1557e-03]],\n",
      "\n",
      "         [[ 1.7163e-02, -2.3303e-03,  7.3972e-02],\n",
      "          [-3.2105e-02, -7.7536e-02, -1.2648e-02],\n",
      "          [ 3.8985e-02, -4.3170e-02,  1.0904e-02]],\n",
      "\n",
      "         [[-2.9643e-02, -5.8534e-02, -5.9736e-02],\n",
      "          [-2.9437e-02, -3.6441e-02, -1.2380e-02],\n",
      "          [-2.2775e-02, -2.4485e-03, -1.6124e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.6830e-02,  1.4267e-02,  6.2658e-02],\n",
      "          [ 3.0585e-04, -5.3241e-03,  3.2786e-03],\n",
      "          [ 2.1097e-02, -2.3189e-02,  1.2102e-02]],\n",
      "\n",
      "         [[-6.1182e-02, -2.9227e-02,  2.0036e-02],\n",
      "          [-7.6089e-02, -7.7057e-02,  8.6544e-02],\n",
      "          [-3.9228e-02, -3.2361e-02, -8.8970e-02]],\n",
      "\n",
      "         [[-1.3372e-01,  8.8362e-02,  8.3836e-02],\n",
      "          [-1.1688e-02,  4.3156e-01, -3.3629e-03],\n",
      "          [-2.3925e-02, -1.0092e-01, -1.0184e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.0165e-02,  4.3042e-02,  2.7325e-03],\n",
      "          [ 3.5269e-02, -1.5504e-02, -3.5011e-02],\n",
      "          [-1.7164e-02, -2.6827e-02, -3.3946e-02]],\n",
      "\n",
      "         [[ 4.5439e-02,  5.1585e-02,  1.8321e-02],\n",
      "          [-3.9647e-02,  2.3956e-02, -2.6609e-02],\n",
      "          [-3.0358e-02, -6.4729e-02,  2.5834e-02]],\n",
      "\n",
      "         [[ 3.8105e-02,  4.0986e-02,  4.1005e-02],\n",
      "          [ 1.7584e-02, -1.6494e-02, -3.2716e-02],\n",
      "          [ 5.5886e-03, -1.7068e-02, -3.0605e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3694e-01, -1.4074e-01,  5.1423e-02],\n",
      "          [-1.2521e-01, -1.3128e-01,  7.5733e-02],\n",
      "          [-4.5032e-02, -1.7081e-02,  7.1252e-02]],\n",
      "\n",
      "         [[ 6.3381e-02,  1.5874e-02, -2.7322e-02],\n",
      "          [ 8.0356e-02,  3.6104e-02, -2.8506e-02],\n",
      "          [ 2.6638e-02,  2.2021e-02,  3.2345e-02]],\n",
      "\n",
      "         [[-1.2068e-03, -4.6179e-02, -1.5351e-02],\n",
      "          [-1.1276e-02,  1.9200e-02,  3.4336e-02],\n",
      "          [ 1.6540e-02, -7.8592e-03, -2.5392e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.3384e-02,  6.9963e-02,  1.0745e-02],\n",
      "          [-1.7518e-02, -5.3524e-02, -6.4960e-02],\n",
      "          [ 3.4248e-04, -4.5557e-02, -4.7336e-02]],\n",
      "\n",
      "         [[-5.1031e-03,  7.9784e-03, -8.6553e-04],\n",
      "          [-1.6557e-03,  1.4661e-02,  5.3365e-03],\n",
      "          [-3.1784e-02, -6.6940e-02, -4.6889e-02]],\n",
      "\n",
      "         [[-1.1775e-02,  7.2759e-03,  7.6622e-03],\n",
      "          [-6.1288e-02, -5.2078e-02, -4.5152e-02],\n",
      "          [-8.6584e-02, -9.7381e-02, -1.0405e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1243e-02,  6.2456e-02,  2.5188e-02],\n",
      "          [-2.2911e-02, -2.1100e-03, -2.7573e-02],\n",
      "          [ 4.6557e-02,  6.4980e-02,  3.1879e-02]],\n",
      "\n",
      "         [[ 6.2867e-03,  2.4255e-02,  8.9674e-02],\n",
      "          [-7.7718e-03, -5.4311e-02, -4.6843e-02],\n",
      "          [-6.7499e-03, -6.6857e-02, -4.9842e-02]],\n",
      "\n",
      "         [[ 4.7326e-03, -3.9533e-02,  1.1500e-03],\n",
      "          [-2.7957e-02, -1.3466e-01, -6.0753e-02],\n",
      "          [-3.2010e-03,  7.2213e-02,  1.1009e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3763e-02, -1.7876e-02, -7.4843e-03],\n",
      "          [ 1.6239e-02,  5.4479e-04, -3.3735e-02],\n",
      "          [-2.2854e-02, -1.4316e-03,  1.1010e-02]],\n",
      "\n",
      "         [[ 5.2277e-03, -2.5941e-03,  5.9594e-03],\n",
      "          [-2.9058e-03, -7.3409e-03,  3.0652e-02],\n",
      "          [ 7.5540e-02,  6.6445e-03,  2.5518e-03]],\n",
      "\n",
      "         [[-6.5970e-02, -4.1286e-02, -3.0278e-02],\n",
      "          [-3.5108e-02, -3.9099e-02, -1.6818e-02],\n",
      "          [-1.0224e-02, -8.6995e-03, -5.9939e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1233e-02, -2.4559e-02, -7.4436e-03],\n",
      "          [-4.3734e-03, -3.2864e-02, -3.3453e-02],\n",
      "          [ 8.9269e-03, -1.7646e-02,  3.8375e-04]],\n",
      "\n",
      "         [[-7.8930e-02, -7.2940e-02, -6.7911e-02],\n",
      "          [-8.4146e-02, -8.3657e-02,  5.3666e-02],\n",
      "          [-3.5577e-02, -3.6835e-02,  5.8987e-03]],\n",
      "\n",
      "         [[ 8.3767e-02,  8.0476e-05,  7.2164e-02],\n",
      "          [-6.4219e-02, -1.2661e-01,  4.6026e-02],\n",
      "          [ 9.3033e-02, -4.7521e-02,  3.6777e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1012e-02,  1.3361e-03, -5.8616e-02],\n",
      "          [ 4.2461e-02,  2.9437e-03, -2.0445e-02],\n",
      "          [ 7.6097e-02,  5.2504e-02, -5.5636e-03]],\n",
      "\n",
      "         [[ 2.2046e-02,  4.0888e-03,  1.4645e-02],\n",
      "          [-7.7532e-02, -1.1912e-01, -7.0892e-02],\n",
      "          [-1.0618e-02, -3.2121e-02, -2.3969e-02]],\n",
      "\n",
      "         [[-2.1612e-02, -2.6110e-03, -3.1664e-02],\n",
      "          [-3.2892e-02, -3.9771e-02, -5.1463e-02],\n",
      "          [-2.6150e-02, -3.6554e-02, -2.3315e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.4600e-03,  8.4181e-02,  2.3199e-02],\n",
      "          [ 5.7595e-02,  1.3036e-01,  3.2172e-02],\n",
      "          [-2.2774e-03,  4.2065e-02, -4.8619e-02]],\n",
      "\n",
      "         [[ 3.1533e-02, -4.3655e-02,  2.0361e-02],\n",
      "          [ 3.9973e-03, -5.1430e-02, -6.3839e-02],\n",
      "          [ 6.4002e-03,  4.5347e-02,  4.7346e-02]],\n",
      "\n",
      "         [[-9.1818e-02,  1.0264e-02,  9.6565e-02],\n",
      "          [-2.1635e-03, -2.3452e-02, -5.9038e-02],\n",
      "          [ 1.9402e-02,  2.8854e-02, -9.6113e-02]]]])\n",
      "tensor([0.3910, 0.4375, 0.3746, 0.3990, 0.3404, 0.3503, 0.2618, 0.2707, 0.2865,\n",
      "        0.4308, 0.1895, 0.3041, 0.3837, 0.2944, 0.2105, 0.3304, 0.2943, 0.2887,\n",
      "        0.2060, 0.4627, 0.2335, 0.1831, 0.4489, 0.2830, 0.3389, 0.2997, 0.3503,\n",
      "        0.2735, 0.3908, 0.2817, 0.2636, 0.4462, 0.3282, 0.3776, 0.4471, 0.3878,\n",
      "        0.2516, 0.3172, 0.3661, 0.3166, 0.3818, 0.3128, 0.2274, 0.3627, 0.2902,\n",
      "        0.2381, 0.2988, 0.2469, 0.3840, 0.2886, 0.3197, 0.2879, 0.3218, 0.4559,\n",
      "        0.3500, 0.2420, 0.3396, 0.3519, 0.3839, 0.3806, 0.4039, 0.2826, 0.4594,\n",
      "        0.3342])\n",
      "tensor([-0.0997, -0.4755, -0.0474, -0.2698, -0.0834, -0.0072,  0.0474,  0.1022,\n",
      "        -0.0170, -0.1471,  0.2307,  0.1447, -0.1775,  0.0273,  0.1559, -0.1836,\n",
      "         0.1238, -0.1522,  0.0554, -0.2881, -0.2606,  0.2316, -0.3242, -0.0219,\n",
      "        -0.2645,  0.0576, -0.2465,  0.0481, -0.3530,  0.0950, -0.1862, -0.1707,\n",
      "        -0.0161, -0.2604, -0.3145, -0.1083,  0.0659, -0.1427, -0.0570, -0.0076,\n",
      "        -0.3006, -0.0744, -0.0683, -0.1104,  0.0253,  0.0489, -0.2515,  0.1150,\n",
      "        -0.3783,  0.0846, -0.0368,  0.1439, -0.0468, -0.3087, -0.0240,  0.1397,\n",
      "        -0.0908, -0.1795, -0.1129, -0.0793, -0.1491,  0.0594, -0.4433, -0.0138])\n",
      "tensor([[[[-2.1574e-02, -4.5688e-03,  4.5483e-03],\n",
      "          [-8.1870e-03,  4.1740e-02,  2.3010e-02],\n",
      "          [-8.9283e-03,  5.7352e-02,  2.9818e-02]],\n",
      "\n",
      "         [[ 5.8627e-02,  4.2864e-02,  4.4912e-02],\n",
      "          [ 2.2281e-02, -1.2969e-02,  7.6099e-03],\n",
      "          [ 4.5373e-02,  3.0712e-02,  3.7700e-02]],\n",
      "\n",
      "         [[-1.5456e-02, -3.8692e-02, -4.6010e-02],\n",
      "          [-2.3123e-02,  2.8293e-02,  4.7790e-03],\n",
      "          [-2.0328e-02,  1.3756e-02,  2.5883e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.1302e-02,  4.2291e-02,  5.7833e-02],\n",
      "          [ 4.5210e-02,  5.5850e-02,  1.4318e-02],\n",
      "          [ 1.4241e-02,  1.7968e-02,  1.4344e-02]],\n",
      "\n",
      "         [[ 4.6012e-03,  1.2566e-02,  4.8931e-02],\n",
      "          [-6.5754e-03, -2.6431e-02,  1.5855e-02],\n",
      "          [ 1.3192e-02,  1.9011e-02,  1.3842e-02]],\n",
      "\n",
      "         [[ 6.1983e-02,  6.9919e-02,  6.1035e-02],\n",
      "          [ 6.1253e-02,  9.9557e-02,  5.9060e-02],\n",
      "          [ 5.8298e-02,  8.1652e-02,  8.1499e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0088e-02, -1.2959e-02,  9.7798e-03],\n",
      "          [ 5.5408e-02,  4.3501e-02,  5.6983e-02],\n",
      "          [ 5.3427e-02,  3.5118e-02,  3.6782e-02]],\n",
      "\n",
      "         [[ 2.4442e-03, -3.0207e-02, -1.0377e-02],\n",
      "          [-4.5297e-02, -4.5318e-02,  5.4623e-03],\n",
      "          [-4.4762e-02, -1.5508e-02,  6.9745e-03]],\n",
      "\n",
      "         [[ 3.9658e-02,  3.6838e-02,  5.8796e-03],\n",
      "          [ 2.3207e-02,  3.9240e-03, -2.0887e-02],\n",
      "          [-1.4829e-02,  5.3606e-03,  1.7404e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.2160e-02,  5.9042e-02,  4.8433e-02],\n",
      "          [-2.6464e-02, -8.0667e-03, -1.0359e-02],\n",
      "          [-2.6699e-02, -9.5411e-03, -2.8902e-02]],\n",
      "\n",
      "         [[-2.9235e-02, -3.9078e-02, -4.4955e-02],\n",
      "          [-2.0346e-02, -4.4891e-02, -3.7477e-02],\n",
      "          [ 1.9653e-02, -1.5562e-03, -5.8245e-03]],\n",
      "\n",
      "         [[-5.0696e-02, -4.8902e-02,  9.1631e-03],\n",
      "          [ 5.1668e-03,  2.0509e-02,  6.6874e-02],\n",
      "          [ 2.8934e-02,  4.6717e-02,  2.1371e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1744e-02, -2.8354e-02, -3.2557e-02],\n",
      "          [ 3.0519e-02,  1.8536e-02,  1.5244e-02],\n",
      "          [ 1.3832e-03,  1.7051e-02,  3.2020e-02]],\n",
      "\n",
      "         [[-3.6293e-02,  1.0914e-02,  4.5371e-02],\n",
      "          [ 1.3399e-02,  6.4272e-02,  8.8210e-02],\n",
      "          [ 4.6697e-02,  9.9653e-02,  8.7606e-02]],\n",
      "\n",
      "         [[-2.4336e-02, -2.9627e-02,  1.9537e-02],\n",
      "          [-3.3412e-02, -2.2290e-02, -2.8879e-02],\n",
      "          [ 1.4765e-02,  1.7234e-02, -1.8185e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.9859e-02, -7.1075e-02, -5.8546e-02],\n",
      "          [ 2.2902e-02,  1.1184e-02, -2.3654e-02],\n",
      "          [ 8.1897e-02,  1.1996e-01,  9.3242e-02]],\n",
      "\n",
      "         [[ 3.1984e-02,  7.4931e-02,  6.6020e-02],\n",
      "          [ 2.8490e-02,  1.1931e-01,  1.2100e-01],\n",
      "          [ 7.9259e-04,  4.3812e-02,  4.4648e-02]],\n",
      "\n",
      "         [[ 3.2748e-02,  4.1444e-02, -8.1932e-03],\n",
      "          [ 4.5541e-02,  2.9426e-02, -8.5440e-03],\n",
      "          [ 1.1634e-04,  1.8045e-03,  1.4826e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.4144e-02, -8.3106e-02, -5.3073e-02],\n",
      "          [ 3.2124e-02,  1.0286e-02,  2.4409e-02],\n",
      "          [ 6.1606e-03, -1.9455e-02,  4.0534e-02]],\n",
      "\n",
      "         [[ 5.6026e-04,  9.6961e-03,  2.5010e-03],\n",
      "          [ 7.1679e-03, -1.7535e-02, -2.3857e-02],\n",
      "          [-9.8745e-03, -1.8550e-02,  1.7301e-03]],\n",
      "\n",
      "         [[ 4.3882e-03,  4.2049e-02,  7.5950e-02],\n",
      "          [-6.5610e-02, -3.6130e-02, -1.9404e-02],\n",
      "          [-3.8091e-02, -2.6749e-02, -1.3865e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.5593e-02, -4.6050e-02, -2.2809e-02],\n",
      "          [-9.7648e-03,  2.4910e-03,  2.4503e-02],\n",
      "          [ 2.0381e-02,  5.2393e-02,  6.9019e-02]],\n",
      "\n",
      "         [[ 9.3306e-04,  1.2483e-02, -1.1817e-02],\n",
      "          [-1.2627e-02, -1.8756e-02, -1.4144e-03],\n",
      "          [-5.2490e-03, -4.6126e-03, -1.3224e-02]],\n",
      "\n",
      "         [[ 7.4689e-04, -1.0135e-02, -7.8264e-03],\n",
      "          [ 1.2491e-02, -2.5865e-02,  4.0514e-02],\n",
      "          [ 5.8855e-03,  4.5990e-02,  1.0651e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2262e-02, -1.5378e-02,  1.3862e-03],\n",
      "          [ 4.1166e-02, -2.4944e-02, -2.6686e-02],\n",
      "          [-1.7423e-02,  5.2690e-03, -2.1861e-02]],\n",
      "\n",
      "         [[-3.1207e-02, -3.3025e-02,  2.2114e-02],\n",
      "          [-2.4009e-02,  1.2988e-02,  2.2430e-02],\n",
      "          [ 1.0332e-02,  4.3601e-03,  4.7321e-03]],\n",
      "\n",
      "         [[ 2.0182e-02,  6.1569e-02, -2.8771e-02],\n",
      "          [ 5.8231e-02,  4.6767e-02, -2.8417e-05],\n",
      "          [ 3.7545e-02, -4.5886e-02,  1.5849e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0431e-03, -3.6082e-03,  7.1986e-03],\n",
      "          [ 2.4895e-02,  6.1671e-03, -3.2427e-02],\n",
      "          [ 7.2338e-03,  2.2406e-03, -5.3330e-02]],\n",
      "\n",
      "         [[ 2.8072e-02, -1.0571e-02, -1.3854e-02],\n",
      "          [-1.0879e-02,  6.1929e-03, -5.6713e-03],\n",
      "          [-2.6083e-02,  8.1861e-03, -3.2873e-02]],\n",
      "\n",
      "         [[-3.1032e-02, -6.0485e-02, -2.5583e-02],\n",
      "          [-4.6239e-02, -2.2805e-02, -7.7678e-03],\n",
      "          [-9.4698e-03,  4.0247e-03, -4.8637e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3128e-02, -5.6038e-02, -3.4572e-02],\n",
      "          [ 1.0638e-03,  5.7929e-02, -7.6970e-03],\n",
      "          [-3.0103e-02,  3.5573e-02, -1.8143e-02]],\n",
      "\n",
      "         [[ 9.6840e-02, -1.1186e-01, -7.8766e-02],\n",
      "          [-1.0444e-01, -1.0851e-01, -1.9553e-01],\n",
      "          [-1.1986e-01, -7.1474e-02,  3.6750e-02]],\n",
      "\n",
      "         [[-2.2194e-02,  6.0298e-03,  5.6914e-02],\n",
      "          [-4.8342e-02,  7.8893e-02, -5.1026e-02],\n",
      "          [-5.1294e-02, -5.7434e-02, -1.9178e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.4896e-02, -8.1267e-02,  5.1794e-02],\n",
      "          [-8.3985e-02, -5.7778e-02,  6.7891e-02],\n",
      "          [ 2.3837e-02,  3.8954e-02,  4.1141e-02]],\n",
      "\n",
      "         [[ 4.6446e-03,  2.7367e-02, -2.3154e-02],\n",
      "          [ 2.0675e-02,  2.3429e-02,  6.4380e-04],\n",
      "          [-5.2222e-02, -1.4854e-02, -2.5150e-02]],\n",
      "\n",
      "         [[ 2.1291e-02,  1.2736e-02,  8.4553e-03],\n",
      "          [-8.2932e-02,  7.2067e-02,  1.3107e-01],\n",
      "          [ 8.5491e-03,  1.3677e-01,  3.9867e-02]]]])\n",
      "tensor([0.2560, 0.5690, 0.4042, 0.5130, 0.2178, 0.4940, 0.3315, 0.5510, 0.4354,\n",
      "        0.5291, 0.2081, 0.4735, 0.5945, 0.5645, 0.2761, 0.2571, 0.4853, 0.6240,\n",
      "        0.4370, 0.2308, 0.4970, 0.3157, 0.5706, 0.2162, 0.1932, 0.1448, 0.2218,\n",
      "        0.2389, 0.5871, 0.3501, 0.4109, 0.3199, 0.5808, 0.3281, 0.2723, 0.1971,\n",
      "        0.6139, 0.4075, 0.6304, 0.3874, 0.7605, 0.2111, 0.3071, 0.4603, 0.3099,\n",
      "        0.1914, 0.4431, 0.2537, 0.5745, 0.6459, 0.3914, 0.3090, 0.6782, 0.1937,\n",
      "        0.5814, 0.2570, 0.3514, 0.2124, 0.5794, 0.3415, 0.2051, 0.0715, 0.4090,\n",
      "        0.4416])\n",
      "tensor([-0.1778, -0.1287,  0.0349, -0.1452,  0.1864, -0.1413, -0.4201, -0.1334,\n",
      "         0.2183, -0.1912,  0.0311, -0.0235, -0.1724, -0.0274, -0.0295, -0.1031,\n",
      "         0.0047,  0.0828, -0.1521,  0.0183, -0.2418, -0.0831, -0.0491, -0.0688,\n",
      "        -0.2560,  0.1381, -0.0165,  0.2092, -0.0028, -0.0265, -0.0225,  0.0286,\n",
      "        -0.1065, -0.3698,  0.2862, -0.1036,  0.3080, -0.0894,  0.2772,  0.1136,\n",
      "        -0.3157,  0.0423,  0.0567,  0.2369, -0.0727,  0.0465, -0.0536,  0.1309,\n",
      "         0.0282, -0.1371,  0.1464, -0.0717, -0.3237, -0.1583, -0.0424, -0.1278,\n",
      "        -0.1703,  0.0413,  0.0891,  0.0770, -0.0730,  0.0683, -0.0391,  0.0476])\n",
      "tensor([[[[-7.1555e-02, -1.1031e-01, -1.3711e-01],\n",
      "          [ 7.0593e-02, -1.4782e-02, -1.0053e-01],\n",
      "          [ 1.1938e-01,  8.7330e-02, -8.2206e-03]],\n",
      "\n",
      "         [[-2.3999e-02, -6.3682e-03,  2.4303e-03],\n",
      "          [ 6.1831e-03,  1.8781e-02,  2.5324e-02],\n",
      "          [ 2.3656e-03, -4.0037e-03, -1.1949e-02]],\n",
      "\n",
      "         [[ 6.0344e-03,  6.3784e-03, -1.2247e-02],\n",
      "          [ 7.8854e-03, -1.3464e-02, -4.2702e-02],\n",
      "          [ 1.7380e-02, -1.3862e-02, -4.7145e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4324e-02,  3.2257e-02,  2.5819e-02],\n",
      "          [ 8.4676e-03, -4.5413e-04, -1.0832e-02],\n",
      "          [-6.7166e-03, -1.5052e-02, -2.6939e-02]],\n",
      "\n",
      "         [[-1.2089e-02, -2.3588e-02, -2.2689e-02],\n",
      "          [ 1.0135e-02,  1.8285e-02, -1.5695e-02],\n",
      "          [ 2.1352e-02,  5.8568e-02,  4.2873e-02]],\n",
      "\n",
      "         [[ 1.4421e-02, -2.8298e-02, -7.0770e-03],\n",
      "          [ 3.0260e-02, -6.6294e-03, -1.6901e-02],\n",
      "          [ 3.9085e-02,  1.4222e-02,  2.2294e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.7911e-02, -7.3929e-02, -3.6671e-02],\n",
      "          [-3.4903e-02, -6.2355e-02, -3.7793e-02],\n",
      "          [-2.8379e-02, -5.4291e-02, -4.9411e-02]],\n",
      "\n",
      "         [[-1.2970e-02, -2.1825e-02, -2.8767e-04],\n",
      "          [ 7.6444e-03,  1.7653e-02,  1.6660e-02],\n",
      "          [ 3.8337e-02,  2.3006e-02, -1.6620e-03]],\n",
      "\n",
      "         [[-8.7592e-02, -8.4735e-02, -5.5818e-02],\n",
      "          [-7.7731e-02, -8.0311e-02, -3.2554e-02],\n",
      "          [-5.6313e-02, -4.2047e-02,  1.5247e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2377e-02, -4.0018e-02, -2.9523e-02],\n",
      "          [-1.5294e-02, -1.4165e-02,  2.7086e-03],\n",
      "          [ 1.1652e-02,  2.3886e-02,  2.4413e-02]],\n",
      "\n",
      "         [[ 2.0891e-03, -3.0475e-02, -3.3818e-02],\n",
      "          [ 6.7829e-03,  3.8681e-04, -1.4540e-02],\n",
      "          [-3.1306e-03,  6.7689e-03,  8.4524e-03]],\n",
      "\n",
      "         [[ 3.0586e-02,  4.6281e-02,  3.8359e-04],\n",
      "          [ 5.3079e-02,  6.7488e-02,  3.0547e-02],\n",
      "          [ 2.3374e-02,  4.3993e-02, -3.8713e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3878e-02,  3.2724e-02,  4.6584e-02],\n",
      "          [-8.0647e-03,  1.6209e-03,  1.5153e-02],\n",
      "          [-7.0342e-02, -5.3299e-02, -4.5920e-02]],\n",
      "\n",
      "         [[ 4.6035e-02,  3.5400e-02,  3.4941e-02],\n",
      "          [ 5.8351e-02,  5.4640e-02,  2.7162e-02],\n",
      "          [ 2.6799e-02,  4.5056e-02,  6.6886e-03]],\n",
      "\n",
      "         [[-3.3766e-02, -3.8605e-02, -2.4172e-02],\n",
      "          [-1.8285e-03,  1.0888e-02,  1.1425e-02],\n",
      "          [ 2.2282e-02,  1.4024e-02,  3.6332e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6330e-02, -6.9552e-02, -8.9737e-02],\n",
      "          [ 3.9766e-02,  1.5501e-02, -2.2695e-02],\n",
      "          [ 1.0290e-01,  1.2294e-01,  6.3867e-02]],\n",
      "\n",
      "         [[-4.2318e-03,  4.9511e-02, -7.6289e-03],\n",
      "          [-2.7720e-02,  7.0398e-03, -9.4052e-03],\n",
      "          [-6.7008e-02, -6.0542e-02, -2.5967e-02]],\n",
      "\n",
      "         [[-5.8560e-03, -1.7573e-02, -3.8016e-02],\n",
      "          [ 2.8579e-03, -4.1603e-03,  1.0113e-02],\n",
      "          [ 2.6243e-02,  3.5200e-02,  3.1143e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.4193e-02, -6.5322e-02, -1.7594e-02],\n",
      "          [-9.3970e-02, -5.8291e-02,  1.2093e-02],\n",
      "          [-2.2998e-02,  3.2463e-02,  7.1731e-02]],\n",
      "\n",
      "         [[-4.7220e-03, -3.0125e-03, -1.8075e-02],\n",
      "          [ 1.2667e-02, -8.0509e-03, -1.4605e-02],\n",
      "          [ 7.8220e-03, -1.0720e-02, -2.6515e-02]],\n",
      "\n",
      "         [[-2.5299e-02, -4.9383e-02, -1.2720e-02],\n",
      "          [-5.2206e-02, -4.7233e-02, -4.2470e-03],\n",
      "          [-4.8697e-02, -2.5320e-02,  8.6178e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7617e-03,  7.8398e-03, -5.9525e-03],\n",
      "          [ 4.0277e-03,  7.3575e-03, -1.1667e-02],\n",
      "          [-3.9997e-02, -3.8038e-02, -5.0469e-02]],\n",
      "\n",
      "         [[-3.8949e-03, -6.8965e-03,  3.4102e-02],\n",
      "          [-6.9814e-03, -4.9762e-02,  5.8711e-02],\n",
      "          [ 1.8361e-02,  2.5874e-02,  8.0028e-02]],\n",
      "\n",
      "         [[-3.3014e-02, -2.1510e-02, -2.1509e-03],\n",
      "          [-4.3894e-02, -3.2009e-02, -1.6265e-02],\n",
      "          [-1.1037e-02,  2.8872e-04,  3.0937e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.9907e-02, -5.0222e-02, -5.0985e-02],\n",
      "          [ 2.2644e-02, -1.4098e-02, -2.4426e-02],\n",
      "          [ 1.9960e-02,  9.6426e-02,  1.0580e-01]],\n",
      "\n",
      "         [[-3.6873e-02,  2.1413e-03,  8.3469e-03],\n",
      "          [-4.0796e-02, -3.3767e-02, -3.4955e-02],\n",
      "          [ 3.9466e-02,  7.0508e-02,  8.6065e-02]],\n",
      "\n",
      "         [[ 1.4842e-02,  6.6914e-03,  1.4324e-02],\n",
      "          [-3.2621e-02, -4.4027e-02, -2.2269e-02],\n",
      "          [ 7.1982e-03, -1.9187e-02, -4.9348e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9938e-03,  1.6018e-02,  1.1242e-02],\n",
      "          [-4.7668e-03,  2.1921e-02,  2.2660e-02],\n",
      "          [-2.6753e-02,  2.6917e-04, -5.6827e-03]],\n",
      "\n",
      "         [[-8.7725e-03,  1.0761e-02,  7.3603e-03],\n",
      "          [-1.8010e-05, -1.7926e-02,  4.8229e-03],\n",
      "          [ 4.2431e-02, -1.5764e-02,  2.3554e-02]],\n",
      "\n",
      "         [[-1.3830e-02, -3.0793e-03, -4.0854e-03],\n",
      "          [ 3.3363e-02,  4.2952e-02,  3.5867e-02],\n",
      "          [-3.9653e-02, -3.0855e-02, -4.3189e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.8617e-02, -3.1549e-03,  2.5739e-03],\n",
      "          [-1.1592e-02,  9.8761e-03,  7.5235e-03],\n",
      "          [-1.9339e-02, -9.8779e-03,  2.1755e-03]],\n",
      "\n",
      "         [[ 1.6889e-04,  1.8302e-03, -8.9537e-03],\n",
      "          [ 5.8343e-03,  1.7360e-02, -1.9029e-02],\n",
      "          [ 5.8642e-03, -7.4307e-04,  1.4667e-03]],\n",
      "\n",
      "         [[-1.6506e-02, -2.8401e-02,  1.3986e-02],\n",
      "          [-2.2922e-02, -4.3484e-02,  1.0471e-02],\n",
      "          [-2.5801e-03, -4.5258e-02,  7.9791e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5260e-03, -7.6469e-03,  1.3597e-02],\n",
      "          [ 5.5301e-04, -2.9176e-03,  2.2147e-02],\n",
      "          [ 3.2763e-03, -1.0775e-05,  1.3163e-02]],\n",
      "\n",
      "         [[ 5.1756e-03,  1.8495e-02, -8.0268e-03],\n",
      "          [-3.5030e-02,  2.6403e-02, -7.1220e-03],\n",
      "          [-5.2325e-02, -1.1185e-02,  1.9146e-02]],\n",
      "\n",
      "         [[-6.8805e-02,  5.1618e-02,  1.9787e-02],\n",
      "          [ 2.5533e-02, -6.1926e-02,  4.9924e-02],\n",
      "          [ 1.0532e-01, -4.4136e-02,  4.9907e-02]]]])\n",
      "tensor([0.3248, 0.3613, 0.2960, 0.2913, 0.3407, 0.3435, 0.3049, 0.3308, 0.3447,\n",
      "        0.3860, 0.3196, 0.2622, 0.2994, 0.2189, 0.2397, 0.3744, 0.3555, 0.1948,\n",
      "        0.3349, 0.2159, 0.3349, 0.3454, 0.3094, 0.3769, 0.3546, 0.3267, 0.3178,\n",
      "        0.3272, 0.3832, 0.2585, 0.2973, 0.3481, 0.2827, 0.2995, 0.3451, 0.3471,\n",
      "        0.3440, 0.3344, 0.3211, 0.3180, 0.2940, 0.3353, 0.3253, 0.3733, 0.3198,\n",
      "        0.2987, 0.1620, 0.3262, 0.3271, 0.3410, 0.3693, 0.3320, 0.3357, 0.2951,\n",
      "        0.3115, 0.3185, 0.3139, 0.2633, 0.3089, 0.3601, 0.2734, 0.3433, 0.3335,\n",
      "        0.3288, 0.2706, 0.2879, 0.3318, 0.3310, 0.3170, 0.2977, 0.3300, 0.3216,\n",
      "        0.3205, 0.3231, 0.3481, 0.3130, 0.2826, 0.2856, 0.3279, 0.3666, 0.3288,\n",
      "        0.3575, 0.3377, 0.2904, 0.3273, 0.3214, 0.3332, 0.3452, 0.1842, 0.3916,\n",
      "        0.3337, 0.2325, 0.3285, 0.3358, 0.2885, 0.3149, 0.3288, 0.2236, 0.3159,\n",
      "        0.2993, 0.3403, 0.3220, 0.3171, 0.2950, 0.2847, 0.3224, 0.3119, 0.2613,\n",
      "        0.3374, 0.3333, 0.3330, 0.2959, 0.4087, 0.2192, 0.2982, 0.4006, 0.3081,\n",
      "        0.3171, 0.2862, 0.2952, 0.3070, 0.3583, 0.3232, 0.3345, 0.3453, 0.3043,\n",
      "        0.3327, 0.3337])\n",
      "tensor([-0.0589, -0.1686, -0.0206,  0.0027, -0.0955, -0.1048,  0.0349, -0.0885,\n",
      "        -0.2053, -0.1764, -0.1224, -0.0364, -0.0785,  0.2088, -0.0403, -0.1820,\n",
      "        -0.1076,  0.2989, -0.0570,  0.2064, -0.0921, -0.1376, -0.1304, -0.1193,\n",
      "        -0.1006, -0.0380, -0.1108, -0.0477, -0.1087,  0.1581, -0.1123, -0.1584,\n",
      "         0.0976, -0.0430, -0.1349, -0.1189, -0.0986, -0.0479, -0.0837, -0.0720,\n",
      "        -0.0836, -0.2442, -0.3376, -0.2124, -0.0693, -0.0651,  0.4979, -0.0811,\n",
      "        -0.1021, -0.0788, -0.1802, -0.1011, -0.1090, -0.0617, -0.0856, -0.0495,\n",
      "        -0.0370,  0.0023, -0.0508, -0.2430,  0.0009, -0.1525, -0.0963, -0.0516,\n",
      "        -0.0473,  0.0884, -0.1028, -0.0907, -0.1086, -0.0379, -0.1030, -0.1609,\n",
      "        -0.0903, -0.0898, -0.1282, -0.0830, -0.0186, -0.0232, -0.0045, -0.2131,\n",
      "        -0.1431, -0.1391, -0.1303, -0.0568, -0.1862, -0.1209, -0.0340, -0.1181,\n",
      "         0.2298, -0.2085, -0.1335,  0.1418, -0.0891, -0.1273,  0.0107, -0.1029,\n",
      "        -0.1025,  0.1562, -0.0937, -0.0657, -0.1245, -0.0451, -0.0707, -0.0447,\n",
      "         0.0715, -0.0484, -0.0312, -0.0437, -0.0927, -0.1465, -0.1151, -0.0183,\n",
      "        -0.1927,  0.2491,  0.0300, -0.1310, -0.0468, -0.0851, -0.0421, -0.0413,\n",
      "        -0.0457, -0.1433, -0.0981, -0.1046, -0.1315, -0.1249, -0.0982, -0.0961])\n",
      "tensor([[[[-0.0074, -0.0098,  0.0028],\n",
      "          [-0.0108,  0.0258,  0.0455],\n",
      "          [-0.0272,  0.0053,  0.0132]],\n",
      "\n",
      "         [[ 0.0354,  0.0251,  0.0078],\n",
      "          [ 0.0040,  0.0199,  0.0274],\n",
      "          [ 0.0353,  0.0355,  0.0133]],\n",
      "\n",
      "         [[ 0.0193, -0.0213, -0.0362],\n",
      "          [-0.0196, -0.0189, -0.0595],\n",
      "          [-0.0218, -0.0077,  0.0039]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0068,  0.0108, -0.0037],\n",
      "          [ 0.0135,  0.0114, -0.0013],\n",
      "          [ 0.0081,  0.0002,  0.0006]],\n",
      "\n",
      "         [[ 0.0077,  0.0077,  0.0044],\n",
      "          [-0.0102, -0.0117, -0.0096],\n",
      "          [-0.0039,  0.0181,  0.0133]],\n",
      "\n",
      "         [[ 0.0124, -0.0269, -0.0120],\n",
      "          [ 0.0268,  0.0264, -0.0215],\n",
      "          [ 0.0129,  0.0028, -0.0054]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0128,  0.0185, -0.0027],\n",
      "          [-0.0168, -0.0123,  0.0355],\n",
      "          [-0.0009,  0.0245,  0.0182]],\n",
      "\n",
      "         [[-0.0067, -0.0207, -0.0144],\n",
      "          [-0.0073,  0.0426,  0.0074],\n",
      "          [ 0.0276,  0.0160,  0.0159]],\n",
      "\n",
      "         [[-0.0229, -0.0206,  0.0236],\n",
      "          [-0.0275, -0.0561, -0.0699],\n",
      "          [ 0.0205,  0.0513,  0.0220]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0272, -0.0390, -0.0395],\n",
      "          [-0.0621, -0.0749, -0.0956],\n",
      "          [-0.0605, -0.0777, -0.0746]],\n",
      "\n",
      "         [[ 0.0287,  0.0293,  0.0287],\n",
      "          [ 0.0079,  0.0471,  0.0146],\n",
      "          [-0.0018,  0.0220,  0.0074]],\n",
      "\n",
      "         [[ 0.0016, -0.0168, -0.0046],\n",
      "          [-0.0081, -0.0255, -0.0524],\n",
      "          [-0.0093, -0.0010, -0.0378]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0047,  0.0061, -0.0071],\n",
      "          [ 0.0236, -0.0931, -0.0793],\n",
      "          [-0.0079, -0.0505, -0.0105]],\n",
      "\n",
      "         [[ 0.0148,  0.0162, -0.0515],\n",
      "          [ 0.0086,  0.0081, -0.0429],\n",
      "          [ 0.0908,  0.0654,  0.0435]],\n",
      "\n",
      "         [[-0.0138, -0.0064,  0.0085],\n",
      "          [ 0.0138, -0.0124,  0.0054],\n",
      "          [ 0.0202, -0.0035,  0.0080]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0009,  0.0185, -0.0306],\n",
      "          [-0.0170,  0.0057, -0.0491],\n",
      "          [-0.0328, -0.0374, -0.0459]],\n",
      "\n",
      "         [[-0.0046,  0.0069, -0.0011],\n",
      "          [-0.0079, -0.0499,  0.0421],\n",
      "          [-0.0752, -0.0048, -0.0058]],\n",
      "\n",
      "         [[ 0.0115, -0.0146,  0.0379],\n",
      "          [ 0.0141,  0.0486,  0.0232],\n",
      "          [ 0.0215, -0.0101,  0.0338]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0264,  0.0040,  0.0075],\n",
      "          [ 0.0636, -0.0320, -0.0018],\n",
      "          [ 0.0262,  0.0076,  0.0495]],\n",
      "\n",
      "         [[-0.0287, -0.0227, -0.0513],\n",
      "          [-0.0260, -0.0487, -0.0140],\n",
      "          [-0.0173, -0.0416, -0.0117]],\n",
      "\n",
      "         [[-0.0350,  0.0356,  0.0347],\n",
      "          [ 0.0183,  0.0436, -0.0263],\n",
      "          [ 0.0178,  0.0356,  0.0113]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0097, -0.0173, -0.0002],\n",
      "          [-0.0199,  0.0820,  0.0129],\n",
      "          [-0.0238, -0.0048,  0.0486]],\n",
      "\n",
      "         [[-0.0244, -0.0258, -0.0353],\n",
      "          [-0.0296, -0.0966, -0.0535],\n",
      "          [-0.0150,  0.0059, -0.0197]],\n",
      "\n",
      "         [[ 0.0068, -0.0368, -0.0255],\n",
      "          [-0.0116, -0.0236, -0.0078],\n",
      "          [ 0.0086,  0.0079, -0.0189]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0329,  0.0297,  0.0111],\n",
      "          [ 0.0390, -0.0090,  0.0226],\n",
      "          [ 0.0078, -0.0280, -0.0230]],\n",
      "\n",
      "         [[ 0.0137,  0.0229, -0.0190],\n",
      "          [ 0.0027,  0.0112,  0.0074],\n",
      "          [ 0.0211,  0.0436,  0.0108]],\n",
      "\n",
      "         [[-0.0237,  0.0221,  0.0004],\n",
      "          [-0.0309,  0.0609,  0.0167],\n",
      "          [-0.0885, -0.0834, -0.0342]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0073, -0.0197,  0.0018],\n",
      "          [ 0.0073, -0.0343, -0.0243],\n",
      "          [-0.0115, -0.0605, -0.0551]],\n",
      "\n",
      "         [[ 0.0030,  0.0026,  0.0171],\n",
      "          [-0.0134, -0.0086,  0.0090],\n",
      "          [ 0.0195,  0.0094,  0.0045]],\n",
      "\n",
      "         [[ 0.0014,  0.0008,  0.0119],\n",
      "          [-0.0024, -0.0107,  0.0126],\n",
      "          [-0.0051, -0.0058, -0.0146]]],\n",
      "\n",
      "\n",
      "        [[[-0.0074,  0.0118, -0.0427],\n",
      "          [ 0.0016, -0.0457, -0.1398],\n",
      "          [-0.0065, -0.0021, -0.0484]],\n",
      "\n",
      "         [[ 0.0075,  0.0527,  0.0388],\n",
      "          [-0.0125,  0.0847,  0.0062],\n",
      "          [ 0.0013, -0.0197, -0.0822]],\n",
      "\n",
      "         [[-0.0249,  0.0166,  0.0169],\n",
      "          [ 0.0087,  0.0214,  0.0117],\n",
      "          [-0.0009,  0.0306,  0.0136]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0078,  0.0220, -0.0175],\n",
      "          [-0.0076, -0.0211, -0.0037],\n",
      "          [ 0.0126, -0.0207, -0.0054]],\n",
      "\n",
      "         [[ 0.0302, -0.0082, -0.0649],\n",
      "          [-0.0238, -0.0954, -0.0530],\n",
      "          [-0.0168, -0.0111,  0.0010]],\n",
      "\n",
      "         [[-0.0245, -0.0847,  0.0251],\n",
      "          [ 0.0106,  0.0387,  0.1400],\n",
      "          [ 0.0155, -0.0095,  0.0041]]]])\n",
      "tensor([0.1454, 0.3270, 0.3113, 0.2538, 0.4086, 0.3937, 0.4400, 0.3108, 0.3406,\n",
      "        0.2168, 0.2170, 0.3857, 0.1971, 0.2692, 0.1663, 0.2454, 0.3232, 0.3686,\n",
      "        0.3893, 0.3264, 0.3875, 0.4707, 0.1958, 0.4717, 0.1673, 0.3938, 0.3044,\n",
      "        0.1929, 0.2175, 0.2119, 0.4230, 0.3683, 0.2455, 0.2229, 0.3370, 0.3229,\n",
      "        0.2688, 0.3557, 0.2581, 0.4031, 0.4492, 0.3642, 0.2599, 0.1881, 0.1359,\n",
      "        0.2958, 0.1913, 0.3065, 0.3981, 0.4102, 0.1874, 0.4516, 0.3340, 0.1628,\n",
      "        0.3599, 0.1624, 0.2886, 0.1358, 0.4491, 0.2694, 0.4823, 0.3393, 0.4764,\n",
      "        0.3155, 0.6005, 0.4654, 0.5264, 0.2991, 0.2992, 0.4621, 0.2614, 0.4247,\n",
      "        0.4662, 0.4249, 0.3345, 0.2655, 0.4048, 0.3605, 0.1782, 0.3833, 0.2823,\n",
      "        0.3843, 0.3307, 0.2151, 0.3317, 0.1458, 0.2771, 0.4917, 0.3199, 0.4222,\n",
      "        0.1559, 0.4884, 0.3267, 0.3440, 0.1608, 0.4855, 0.2677, 0.1616, 0.3221,\n",
      "        0.4243, 0.3661, 0.1893, 0.3400, 0.3648, 0.1779, 0.3544, 0.2852, 0.2437,\n",
      "        0.4472, 0.3011, 0.3997, 0.6173, 0.2794, 0.4867, 0.1502, 0.6021, 0.3604,\n",
      "        0.4696, 0.3711, 0.2388, 0.5347, 0.1509, 0.3213, 0.4394, 0.3229, 0.4329,\n",
      "        0.1489, 0.3702])\n",
      "tensor([ 0.0246,  0.0593,  0.1347, -0.1089, -0.0470, -0.1359, -0.0550,  0.0509,\n",
      "        -0.0613,  0.0916,  0.0031, -0.0274, -0.0539,  0.0177,  0.0432,  0.0074,\n",
      "         0.0548, -0.0321, -0.0224,  0.0142, -0.2150, -0.1160,  0.0486, -0.1141,\n",
      "         0.1066,  0.0355,  0.0140,  0.0177,  0.0781,  0.1331,  0.0139,  0.0447,\n",
      "         0.1063,  0.0528, -0.0539, -0.1160,  0.1055, -0.1591,  0.0100,  0.1197,\n",
      "         0.0170,  0.0929, -0.0675,  0.0987,  0.1034,  0.0501,  0.0297,  0.0281,\n",
      "        -0.0075, -0.0577, -0.0144, -0.1640,  0.1255,  0.0817,  0.0635,  0.0936,\n",
      "         0.0213,  0.0486, -0.1174,  0.0237, -0.2177,  0.0099, -0.1883,  0.0467,\n",
      "        -0.0829,  0.0585, -0.0306,  0.0509,  0.0541, -0.1671,  0.0115, -0.0302,\n",
      "        -0.1393,  0.0115,  0.0428,  0.1189, -0.1289,  0.0479,  0.0474, -0.0625,\n",
      "         0.0009, -0.0144,  0.0909,  0.1342, -0.0338,  0.0560,  0.0848, -0.0467,\n",
      "         0.0228, -0.0097,  0.1360, -0.2625,  0.0088, -0.0553,  0.0383, -0.0720,\n",
      "         0.0907,  0.1612, -0.1076,  0.1011, -0.0519,  0.0838, -0.0704, -0.0806,\n",
      "        -0.0243,  0.0533,  0.1277,  0.1403, -0.0593, -0.0639, -0.0766, -0.1163,\n",
      "         0.0661, -0.1644,  0.0422, -0.2786, -0.1006, -0.0696, -0.0761,  0.0371,\n",
      "        -0.0247,  0.0916, -0.0200, -0.0176,  0.0298, -0.0373,  0.0466, -0.1371])\n",
      "tensor([[[[ 0.0159]],\n",
      "\n",
      "         [[-0.3109]],\n",
      "\n",
      "         [[ 0.0126]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1672]],\n",
      "\n",
      "         [[ 0.0127]],\n",
      "\n",
      "         [[ 0.0132]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0036]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[-0.0083]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0234]],\n",
      "\n",
      "         [[-0.0756]],\n",
      "\n",
      "         [[-0.0126]]],\n",
      "\n",
      "\n",
      "        [[[-0.0419]],\n",
      "\n",
      "         [[ 0.0079]],\n",
      "\n",
      "         [[-0.1662]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0319]],\n",
      "\n",
      "         [[-0.0188]],\n",
      "\n",
      "         [[ 0.0645]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0287]],\n",
      "\n",
      "         [[ 0.0470]],\n",
      "\n",
      "         [[-0.0523]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0474]],\n",
      "\n",
      "         [[ 0.0586]],\n",
      "\n",
      "         [[ 0.0588]]],\n",
      "\n",
      "\n",
      "        [[[-0.0078]],\n",
      "\n",
      "         [[-0.0203]],\n",
      "\n",
      "         [[ 0.0564]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7802]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0259]]],\n",
      "\n",
      "\n",
      "        [[[-0.0283]],\n",
      "\n",
      "         [[-0.0132]],\n",
      "\n",
      "         [[-0.0514]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0236]],\n",
      "\n",
      "         [[-0.0677]],\n",
      "\n",
      "         [[ 0.0268]]]])\n",
      "tensor([ 0.3334,  0.0581,  0.0715,  0.3442,  0.1756,  0.1509,  0.1568,  0.3100,\n",
      "         0.1927,  0.1516,  0.3044,  0.2238,  0.3706,  0.1739,  0.3051,  0.2610,\n",
      "         0.1575,  0.2015,  0.2933,  0.1010,  0.5871,  0.0676,  0.2499,  0.0929,\n",
      "         0.2443,  0.0495,  0.2449,  0.2750,  0.3071,  0.3025,  0.1818,  0.0688,\n",
      "         0.2223,  0.3766,  0.4661,  0.3284,  0.1035,  0.3400,  0.2325,  0.1514,\n",
      "         0.1753,  0.2269,  0.2606,  0.1831,  0.2894,  0.2590,  0.2208,  0.1399,\n",
      "         0.0643,  0.2833,  0.3451,  0.2017,  0.0696,  0.2722,  0.1127,  0.2917,\n",
      "         0.2358,  0.2703,  0.0911,  0.2591,  0.1302,  0.2261,  0.1967,  0.0539,\n",
      "         0.0697,  0.0524,  0.1050,  0.0861,  0.1173,  0.0957,  0.1862,  0.1642,\n",
      "         0.1336,  0.1065,  0.1312,  0.0888,  0.0793,  0.0475,  0.3049,  0.2325,\n",
      "         0.2908,  0.1292,  0.0778,  0.2263,  0.2379,  0.3405,  0.0914,  0.1936,\n",
      "         0.1223,  0.1400,  0.2953,  0.2360,  0.1681,  0.1338,  0.2666,  0.1495,\n",
      "         0.0761,  0.1674,  0.1784,  0.1720,  0.2318,  0.3753,  0.2103,  0.1922,\n",
      "         0.4002,  0.1718,  0.0593,  0.0742,  0.0686,  0.1931,  0.1386,  0.1111,\n",
      "         0.3055,  0.1205,  0.3443,  0.1633,  0.3673,  0.1534,  0.0742,  0.2088,\n",
      "         0.0394,  0.2594,  0.1385, -0.0051,  0.1905,  0.1275,  0.3071,  0.1682])\n",
      "tensor([ 0.0246,  0.0593,  0.1347, -0.1089, -0.0470, -0.1359, -0.0550,  0.0509,\n",
      "        -0.0613,  0.0916,  0.0031, -0.0274, -0.0539,  0.0177,  0.0432,  0.0074,\n",
      "         0.0548, -0.0321, -0.0224,  0.0142, -0.2150, -0.1160,  0.0486, -0.1141,\n",
      "         0.1066,  0.0355,  0.0140,  0.0177,  0.0781,  0.1331,  0.0139,  0.0447,\n",
      "         0.1063,  0.0528, -0.0539, -0.1160,  0.1055, -0.1591,  0.0100,  0.1197,\n",
      "         0.0170,  0.0929, -0.0675,  0.0987,  0.1034,  0.0501,  0.0297,  0.0281,\n",
      "        -0.0075, -0.0577, -0.0144, -0.1640,  0.1255,  0.0817,  0.0635,  0.0936,\n",
      "         0.0213,  0.0486, -0.1174,  0.0237, -0.2177,  0.0099, -0.1883,  0.0467,\n",
      "        -0.0829,  0.0585, -0.0306,  0.0509,  0.0541, -0.1671,  0.0115, -0.0302,\n",
      "        -0.1393,  0.0115,  0.0428,  0.1189, -0.1289,  0.0479,  0.0474, -0.0625,\n",
      "         0.0009, -0.0144,  0.0909,  0.1342, -0.0338,  0.0560,  0.0848, -0.0467,\n",
      "         0.0228, -0.0097,  0.1360, -0.2625,  0.0088, -0.0553,  0.0383, -0.0720,\n",
      "         0.0907,  0.1612, -0.1076,  0.1011, -0.0519,  0.0838, -0.0704, -0.0806,\n",
      "        -0.0243,  0.0533,  0.1277,  0.1403, -0.0593, -0.0639, -0.0766, -0.1163,\n",
      "         0.0661, -0.1644,  0.0422, -0.2786, -0.1006, -0.0696, -0.0761,  0.0371,\n",
      "        -0.0247,  0.0916, -0.0200, -0.0176,  0.0298, -0.0373,  0.0466, -0.1371])\n",
      "tensor([[[[-9.9023e-04, -7.7429e-03, -7.9740e-03],\n",
      "          [ 2.4844e-02,  1.8642e-03,  5.8352e-03],\n",
      "          [ 9.5089e-03, -1.6476e-02,  3.9157e-03]],\n",
      "\n",
      "         [[-2.1488e-02, -1.2330e-03, -1.4281e-02],\n",
      "          [-1.7044e-02,  9.5922e-03,  7.0445e-03],\n",
      "          [ 1.0790e-02, -7.2350e-03, -1.1357e-02]],\n",
      "\n",
      "         [[-1.1126e-03,  3.0388e-02,  2.2247e-02],\n",
      "          [-6.1184e-02, -2.3797e-02,  2.3747e-03],\n",
      "          [ 4.0678e-02, -1.0356e-01, -6.0011e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.5833e-03,  1.1438e-02,  2.0800e-02],\n",
      "          [-1.6565e-02, -3.9587e-02,  1.2594e-02],\n",
      "          [-1.4314e-03, -5.4257e-03,  3.6794e-02]],\n",
      "\n",
      "         [[-1.3687e-02, -2.9514e-02, -1.4745e-02],\n",
      "          [ 2.8299e-02,  2.2096e-02,  3.4839e-03],\n",
      "          [-4.3521e-03, -2.6706e-03,  1.2258e-04]],\n",
      "\n",
      "         [[ 7.6403e-03,  2.0666e-02,  3.7429e-02],\n",
      "          [ 6.9478e-03,  4.3983e-02,  1.7538e-02],\n",
      "          [-9.7797e-03, -2.4789e-02, -1.1349e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.4439e-02,  8.4827e-02, -5.1478e-02],\n",
      "          [ 3.5253e-02, -1.1375e-03, -1.0331e-01],\n",
      "          [-6.4078e-02, -1.2660e-01, -1.2952e-01]],\n",
      "\n",
      "         [[ 1.0628e-03, -1.4083e-02,  4.7109e-03],\n",
      "          [-2.1059e-02, -2.8778e-02,  9.9708e-03],\n",
      "          [ 1.4074e-02,  1.8691e-02,  5.8192e-02]],\n",
      "\n",
      "         [[ 2.2139e-02,  8.9027e-03,  1.4790e-02],\n",
      "          [-1.7497e-02, -5.3924e-03,  2.7834e-02],\n",
      "          [-1.3855e-02, -1.3346e-02,  1.7668e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.8032e-02, -2.3097e-02, -7.1775e-03],\n",
      "          [-3.5089e-02,  1.0861e-02,  1.3640e-02],\n",
      "          [ 6.3449e-04,  9.7476e-03,  7.3670e-03]],\n",
      "\n",
      "         [[-4.4184e-02, -1.6190e-02,  1.2243e-02],\n",
      "          [-4.0349e-02, -1.7894e-02,  2.8911e-02],\n",
      "          [-6.5176e-03, -1.0490e-02,  9.1658e-03]],\n",
      "\n",
      "         [[ 4.3621e-03,  1.3119e-02,  1.8442e-03],\n",
      "          [ 1.1555e-02, -1.3031e-02, -9.5657e-03],\n",
      "          [-2.3314e-02,  1.1609e-03,  2.6771e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.1180e-02, -6.2213e-03,  1.7609e-03],\n",
      "          [-4.7424e-03,  1.1101e-02,  1.1296e-02],\n",
      "          [-1.4529e-02,  2.9843e-02,  2.4383e-03]],\n",
      "\n",
      "         [[ 6.9183e-03,  9.2937e-03,  3.0078e-02],\n",
      "          [-4.2612e-03,  4.9560e-03, -4.7338e-03],\n",
      "          [ 3.1360e-02,  1.9035e-03, -4.7242e-03]],\n",
      "\n",
      "         [[-3.6726e-02,  5.7285e-03,  1.3919e-01],\n",
      "          [-4.2992e-02,  9.4023e-04,  7.7141e-02],\n",
      "          [-5.0050e-02, -4.9479e-03,  2.4693e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7203e-02,  7.4712e-03, -4.2659e-02],\n",
      "          [-8.1729e-03, -9.2536e-02, -5.4934e-03],\n",
      "          [-2.5927e-02,  8.3993e-04,  7.4632e-02]],\n",
      "\n",
      "         [[ 1.8076e-02,  4.5272e-03, -1.3757e-02],\n",
      "          [-1.8939e-02, -3.2739e-02, -2.9666e-02],\n",
      "          [-2.0608e-02, -4.6167e-03,  1.3080e-03]],\n",
      "\n",
      "         [[-1.2078e-02, -2.0285e-03, -1.6998e-02],\n",
      "          [-3.4805e-02, -4.9195e-02, -3.1973e-02],\n",
      "          [-2.1021e-02, -5.1164e-03, -4.8522e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.1791e-02,  2.2948e-02,  1.0390e-02],\n",
      "          [-1.2628e-02, -2.9320e-03,  4.2645e-03],\n",
      "          [-2.1707e-02, -1.0856e-02,  1.6094e-02]],\n",
      "\n",
      "         [[-1.4525e-03, -1.0131e-02, -4.6862e-04],\n",
      "          [ 2.2130e-02,  2.2736e-02,  5.0183e-03],\n",
      "          [-6.0125e-02, -4.3150e-02, -4.4480e-02]],\n",
      "\n",
      "         [[ 3.0761e-03,  3.4396e-03,  6.0877e-03],\n",
      "          [-1.3683e-02,  4.0576e-03, -2.6544e-02],\n",
      "          [ 6.8231e-02,  6.3474e-02, -9.3660e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8752e-02,  1.9400e-02,  4.1691e-02],\n",
      "          [ 8.7770e-03,  8.2394e-04,  1.8619e-02],\n",
      "          [ 1.8796e-02,  6.2238e-02, -2.3801e-02]],\n",
      "\n",
      "         [[-2.9788e-02, -3.4598e-02, -2.5225e-02],\n",
      "          [ 8.4234e-03, -2.3222e-02, -9.4612e-03],\n",
      "          [ 6.9035e-03,  6.9737e-02, -1.3359e-02]],\n",
      "\n",
      "         [[ 2.6981e-03, -4.3182e-02, -1.6731e-02],\n",
      "          [ 2.5812e-02, -7.2025e-02, -6.5399e-02],\n",
      "          [ 4.6257e-02,  2.9469e-02, -1.5811e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.1079e-02,  3.8220e-02,  8.3305e-03],\n",
      "          [-5.9912e-03,  3.5584e-02, -1.7534e-03],\n",
      "          [ 1.8735e-02,  7.0859e-03, -3.5151e-03]],\n",
      "\n",
      "         [[-4.5937e-02, -7.4695e-02, -5.3608e-02],\n",
      "          [-8.6266e-03,  9.0894e-03, -3.0345e-02],\n",
      "          [-2.8158e-02, -2.1204e-02, -8.4730e-03]],\n",
      "\n",
      "         [[-7.1772e-02, -6.8582e-02,  2.5544e-02],\n",
      "          [ 5.0363e-02,  2.5269e-02,  5.6668e-02],\n",
      "          [ 2.6238e-03,  1.3871e-03, -8.4692e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9644e-02,  1.0896e-02, -3.0402e-02],\n",
      "          [ 1.5095e-03,  5.0455e-02,  1.5597e-02],\n",
      "          [-2.1015e-02, -1.0757e-02, -3.4942e-02]],\n",
      "\n",
      "         [[-2.7573e-02,  2.9707e-02, -2.9490e-02],\n",
      "          [ 2.3301e-03, -3.9011e-02,  6.8010e-03],\n",
      "          [ 4.4006e-02,  3.5397e-02,  7.9087e-02]],\n",
      "\n",
      "         [[-2.7480e-02,  5.0337e-02,  1.4290e-02],\n",
      "          [-5.2482e-02, -4.7748e-03,  1.2988e-02],\n",
      "          [-1.8935e-02, -3.0808e-02, -1.7583e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2280e-02,  4.7408e-02,  3.4054e-02],\n",
      "          [ 2.1445e-02,  3.8987e-03,  4.6985e-04],\n",
      "          [ 1.5159e-02,  8.2067e-03,  3.2426e-02]],\n",
      "\n",
      "         [[ 9.2653e-03,  2.3661e-02,  4.2089e-02],\n",
      "          [ 2.1976e-02,  4.6128e-02,  1.1402e-02],\n",
      "          [ 7.2843e-03,  5.2285e-02,  8.6340e-03]],\n",
      "\n",
      "         [[ 1.4022e-02,  1.2800e-02,  3.5398e-02],\n",
      "          [-4.4398e-02,  1.7399e-02, -1.5838e-02],\n",
      "          [ 3.1712e-02,  5.8679e-02, -9.3244e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.8399e-03,  7.8628e-03, -5.6169e-04],\n",
      "          [ 8.0402e-03,  1.7392e-02,  7.8734e-03],\n",
      "          [-1.7713e-02, -4.5957e-02, -9.8762e-03]],\n",
      "\n",
      "         [[-9.7569e-03, -7.5795e-03, -2.4627e-02],\n",
      "          [-8.2454e-03,  6.3065e-02, -3.2954e-03],\n",
      "          [-7.7549e-03, -1.3404e-04, -8.1337e-03]],\n",
      "\n",
      "         [[ 1.7664e-02,  1.0114e-02,  4.2687e-03],\n",
      "          [-3.7950e-03,  2.6715e-02,  2.0121e-02],\n",
      "          [ 1.6868e-02, -6.6515e-03, -1.1107e-02]]]])\n",
      "tensor([0.3323, 0.2908, 0.3246, 0.3435, 0.3011, 0.3054, 0.3041, 0.3539, 0.2862,\n",
      "        0.3601, 0.2970, 0.3381, 0.2565, 0.3276, 0.3030, 0.4085, 0.3519, 0.4218,\n",
      "        0.3055, 0.2551, 0.3425, 0.3215, 0.3366, 0.2700, 0.2849, 0.3954, 0.3166,\n",
      "        0.3286, 0.3515, 0.3953, 0.2768, 0.3625, 0.1988, 0.2717, 0.3355, 0.2797,\n",
      "        0.2510, 0.3832, 0.3266, 0.3263, 0.3681, 0.3401, 0.3651, 0.3391, 0.3071,\n",
      "        0.3231, 0.3691, 0.2410, 0.3536, 0.3189, 0.3238, 0.3611, 0.3086, 0.3309,\n",
      "        0.3886, 0.4362, 0.4550, 0.2962, 0.3071, 0.3386, 0.3317, 0.3228, 0.2393,\n",
      "        0.3147, 0.2738, 0.3218, 0.3198, 0.3411, 0.3611, 0.2833, 0.3035, 0.3183,\n",
      "        0.3146, 0.3890, 0.2607, 0.3479, 0.3236, 0.3709, 0.2592, 0.3742, 0.2555,\n",
      "        0.2966, 0.3505, 0.3165, 0.2808, 0.2660, 0.2817, 0.4795, 0.3372, 0.2723,\n",
      "        0.2955, 0.3225, 0.2470, 0.3160, 0.3515, 0.3131, 0.3372, 0.2837, 0.3540,\n",
      "        0.2897, 0.2490, 0.3019, 0.3114, 0.3510, 0.3022, 0.3617, 0.2859, 0.2831,\n",
      "        0.3243, 0.2769, 0.3314, 0.2394, 0.2932, 0.2788, 0.2686, 0.3194, 0.3542,\n",
      "        0.2683, 0.2955, 0.2924, 0.3538, 0.4256, 0.3603, 0.3013, 0.2763, 0.4354,\n",
      "        0.3991, 0.2694])\n",
      "tensor([-0.1735, -0.2337, -0.3383, -0.0806, -0.1920, -0.0621, -0.1885, -0.2830,\n",
      "        -0.1680, -0.1796, -0.2645, -0.1983, -0.1183, -0.2432, -0.1706, -0.3090,\n",
      "        -0.2661, -0.4040, -0.1949, -0.1392, -0.2449, -0.1242, -0.2012, -0.1901,\n",
      "        -0.1014, -0.3468, -0.2245, -0.3272, -0.3057, -0.3289, -0.1532, -0.1967,\n",
      "        -0.0667, -0.3281, -0.1418, -0.1527, -0.0987, -0.3243, -0.2252, -0.3462,\n",
      "        -0.2284, -0.2263, -0.1810, -0.1564, -0.1730, -0.1507, -0.2913, -0.1643,\n",
      "        -0.1998, -0.1532, -0.2211, -0.2247, -0.0913, -0.1563, -0.2453, -0.4854,\n",
      "        -0.4428, -0.1021, -0.1615, -0.2125, -0.2239, -0.1952, -0.0447, -0.1733,\n",
      "        -0.1178, -0.4775, -0.2110, -0.2305, -0.1795, -0.1582, -0.2008, -0.2041,\n",
      "        -0.1974, -0.2750, -0.0395, -0.2161, -0.2786, -0.2626, -0.0997, -0.2953,\n",
      "        -0.1431, -0.1448, -0.1894, -0.1283, -0.1807, -0.1144, -0.1308, -0.4154,\n",
      "        -0.2324, -0.1376, -0.1154, -0.2099, -0.0966, -0.1669, -0.3835, -0.2545,\n",
      "        -0.1603, -0.1904, -0.2420, -0.1658, -0.1133, -0.1498, -0.1213, -0.2318,\n",
      "        -0.2017, -0.3827, -0.1491, -0.1174, -0.1261, -0.2031, -0.1832, -0.2274,\n",
      "        -0.1281, -0.2557, -0.1400, -0.0723, -0.2212, -0.1486, -0.2914, -0.1116,\n",
      "        -0.2194, -0.4898, -0.3693, -0.1437, -0.1232, -0.3723, -0.6794, -0.1536])\n",
      "tensor([[[[-1.6153e-02,  5.0134e-03, -9.0186e-04],\n",
      "          [-8.8386e-03, -1.9390e-02, -2.4174e-02],\n",
      "          [ 6.3052e-03,  1.0245e-02, -1.3816e-02]],\n",
      "\n",
      "         [[-1.0979e-02,  2.6164e-03,  2.3656e-02],\n",
      "          [-1.7687e-02,  1.9861e-02,  6.4150e-02],\n",
      "          [ 6.0224e-03,  7.6342e-02,  1.0215e-01]],\n",
      "\n",
      "         [[-8.1113e-03,  6.8414e-03,  2.5436e-02],\n",
      "          [-8.0696e-03,  9.2929e-03,  8.2899e-03],\n",
      "          [ 7.7306e-03,  1.2159e-02,  7.1625e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5175e-02,  6.2196e-03,  2.1798e-02],\n",
      "          [-1.5199e-02, -8.5439e-02, -2.4713e-02],\n",
      "          [-1.8460e-02, -4.9767e-02, -1.6818e-03]],\n",
      "\n",
      "         [[ 3.0728e-02,  3.9962e-02,  3.1253e-02],\n",
      "          [-1.8738e-02, -6.7510e-02, -2.7649e-02],\n",
      "          [ 2.8429e-02,  3.1854e-02,  1.0543e-02]],\n",
      "\n",
      "         [[-1.8320e-02, -1.5854e-02, -1.0685e-02],\n",
      "          [-2.7442e-02, -3.0616e-02, -1.0485e-02],\n",
      "          [-1.5122e-02, -1.0595e-02, -2.5322e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6868e-03,  3.0996e-02,  4.2763e-02],\n",
      "          [ 4.6537e-02,  4.8606e-02,  2.3800e-03],\n",
      "          [ 1.6654e-02,  1.2900e-02, -1.8230e-02]],\n",
      "\n",
      "         [[-1.0441e-02, -1.5934e-03, -1.6128e-02],\n",
      "          [-1.2799e-02,  4.9570e-03, -1.4585e-02],\n",
      "          [-2.3553e-02, -3.7023e-03, -1.4399e-02]],\n",
      "\n",
      "         [[ 1.0338e-02, -1.7560e-02, -3.3046e-02],\n",
      "          [-3.2090e-02, -5.9258e-03,  2.0201e-03],\n",
      "          [-4.1428e-02,  4.9121e-03,  1.6906e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9525e-02, -4.6498e-02, -5.9916e-02],\n",
      "          [-2.6670e-02, -1.9079e-02, -2.9419e-02],\n",
      "          [-3.9683e-03,  1.9405e-02,  7.3317e-03]],\n",
      "\n",
      "         [[ 1.4293e-02,  1.5643e-02,  5.8117e-04],\n",
      "          [ 5.1493e-03,  7.4332e-03, -3.6928e-03],\n",
      "          [-1.3522e-02, -8.5536e-03, -2.1259e-03]],\n",
      "\n",
      "         [[-3.0908e-02, -1.9839e-02, -1.9375e-02],\n",
      "          [-1.0368e-02, -2.4294e-02,  2.4103e-04],\n",
      "          [-1.9275e-02, -2.9707e-02, -1.5623e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.9212e-02, -2.9588e-02,  8.8023e-02],\n",
      "          [ 4.7453e-03,  4.3564e-02,  9.3115e-02],\n",
      "          [ 7.4083e-02,  4.2868e-02, -5.1033e-02]],\n",
      "\n",
      "         [[ 6.6992e-03,  2.1676e-02, -5.4254e-04],\n",
      "          [ 1.9286e-02,  1.0920e-02, -4.5440e-03],\n",
      "          [ 3.1075e-02, -1.7168e-03, -2.7603e-02]],\n",
      "\n",
      "         [[ 6.0096e-02, -2.9359e-02, -5.8911e-02],\n",
      "          [-1.9133e-02, -8.1624e-02, -2.2553e-02],\n",
      "          [ 1.1597e-02,  2.5092e-02,  1.2130e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.4307e-03, -2.3130e-02,  9.6233e-03],\n",
      "          [-4.3785e-02, -2.6735e-02,  2.1993e-02],\n",
      "          [-3.5919e-02, -4.1009e-02, -2.1860e-02]],\n",
      "\n",
      "         [[ 3.3705e-02,  6.2938e-02,  4.3502e-02],\n",
      "          [ 1.1111e-03,  1.9243e-02, -1.9707e-03],\n",
      "          [-1.1493e-02, -5.3445e-02, -9.6676e-03]],\n",
      "\n",
      "         [[-2.6664e-03, -2.6954e-02, -1.7667e-02],\n",
      "          [-8.3382e-03,  8.9920e-03,  8.1260e-04],\n",
      "          [-2.6832e-02, -3.5991e-02, -4.2495e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.8876e-03, -2.2728e-02, -4.2991e-03],\n",
      "          [-9.2231e-03, -3.4333e-02, -1.3392e-02],\n",
      "          [-1.2774e-02, -1.1435e-02,  1.5617e-02]],\n",
      "\n",
      "         [[ 1.0703e-02,  1.2792e-02,  2.2662e-02],\n",
      "          [ 7.3185e-03, -1.7847e-02,  1.0674e-02],\n",
      "          [-1.5936e-02, -1.9318e-02,  2.1768e-02]],\n",
      "\n",
      "         [[-7.3009e-03,  3.0234e-02, -1.1899e-02],\n",
      "          [-2.6099e-02,  3.7452e-03,  3.2776e-02],\n",
      "          [-3.3101e-02, -7.1923e-03,  1.6559e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2818e-02, -1.0021e-01, -4.7012e-02],\n",
      "          [ 2.8293e-03,  4.1410e-02, -1.1391e-02],\n",
      "          [-1.1152e-02, -5.5861e-03,  1.9968e-02]],\n",
      "\n",
      "         [[-2.3932e-02, -3.0687e-02, -1.1756e-03],\n",
      "          [ 1.5311e-03, -3.5002e-02, -2.4414e-02],\n",
      "          [-8.7575e-03, -7.7842e-02, -3.8842e-02]],\n",
      "\n",
      "         [[ 2.6107e-02,  1.5406e-02,  1.7569e-02],\n",
      "          [-1.5130e-02, -4.8687e-03,  3.0773e-03],\n",
      "          [-1.3470e-02, -9.3201e-03, -4.8982e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.0228e-02, -3.0006e-02, -9.8419e-03],\n",
      "          [-3.8676e-02, -3.3481e-02, -7.4265e-03],\n",
      "          [-2.8935e-02, -3.2037e-02,  2.9245e-03]],\n",
      "\n",
      "         [[-1.2900e-02,  3.8046e-03,  1.5940e-02],\n",
      "          [-2.4030e-02,  2.0666e-03,  5.7250e-03],\n",
      "          [ 6.9989e-03,  1.2192e-02,  1.5406e-02]],\n",
      "\n",
      "         [[-1.5018e-02, -9.0988e-03,  2.4450e-02],\n",
      "          [ 1.0039e-02,  1.2561e-02,  2.6997e-02],\n",
      "          [ 2.9556e-02,  1.9463e-02, -2.6584e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8481e-02,  3.9417e-04,  9.9768e-03],\n",
      "          [-4.5447e-03,  1.2307e-02,  3.5507e-02],\n",
      "          [-1.1873e-03, -2.6185e-03,  1.1547e-02]],\n",
      "\n",
      "         [[ 4.6292e-03, -1.3690e-02, -1.0171e-02],\n",
      "          [ 1.2104e-02,  1.6793e-02,  1.3003e-02],\n",
      "          [ 1.3328e-03,  3.4701e-03,  1.7323e-02]],\n",
      "\n",
      "         [[-8.7332e-05,  5.8646e-03, -3.5117e-03],\n",
      "          [ 3.8112e-03, -7.1828e-03, -1.1407e-02],\n",
      "          [ 1.9705e-02,  2.0556e-02,  5.7084e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6998e-02,  3.2616e-02, -9.4535e-04],\n",
      "          [-2.9484e-02, -2.3441e-02, -2.8085e-02],\n",
      "          [-2.5451e-02,  3.9048e-02,  3.6686e-02]],\n",
      "\n",
      "         [[-1.8732e-02, -1.5352e-02,  1.1149e-02],\n",
      "          [-2.1324e-03, -2.3177e-02,  1.7628e-02],\n",
      "          [-4.0012e-03,  1.5463e-02,  9.2496e-03]],\n",
      "\n",
      "         [[-2.9346e-02,  7.7071e-03, -5.6520e-03],\n",
      "          [-2.3611e-02, -1.9390e-03,  2.0221e-02],\n",
      "          [ 8.0955e-03, -2.3268e-02, -2.8827e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3532e-02, -2.9092e-02, -4.0045e-02],\n",
      "          [ 2.6530e-03, -2.0568e-02,  1.3075e-02],\n",
      "          [ 1.6061e-02, -5.5725e-02, -4.9167e-02]],\n",
      "\n",
      "         [[-7.9132e-03,  2.1466e-02,  2.0913e-02],\n",
      "          [-1.7259e-02, -2.5851e-02,  2.7177e-03],\n",
      "          [-4.6532e-02, -2.4846e-02, -1.9911e-02]],\n",
      "\n",
      "         [[-5.0350e-02, -2.5574e-02,  1.7763e-02],\n",
      "          [-3.4474e-02,  5.5247e-03, -2.7754e-02],\n",
      "          [-2.0743e-02, -2.2332e-02, -4.3512e-02]]]])\n",
      "tensor([0.1194, 0.1625, 0.3084, 0.2931, 0.2957, 0.5263, 0.4038, 0.2024, 0.3401,\n",
      "        0.1982, 0.2559, 0.2311, 0.1630, 0.2891, 0.2248, 0.2311, 0.2417, 0.2187,\n",
      "        0.1922, 0.3103, 0.2015, 0.4802, 0.2481, 0.3898, 0.3204, 0.4035, 0.2617,\n",
      "        0.1551, 0.2256, 0.2117, 0.2708, 0.3537, 0.2505, 0.1843, 0.2465, 0.6501,\n",
      "        0.3898, 0.4289, 0.1799, 0.1604, 0.1775, 0.3600, 0.2694, 0.1283, 0.1662,\n",
      "        0.1716, 0.1837, 0.1710, 0.4178, 0.3249, 0.1759, 0.4717, 0.4115, 0.1995,\n",
      "        0.2025, 0.1492, 0.2860, 0.1072, 0.3649, 0.1906, 0.5369, 0.2400, 0.4411,\n",
      "        0.1702, 0.1993, 0.2045, 0.1972, 0.4041, 0.3034, 0.6168, 0.2284, 0.3228,\n",
      "        0.4547, 0.4370, 0.1570, 0.4057, 0.5791, 0.2338, 0.1586, 0.3130, 0.2201,\n",
      "        0.3195, 0.1166, 0.2517, 0.2184, 0.0989, 0.3116, 0.2613, 0.3277, 0.1778,\n",
      "        0.2718, 0.4174, 0.5140, 0.2136, 0.1905, 0.2898, 0.2472, 0.1341, 0.6212,\n",
      "        0.1810, 0.2394, 0.1417, 0.1759, 0.2827, 0.1987, 0.3775, 0.3749, 0.1274,\n",
      "        0.3656, 0.4305, 0.4212, 0.2673, 0.2016, 0.5098, 0.1449, 0.4408, 0.3583,\n",
      "        0.2503, 0.5682, 0.2518, 0.1392, 0.0617, 0.3406, 0.1313, 0.4586, 0.2914,\n",
      "        0.1326, 0.3915])\n",
      "tensor([-0.1403, -0.0889, -0.4147, -0.2264, -0.0737, -0.3534, -0.3379, -0.0752,\n",
      "        -0.1791,  0.0448, -0.2842, -0.1765, -0.1591, -0.0675, -0.1543, -0.1061,\n",
      "        -0.2334, -0.0981, -0.0908, -0.0567, -0.1908, -0.2055, -0.2704, -0.1883,\n",
      "        -0.3570, -0.1125, -0.1632, -0.0211, -0.1687, -0.2124, -0.1713, -0.0872,\n",
      "        -0.2194, -0.1888, -0.2954, -0.4570, -0.0226, -0.0527,  0.0406, -0.0609,\n",
      "        -0.0456, -0.1176, -0.0145,  0.0318, -0.2046, -0.0953, -0.0496, -0.1051,\n",
      "        -0.0793, -0.1933, -0.1467, -0.3215, -0.3257, -0.2287, -0.0356, -0.1869,\n",
      "        -0.1932, -0.0771,  0.2768, -0.0656, -0.0895, -0.2548, -0.2365,  0.0021,\n",
      "        -0.0987, -0.3178,  0.1613,  0.0006, -0.2347, -0.4150, -0.1310, -0.3142,\n",
      "        -0.2582, -0.5400,  0.0772, -0.2546, -0.4454, -0.0262, -0.0937, -0.2201,\n",
      "        -0.2044, -0.0155, -0.0893, -0.2167,  0.1112, -0.0619, -0.1217, -0.1593,\n",
      "        -0.1317, -0.1717, -0.3729, -0.3354, -0.3414,  0.0358, -0.2067, -0.1087,\n",
      "         0.0141, -0.0338, -0.2129, -0.1122, -0.1627, -0.2000,  0.0908, -0.0041,\n",
      "        -0.1313, -0.2942,  0.0160, -0.1065, -0.1289, -0.1699, -0.1721, -0.1809,\n",
      "        -0.2295, -0.3611, -0.1746, -0.3540, -0.1554, -0.2709, -0.2607,  0.0084,\n",
      "        -0.0311, -0.0022, -0.0831,  0.0380, -0.4893, -0.2749,  0.1245, -0.1272])\n",
      "tensor([[[[-0.0159, -0.0166, -0.0159],\n",
      "          [-0.0053,  0.0151,  0.0099],\n",
      "          [-0.0149,  0.0004, -0.0114]],\n",
      "\n",
      "         [[-0.0095, -0.0186, -0.0061],\n",
      "          [ 0.0098, -0.0123, -0.0053],\n",
      "          [ 0.0071, -0.0161, -0.0071]],\n",
      "\n",
      "         [[-0.0227, -0.0377, -0.0337],\n",
      "          [-0.0316, -0.0580, -0.0391],\n",
      "          [-0.0346, -0.0388, -0.0157]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0049,  0.0080,  0.0144],\n",
      "          [-0.0015,  0.0242,  0.0056],\n",
      "          [-0.0044,  0.0062,  0.0069]],\n",
      "\n",
      "         [[ 0.0160, -0.0120, -0.0013],\n",
      "          [ 0.0096,  0.0057,  0.0016],\n",
      "          [-0.0099, -0.0136, -0.0064]],\n",
      "\n",
      "         [[ 0.0534,  0.0464,  0.0248],\n",
      "          [ 0.0341, -0.0029, -0.0041],\n",
      "          [-0.0140, -0.0046, -0.0142]]],\n",
      "\n",
      "\n",
      "        [[[-0.0012, -0.0186, -0.0345],\n",
      "          [ 0.0050, -0.0117, -0.0333],\n",
      "          [ 0.0060, -0.0162, -0.0175]],\n",
      "\n",
      "         [[ 0.0107,  0.0140, -0.0192],\n",
      "          [ 0.0029,  0.0126,  0.0073],\n",
      "          [-0.0168, -0.0187, -0.0014]],\n",
      "\n",
      "         [[-0.0219,  0.0063,  0.0159],\n",
      "          [-0.0035,  0.0057,  0.0311],\n",
      "          [ 0.0023,  0.0032,  0.0175]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0162, -0.0078,  0.0077],\n",
      "          [-0.0015, -0.0122, -0.0093],\n",
      "          [ 0.0009, -0.0022,  0.0074]],\n",
      "\n",
      "         [[ 0.0029, -0.0236,  0.0058],\n",
      "          [ 0.0143, -0.0169, -0.0062],\n",
      "          [-0.0077, -0.0323, -0.0337]],\n",
      "\n",
      "         [[ 0.0087,  0.0140,  0.0081],\n",
      "          [-0.0034,  0.0105,  0.0150],\n",
      "          [ 0.0189,  0.0309,  0.0256]]],\n",
      "\n",
      "\n",
      "        [[[-0.0358, -0.0226, -0.0144],\n",
      "          [-0.0075, -0.0221,  0.0111],\n",
      "          [-0.0036, -0.0148, -0.0164]],\n",
      "\n",
      "         [[-0.0146, -0.0307, -0.0204],\n",
      "          [-0.0285, -0.0453, -0.0579],\n",
      "          [ 0.0288, -0.0152, -0.0245]],\n",
      "\n",
      "         [[ 0.0174,  0.0199, -0.0046],\n",
      "          [ 0.0178,  0.0236,  0.0136],\n",
      "          [ 0.0293,  0.0434,  0.0186]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0079, -0.0122, -0.0158],\n",
      "          [-0.0107, -0.0314, -0.0075],\n",
      "          [-0.0070, -0.0173, -0.0308]],\n",
      "\n",
      "         [[-0.0103, -0.0118, -0.0171],\n",
      "          [-0.0264, -0.0015,  0.0280],\n",
      "          [-0.0089,  0.0054,  0.0097]],\n",
      "\n",
      "         [[ 0.0145, -0.0315, -0.0197],\n",
      "          [-0.0149, -0.0177,  0.0077],\n",
      "          [ 0.0125,  0.0071, -0.0062]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0053,  0.0255,  0.0071],\n",
      "          [ 0.0197,  0.0270,  0.0420],\n",
      "          [ 0.0412,  0.0223,  0.0350]],\n",
      "\n",
      "         [[ 0.0026,  0.0041,  0.0106],\n",
      "          [ 0.0046,  0.0203,  0.0081],\n",
      "          [ 0.0145, -0.0030, -0.0197]],\n",
      "\n",
      "         [[-0.0040,  0.0255,  0.0023],\n",
      "          [-0.0152,  0.0260,  0.0083],\n",
      "          [-0.0021,  0.0269,  0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0055, -0.0049,  0.0183],\n",
      "          [ 0.0085,  0.0023,  0.0077],\n",
      "          [ 0.0082,  0.0096,  0.0215]],\n",
      "\n",
      "         [[-0.0177,  0.0099, -0.0129],\n",
      "          [-0.0127,  0.0096, -0.0124],\n",
      "          [ 0.0090,  0.0493,  0.0362]],\n",
      "\n",
      "         [[ 0.0123,  0.0184, -0.0178],\n",
      "          [ 0.0058, -0.0058, -0.0117],\n",
      "          [ 0.0034, -0.0103, -0.0425]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0316,  0.0765,  0.0443],\n",
      "          [ 0.0940,  0.1480,  0.1510],\n",
      "          [ 0.0665,  0.1386,  0.1132]],\n",
      "\n",
      "         [[-0.0282, -0.0041, -0.0200],\n",
      "          [-0.0193, -0.0012,  0.0107],\n",
      "          [-0.0165, -0.0028,  0.0008]],\n",
      "\n",
      "         [[-0.0122, -0.0322, -0.0153],\n",
      "          [-0.0009, -0.0123, -0.0039],\n",
      "          [-0.0193, -0.0107, -0.0211]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0030,  0.0066, -0.0003],\n",
      "          [ 0.0146, -0.0167,  0.0142],\n",
      "          [ 0.0227,  0.0120,  0.0072]],\n",
      "\n",
      "         [[ 0.0039, -0.0197,  0.0111],\n",
      "          [-0.0251, -0.0384, -0.0447],\n",
      "          [-0.0359, -0.0981, -0.0684]],\n",
      "\n",
      "         [[-0.0085,  0.0023,  0.0031],\n",
      "          [ 0.0038,  0.0182,  0.0069],\n",
      "          [ 0.0089,  0.0080,  0.0126]]],\n",
      "\n",
      "\n",
      "        [[[-0.0130, -0.0090,  0.0011],\n",
      "          [-0.0260, -0.0195, -0.0094],\n",
      "          [ 0.0048,  0.0024,  0.0106]],\n",
      "\n",
      "         [[-0.0025, -0.0140, -0.0286],\n",
      "          [-0.0024,  0.0011, -0.0233],\n",
      "          [ 0.0123,  0.0008,  0.0014]],\n",
      "\n",
      "         [[-0.0490, -0.0439, -0.0579],\n",
      "          [-0.0359, -0.0365, -0.0386],\n",
      "          [-0.0410, -0.0333, -0.0137]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0118, -0.0081, -0.0158],\n",
      "          [-0.0272, -0.0285,  0.0075],\n",
      "          [-0.0244,  0.0139,  0.0061]],\n",
      "\n",
      "         [[-0.0273,  0.0197,  0.0222],\n",
      "          [-0.0376,  0.0208,  0.0187],\n",
      "          [-0.0446, -0.0044, -0.0168]],\n",
      "\n",
      "         [[ 0.0199,  0.0268,  0.0120],\n",
      "          [ 0.0203,  0.0215,  0.0025],\n",
      "          [ 0.0002, -0.0076, -0.0196]]]])\n",
      "tensor([0.2856, 0.2425, 0.3032, 0.3168, 0.3011, 0.3475, 0.3076, 0.3105, 0.3646,\n",
      "        0.3255, 0.2195, 0.3167, 0.2674, 0.3104, 0.3026, 0.3443, 0.2915, 0.3379,\n",
      "        0.2887, 0.2996, 0.3588, 0.3164, 0.2882, 0.2917, 0.3492, 0.3749, 0.3587,\n",
      "        0.3166, 0.2756, 0.2978, 0.3364, 0.2893, 0.3106, 0.2506, 0.3460, 0.3621,\n",
      "        0.2570, 0.3695, 0.2935, 0.3286, 0.3243, 0.3188, 0.3093, 0.3314, 0.3550,\n",
      "        0.2978, 0.2737, 0.3023, 0.3179, 0.2831, 0.3065, 0.3390, 0.3053, 0.3099,\n",
      "        0.3017, 0.3472, 0.3034, 0.2935, 0.3352, 0.3676, 0.3163, 0.3404, 0.3078,\n",
      "        0.2819, 0.3794, 0.3083, 0.2778, 0.3363, 0.2284, 0.3259, 0.2790, 0.3072,\n",
      "        0.2975, 0.3847, 0.3372, 0.2253, 0.2827, 0.3737, 0.2796, 0.3485, 0.3879,\n",
      "        0.3288, 0.3340, 0.3335, 0.2756, 0.3500, 0.2897, 0.2798, 0.2907, 0.3220,\n",
      "        0.3824, 0.3522, 0.3278, 0.3689, 0.3147, 0.3600, 0.3123, 0.2519, 0.2355,\n",
      "        0.3211, 0.3203, 0.3345, 0.2768, 0.3341, 0.3153, 0.3175, 0.2224, 0.2956,\n",
      "        0.3206, 0.2658, 0.3662, 0.2715, 0.3655, 0.3427, 0.2820, 0.2754, 0.4669,\n",
      "        0.3090, 0.3468, 0.3144, 0.3220, 0.2765, 0.3301, 0.3219, 0.3152, 0.2813,\n",
      "        0.2497, 0.3514, 0.3264, 0.3014, 0.2734, 0.3522, 0.3831, 0.3028, 0.2940,\n",
      "        0.2825, 0.3099, 0.2373, 0.2705, 0.4189, 0.2985, 0.3841, 0.2754, 0.3091,\n",
      "        0.3169, 0.2824, 0.2749, 0.3493, 0.4018, 0.3108, 0.2176, 0.2821, 0.3199,\n",
      "        0.3358, 0.2468, 0.3332, 0.2876, 0.2964, 0.2385, 0.3451, 0.3081, 0.2760,\n",
      "        0.2533, 0.2576, 0.3092, 0.2950, 0.3089, 0.3113, 0.3475, 0.3172, 0.2474,\n",
      "        0.3371, 0.3450, 0.3189, 0.3150, 0.3008, 0.2694, 0.3730, 0.3235, 0.2988,\n",
      "        0.2812, 0.3245, 0.3630, 0.2843, 0.3533, 0.3451, 0.3244, 0.3524, 0.3118,\n",
      "        0.3429, 0.3215, 0.2748, 0.3287, 0.3656, 0.2901, 0.2523, 0.3284, 0.2523,\n",
      "        0.3426, 0.2851, 0.2918, 0.2497, 0.5159, 0.3026, 0.2743, 0.2379, 0.3524,\n",
      "        0.3394, 0.2264, 0.2652, 0.3759, 0.3777, 0.2459, 0.3046, 0.3067, 0.3775,\n",
      "        0.2976, 0.3552, 0.2696, 0.2649, 0.2872, 0.2985, 0.2867, 0.3676, 0.3494,\n",
      "        0.3823, 0.3246, 0.3567, 0.2662, 0.3357, 0.2935, 0.2987, 0.2664, 0.3019,\n",
      "        0.3175, 0.2436, 0.3274, 0.2764, 0.2466, 0.2876, 0.3060, 0.3157, 0.3329,\n",
      "        0.2984, 0.2961, 0.3309, 0.3729, 0.3238, 0.3491, 0.3342, 0.3037, 0.3578,\n",
      "        0.2849, 0.2827, 0.2809, 0.3249])\n",
      "tensor([-0.0915,  0.0189, -0.1235, -0.0613, -0.1003, -0.1306, -0.1473, -0.1079,\n",
      "        -0.2438, -0.1113,  0.1361, -0.1477,  0.0387, -0.0907,  0.0352, -0.1851,\n",
      "        -0.1319, -0.1746, -0.0815, -0.1004, -0.3394, -0.1712, -0.0807, -0.1228,\n",
      "        -0.2263, -0.1503, -0.2314, -0.2327, -0.0854, -0.0802, -0.0716, -0.0839,\n",
      "        -0.0592,  0.0358, -0.0322, -0.2197,  0.0027, -0.1471, -0.0264, -0.1886,\n",
      "        -0.2417, -0.1494, -0.1904, -0.1089, -0.2657, -0.1362, -0.0487, -0.1340,\n",
      "        -0.0930, -0.0064, -0.1721, -0.1476, -0.1714,  0.0336, -0.1011, -0.1761,\n",
      "        -0.1184, -0.0482, -0.3260, -0.1555, -0.0169, -0.2373, -0.1015, -0.1051,\n",
      "        -0.2738, -0.1917, -0.0503, -0.1098,  0.1484, -0.2282, -0.0700, -0.1427,\n",
      "        -0.1417, -0.3096, -0.2043,  0.0269, -0.0779, -0.0842, -0.0464, -0.1429,\n",
      "        -0.3917,  0.0257, -0.1779, -0.0993, -0.0507, -0.2222, -0.0951, -0.0861,\n",
      "        -0.0743, -0.1666, -0.2054, -0.1782, -0.1150, -0.2525, -0.0694, -0.0536,\n",
      "        -0.0499, -0.0311,  0.1212, -0.0988, -0.1570, -0.3093, -0.0797, -0.0994,\n",
      "        -0.1774, -0.0505,  0.0766, -0.0480, -0.1278, -0.0651, -0.1737,  0.0303,\n",
      "        -0.1334, -0.2435, -0.0746, -0.0365, -0.1843, -0.0887, -0.1924, -0.1110,\n",
      "        -0.1458, -0.0895, -0.0956, -0.2042, -0.1338, -0.0637, -0.0699, -0.1656,\n",
      "        -0.1521, -0.1317, -0.0826, -0.2470, -0.1174, -0.1475, -0.0840, -0.0681,\n",
      "        -0.1789,  0.0288, -0.0362, -0.3005, -0.1441, -0.0812, -0.0492, -0.0657,\n",
      "        -0.1249, -0.1104,  0.0187, -0.1351, -0.1944, -0.0909,  0.2067, -0.1081,\n",
      "        -0.2499, -0.0999,  0.0507, -0.1899, -0.0369, -0.1432,  0.1279, -0.1782,\n",
      "        -0.1172, -0.0099,  0.0785, -0.0681, -0.0365, -0.1596, -0.1606, -0.0922,\n",
      "        -0.1773, -0.1788,  0.0306, -0.1101, -0.1355, -0.2244, -0.0860, -0.1232,\n",
      "        -0.0927, -0.1666, -0.1393, -0.0898, -0.0614, -0.1740, -0.2503, -0.0593,\n",
      "        -0.1272, -0.1422, -0.0743, -0.2208, -0.2207, -0.2742, -0.1302, -0.0916,\n",
      "        -0.1696, -0.2481, -0.1524,  0.0410, -0.1077,  0.0408, -0.1915, -0.0697,\n",
      "        -0.1049, -0.0110, -0.3257, -0.1336, -0.1021,  0.0128, -0.2717, -0.1245,\n",
      "         0.0288, -0.1025, -0.2405, -0.1476,  0.1008, -0.0220, -0.0983, -0.4417,\n",
      "        -0.0774, -0.3207, -0.0272, -0.0726, -0.0608, -0.0430, -0.0872, -0.1280,\n",
      "        -0.1608, -0.1529, -0.1745, -0.1702, -0.0486, -0.1459, -0.0552, -0.0808,\n",
      "        -0.0264, -0.0952, -0.1126, -0.0452, -0.0837, -0.0331,  0.0127, -0.0865,\n",
      "        -0.1446, -0.0732, -0.2160, -0.0952, -0.1297, -0.2008, -0.2135, -0.2204,\n",
      "        -0.2381, -0.1787, -0.1386, -0.1901, -0.0981, -0.0850, -0.0761, -0.0586])\n",
      "tensor([[[[-0.0093, -0.0339, -0.0119],\n",
      "          [-0.0246, -0.0798, -0.0487],\n",
      "          [-0.0435, -0.0801, -0.0653]],\n",
      "\n",
      "         [[-0.0289,  0.0002, -0.0286],\n",
      "          [ 0.0099,  0.0103, -0.0177],\n",
      "          [-0.0107,  0.0028, -0.0125]],\n",
      "\n",
      "         [[-0.0147,  0.0226,  0.0044],\n",
      "          [ 0.0155,  0.0109, -0.0040],\n",
      "          [-0.0208, -0.0180, -0.0173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0003, -0.0041, -0.0008],\n",
      "          [-0.0217, -0.0223, -0.0299],\n",
      "          [ 0.0105,  0.0035, -0.0114]],\n",
      "\n",
      "         [[ 0.0097,  0.0184,  0.0370],\n",
      "          [ 0.0037,  0.0104,  0.0152],\n",
      "          [ 0.0084,  0.0183,  0.0302]],\n",
      "\n",
      "         [[ 0.0014,  0.0084,  0.0097],\n",
      "          [ 0.0265,  0.0415,  0.0553],\n",
      "          [ 0.0169,  0.0610,  0.0563]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0117,  0.0165,  0.0051],\n",
      "          [ 0.0294,  0.0204,  0.0216],\n",
      "          [ 0.0079,  0.0133,  0.0117]],\n",
      "\n",
      "         [[-0.0153, -0.0213, -0.0090],\n",
      "          [-0.0292, -0.0516, -0.0436],\n",
      "          [-0.0045, -0.0372, -0.0420]],\n",
      "\n",
      "         [[ 0.0003,  0.0398, -0.0001],\n",
      "          [ 0.0129,  0.0332,  0.0163],\n",
      "          [-0.0096, -0.0057, -0.0170]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0447,  0.0082,  0.0169],\n",
      "          [-0.0066, -0.0371, -0.0059],\n",
      "          [-0.0239, -0.0607, -0.0289]],\n",
      "\n",
      "         [[-0.0057, -0.0423, -0.0219],\n",
      "          [-0.0228, -0.0314, -0.0583],\n",
      "          [-0.0196, -0.0530, -0.0485]],\n",
      "\n",
      "         [[ 0.0065,  0.0033,  0.0093],\n",
      "          [ 0.0010, -0.0049, -0.0110],\n",
      "          [-0.0160, -0.0101, -0.0146]]],\n",
      "\n",
      "\n",
      "        [[[-0.0061, -0.0067, -0.0069],\n",
      "          [-0.0052, -0.0089, -0.0143],\n",
      "          [-0.0115, -0.0171, -0.0237]],\n",
      "\n",
      "         [[ 0.0399,  0.0167,  0.0210],\n",
      "          [ 0.0165, -0.0262, -0.0116],\n",
      "          [ 0.0059, -0.0206, -0.0153]],\n",
      "\n",
      "         [[ 0.0060,  0.0242,  0.0207],\n",
      "          [ 0.0050, -0.0062,  0.0148],\n",
      "          [ 0.0099, -0.0279, -0.0054]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0097, -0.0017, -0.0101],\n",
      "          [-0.0006,  0.0434,  0.0381],\n",
      "          [ 0.0038,  0.0450,  0.0392]],\n",
      "\n",
      "         [[ 0.0134,  0.0069,  0.0196],\n",
      "          [ 0.0068,  0.0250,  0.0152],\n",
      "          [ 0.0018, -0.0044,  0.0037]],\n",
      "\n",
      "         [[-0.0174, -0.0163, -0.0240],\n",
      "          [-0.0197, -0.0174, -0.0178],\n",
      "          [-0.0300, -0.0137, -0.0211]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0066,  0.0061,  0.0284],\n",
      "          [-0.0130, -0.0509, -0.0204],\n",
      "          [-0.0154, -0.0149, -0.0181]],\n",
      "\n",
      "         [[ 0.0123,  0.0325,  0.0227],\n",
      "          [-0.0042, -0.0181,  0.0022],\n",
      "          [ 0.0029, -0.0174,  0.0032]],\n",
      "\n",
      "         [[-0.0110,  0.0111, -0.0142],\n",
      "          [ 0.0082,  0.0301,  0.0416],\n",
      "          [ 0.0006,  0.0002,  0.0213]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0008,  0.0266,  0.0118],\n",
      "          [-0.0159, -0.0074, -0.0051],\n",
      "          [ 0.0042, -0.0069, -0.0073]],\n",
      "\n",
      "         [[-0.0070,  0.0010,  0.0019],\n",
      "          [ 0.0061,  0.0510, -0.0035],\n",
      "          [-0.0081, -0.0303, -0.0174]],\n",
      "\n",
      "         [[-0.0100,  0.0003, -0.0011],\n",
      "          [-0.0015, -0.0297, -0.0195],\n",
      "          [-0.0010,  0.0049,  0.0151]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0161,  0.0126,  0.0055],\n",
      "          [-0.0132, -0.0196, -0.0208],\n",
      "          [-0.0077, -0.0230, -0.0202]],\n",
      "\n",
      "         [[-0.0019,  0.0530,  0.0134],\n",
      "          [ 0.0027,  0.0249,  0.0167],\n",
      "          [-0.0192, -0.0188, -0.0200]],\n",
      "\n",
      "         [[ 0.0081,  0.0300,  0.0006],\n",
      "          [ 0.0036,  0.0254, -0.0104],\n",
      "          [-0.0010, -0.0193, -0.0121]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0163,  0.0251,  0.0290],\n",
      "          [-0.0073, -0.0014,  0.0124],\n",
      "          [-0.0211, -0.0347, -0.0195]],\n",
      "\n",
      "         [[ 0.0165,  0.0519,  0.0494],\n",
      "          [ 0.0058,  0.0199,  0.0363],\n",
      "          [-0.0049, -0.0165, -0.0130]],\n",
      "\n",
      "         [[-0.0102, -0.0308, -0.0340],\n",
      "          [ 0.0055, -0.0109,  0.0005],\n",
      "          [ 0.0382,  0.0209,  0.0326]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0184, -0.0031,  0.0103],\n",
      "          [-0.0072, -0.0013, -0.0070],\n",
      "          [ 0.0217,  0.0011, -0.0033]],\n",
      "\n",
      "         [[-0.0166,  0.0002, -0.0092],\n",
      "          [ 0.0150,  0.0584,  0.0218],\n",
      "          [-0.0136,  0.0181,  0.0164]],\n",
      "\n",
      "         [[ 0.0219,  0.0316,  0.0183],\n",
      "          [-0.0021, -0.0156,  0.0203],\n",
      "          [ 0.0053,  0.0005,  0.0157]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0045, -0.0025,  0.0017],\n",
      "          [-0.0052, -0.0088,  0.0137],\n",
      "          [-0.0107,  0.0010,  0.0170]],\n",
      "\n",
      "         [[-0.0060, -0.0368,  0.0031],\n",
      "          [ 0.0102,  0.0291, -0.0007],\n",
      "          [ 0.0099,  0.0600,  0.0272]],\n",
      "\n",
      "         [[-0.0092, -0.0183,  0.0062],\n",
      "          [-0.0319, -0.0294, -0.0149],\n",
      "          [-0.0148, -0.0123, -0.0236]]]])\n",
      "tensor([0.3212, 0.2124, 0.2661, 0.3594, 0.2785, 0.2582, 0.3108, 0.3096, 0.3348,\n",
      "        0.2992, 0.2545, 0.2458, 0.3133, 0.4159, 0.2997, 0.3070, 0.3135, 0.4418,\n",
      "        0.3743, 0.2570, 0.2943, 0.3078, 0.2738, 0.3948, 0.2928, 0.3572, 0.3435,\n",
      "        0.5379, 0.4243, 0.3908, 0.2745, 0.2798, 0.3217, 0.1956, 0.2751, 0.3187,\n",
      "        0.3507, 0.2751, 0.1919, 0.3307, 0.2850, 0.3038, 0.2179, 0.2652, 0.2944,\n",
      "        0.2138, 0.2184, 0.2948, 0.3262, 0.3759, 0.2557, 0.3796, 0.2950, 0.3386,\n",
      "        0.3243, 0.3070, 0.3331, 0.2302, 0.3036, 0.3377, 0.2922, 0.2204, 0.3267,\n",
      "        0.3198, 0.4023, 0.2987, 0.4860, 0.2854, 0.2716, 0.4341, 0.2834, 0.2296,\n",
      "        0.2507, 0.3120, 0.3673, 0.3244, 0.3380, 0.3272, 0.2868, 0.2877, 0.3210,\n",
      "        0.2332, 0.3379, 0.2767, 0.2942, 0.2672, 0.4401, 0.2908, 0.3771, 0.2789,\n",
      "        0.3056, 0.3276, 0.3871, 0.2453, 0.2559, 0.2783, 0.3168, 0.3410, 0.2318,\n",
      "        0.3577, 0.5036, 0.3557, 0.2475, 0.1852, 0.2273, 0.3602, 0.2919, 0.3928,\n",
      "        0.4423, 0.2052, 0.2524, 0.2189, 0.4113, 0.3611, 0.4284, 0.2333, 0.3504,\n",
      "        0.7001, 0.3754, 0.2874, 0.3702, 0.3174, 0.3640, 0.2889, 0.4155, 0.2479,\n",
      "        0.2898, 0.3740, 0.4926, 0.2808, 0.2388, 0.3473, 0.1868, 0.2837, 0.3090,\n",
      "        0.3614, 0.2797, 0.6871, 0.2854, 0.2937, 0.3128, 0.4863, 0.2193, 0.2871,\n",
      "        0.2554, 0.4175, 0.3044, 0.3230, 0.3343, 0.4947, 0.3924, 0.2264, 0.2657,\n",
      "        0.4193, 0.3483, 0.3551, 0.2877, 0.2559, 0.2459, 0.2775, 0.3842, 0.2949,\n",
      "        0.3510, 0.1926, 0.3101, 0.3417, 0.3931, 0.3918, 0.3239, 0.2851, 0.4583,\n",
      "        0.2669, 0.2663, 0.4433, 0.3221, 0.3655, 0.3336, 0.4393, 0.3970, 0.3727,\n",
      "        0.3523, 0.3586, 0.3286, 0.4181, 0.2955, 0.3050, 0.2988, 0.4320, 0.2309,\n",
      "        0.3826, 0.2270, 0.2228, 0.3206, 0.3273, 0.2627, 0.3087, 0.2920, 0.2328,\n",
      "        0.4144, 0.4075, 0.3264, 0.3583, 0.3014, 0.3150, 0.4438, 0.4042, 0.2028,\n",
      "        0.3855, 0.2570, 0.2361, 0.2343, 0.3312, 0.2303, 0.3744, 0.4727, 0.3601,\n",
      "        0.2754, 0.1987, 0.3027, 0.3427, 0.2994, 0.2533, 0.2639, 0.3460, 0.3847,\n",
      "        0.4368, 0.3786, 0.3123, 0.2591, 0.3979, 0.2577, 0.3131, 0.2934, 0.3027,\n",
      "        0.2942, 0.2266, 0.2806, 0.2977, 0.1858, 0.2788, 0.2504, 0.3948, 0.3496,\n",
      "        0.2429, 0.2155, 0.2683, 0.4100, 0.3495, 0.4243, 0.2627, 0.3329, 0.2849,\n",
      "        0.3924, 0.3728, 0.2655, 0.3338])\n",
      "tensor([-2.6385e-02,  9.9545e-02, -6.8362e-03, -8.7678e-02,  7.8322e-03,\n",
      "         4.0653e-02, -3.0738e-02,  5.9561e-03,  1.6938e-03,  4.7769e-02,\n",
      "         6.3004e-02,  3.5760e-02, -5.0405e-02,  2.1372e-02, -9.0327e-03,\n",
      "        -3.3702e-02, -4.5529e-02, -1.9238e-01, -6.7608e-02,  7.7464e-02,\n",
      "        -3.3957e-02, -7.9947e-02,  1.3138e-01, -1.2730e-01, -6.2825e-02,\n",
      "        -5.4973e-03, -9.1520e-02, -1.7570e-01, -8.2958e-03, -9.4468e-02,\n",
      "         2.4915e-03, -3.1894e-02, -1.5755e-02,  1.4372e-01, -3.4770e-03,\n",
      "         1.0756e-02, -5.1074e-02,  3.5848e-02,  8.7819e-02, -4.5240e-02,\n",
      "        -4.5785e-02,  1.4690e-02,  6.8685e-02,  1.6806e-02, -4.7715e-02,\n",
      "         5.6814e-02,  4.5985e-02, -5.0668e-02,  5.8914e-03, -1.0335e-01,\n",
      "         1.0281e-02, -1.0520e-01, -1.6557e-02, -1.9213e-02, -3.4522e-02,\n",
      "         2.0138e-02, -1.3621e-01,  3.9602e-02, -8.8277e-03, -1.0837e-02,\n",
      "        -2.9803e-02,  7.2119e-02, -6.6893e-02, -9.4287e-03, -3.0956e-02,\n",
      "        -2.6700e-02, -1.4184e-01,  1.1900e-01,  6.6929e-02, -2.1375e-01,\n",
      "         4.2720e-02,  4.7782e-02,  3.3905e-02,  1.3857e-04, -1.4822e-01,\n",
      "        -2.3684e-02, -7.4337e-02, -6.8427e-02, -2.0076e-02,  1.4697e-02,\n",
      "        -3.9620e-02,  1.9393e-02, -6.9640e-02, -5.5779e-02,  7.9664e-03,\n",
      "         2.3639e-02, -2.5781e-01,  6.3564e-03, -1.0041e-01,  2.8047e-02,\n",
      "         1.5223e-02, -4.8413e-02, -1.5355e-01,  1.0493e-01,  4.9892e-02,\n",
      "         6.5689e-02, -5.4139e-02,  7.7265e-03,  9.4090e-02, -1.9978e-02,\n",
      "        -2.3561e-01, -6.2284e-02,  3.3363e-02,  1.1017e-01,  7.6999e-02,\n",
      "        -3.2478e-02,  4.8089e-02, -1.4995e-01, -1.6503e-01,  1.2304e-01,\n",
      "         7.1157e-02,  5.8895e-02, -4.8155e-02, -9.7220e-02, -1.8604e-01,\n",
      "         8.5326e-02, -5.1602e-02, -3.0799e-01, -6.0410e-02, -7.7131e-02,\n",
      "        -2.7282e-01,  2.8950e-02, -1.3280e-01,  1.7264e-02, -3.9184e-02,\n",
      "         5.4153e-02, -3.7219e-02, -1.5283e-01, -1.7663e-01,  8.3937e-02,\n",
      "         6.9329e-02, -8.2645e-02,  1.1178e-01, -5.0772e-02, -4.4772e-02,\n",
      "        -3.7455e-02,  3.0361e-02, -3.7824e-01,  1.4857e-02,  6.8245e-03,\n",
      "        -5.2090e-02, -2.9501e-01,  8.9860e-02,  2.9646e-02,  1.9930e-02,\n",
      "        -8.3483e-02, -9.6416e-02, -2.3830e-02,  3.4911e-02, -2.6626e-01,\n",
      "        -1.6184e-01,  7.3552e-02,  2.7568e-02, -1.1087e-01, -1.0322e-02,\n",
      "        -9.7499e-02,  1.3995e-02,  1.0802e-02,  7.8444e-02,  1.3057e-02,\n",
      "        -3.9476e-02,  2.4805e-02, -7.7445e-02, -2.8419e-02,  1.0412e-02,\n",
      "        -4.2330e-02, -1.6634e-01, -9.4908e-02, -3.4328e-02,  4.5544e-02,\n",
      "        -3.0001e-01, -6.9376e-03,  1.4103e-02, -2.6155e-01, -7.3598e-02,\n",
      "        -1.0627e-01, -1.0544e-02, -7.1160e-02, -1.0339e-01, -2.9787e-02,\n",
      "        -1.4277e-01, -5.1710e-02, -5.7126e-02, -5.4392e-02, -4.2260e-02,\n",
      "        -8.5120e-03,  1.5887e-02, -6.5358e-02, -6.1346e-02, -1.4502e-01,\n",
      "         3.9939e-02,  8.1640e-02, -7.7597e-03, -3.4062e-02,  3.1969e-02,\n",
      "        -4.4756e-02, -7.0306e-02,  1.0213e-01, -1.7993e-01, -2.1173e-01,\n",
      "        -5.9797e-02, -1.1596e-01,  3.9271e-02, -4.5443e-02, -1.8446e-01,\n",
      "        -1.0848e-01,  5.5781e-02, -6.3649e-02,  1.6825e-02,  1.6623e-04,\n",
      "         7.9866e-02, -6.7240e-02,  7.9827e-02, -3.9905e-03, -1.9016e-01,\n",
      "         2.0026e-02,  7.3181e-02,  1.0323e-01, -2.6431e-02,  2.3963e-02,\n",
      "        -4.4247e-02,  2.2896e-02,  2.3444e-02, -2.3481e-02,  1.0516e-02,\n",
      "        -2.1494e-01, -1.2810e-01, -1.8338e-02, -6.0221e-04, -5.1624e-02,\n",
      "         5.6575e-02, -5.4297e-02,  1.4146e-02, -4.9852e-02,  6.7255e-02,\n",
      "         5.1713e-02, -4.0266e-03,  3.5057e-02,  8.2781e-02,  1.0034e-02,\n",
      "         5.9230e-02, -2.0429e-01, -7.6169e-02,  4.1432e-02,  7.7517e-02,\n",
      "         7.6005e-02, -1.5919e-01, -8.3613e-02, -1.6625e-01,  2.2968e-03,\n",
      "        -6.8489e-02,  3.8119e-02, -9.8731e-02, -2.0290e-02,  1.5394e-02,\n",
      "        -1.0548e-01])\n",
      "tensor([[[[ 0.0081]],\n",
      "\n",
      "         [[-0.0192]],\n",
      "\n",
      "         [[-0.0173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0128]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0054]]],\n",
      "\n",
      "\n",
      "        [[[-0.0143]],\n",
      "\n",
      "         [[-0.0554]],\n",
      "\n",
      "         [[-0.0346]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0275]],\n",
      "\n",
      "         [[ 0.0360]],\n",
      "\n",
      "         [[ 0.0240]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0076]],\n",
      "\n",
      "         [[ 0.0207]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0278]],\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         [[-0.0022]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0336]],\n",
      "\n",
      "         [[-0.0424]],\n",
      "\n",
      "         [[ 0.0226]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0330]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[-0.0177]]],\n",
      "\n",
      "\n",
      "        [[[-0.0114]],\n",
      "\n",
      "         [[-0.0183]],\n",
      "\n",
      "         [[ 0.0076]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0151]],\n",
      "\n",
      "         [[ 0.0332]],\n",
      "\n",
      "         [[ 0.0002]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0063]],\n",
      "\n",
      "         [[-0.0200]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0191]],\n",
      "\n",
      "         [[ 0.0455]],\n",
      "\n",
      "         [[ 0.0078]]]])\n",
      "tensor([0.0674, 0.0514, 0.0385, 0.1692, 0.0604, 0.0460, 0.1209, 0.1110, 0.0418,\n",
      "        0.0387, 0.0442, 0.0707, 0.0790, 0.1094, 0.0959, 0.0544, 0.1032, 0.2190,\n",
      "        0.0459, 0.0372, 0.1410, 0.0587, 0.0360, 0.0955, 0.1657, 0.1024, 0.1417,\n",
      "        0.0580, 0.0536, 0.0716, 0.0865, 0.1110, 0.0511, 0.0515, 0.0809, 0.1154,\n",
      "        0.0777, 0.0449, 0.0490, 0.1056, 0.1457, 0.0744, 0.0530, 0.0600, 0.1026,\n",
      "        0.0486, 0.0408, 0.1312, 0.0639, 0.1062, 0.0915, 0.1476, 0.0900, 0.0742,\n",
      "        0.1069, 0.0776, 0.1423, 0.0495, 0.0974, 0.0661, 0.1292, 0.0548, 0.1145,\n",
      "        0.0950, 0.0921, 0.1579, 0.0496, 0.0236, 0.0398, 0.0935, 0.0291, 0.0653,\n",
      "        0.0885, 0.1190, 0.1692, 0.0692, 0.1316, 0.0606, 0.0480, 0.0654, 0.1082,\n",
      "        0.0624, 0.1103, 0.1106, 0.1076, 0.0400, 0.0723, 0.0947, 0.0662, 0.0464,\n",
      "        0.0444, 0.1727, 0.0921, 0.0345, 0.0451, 0.0374, 0.0940, 0.0818, 0.0397,\n",
      "        0.0452, 0.0985, 0.1095, 0.1072, 0.0506, 0.0444, 0.0755, 0.0420, 0.1046,\n",
      "        0.1172, 0.0447, 0.0459, 0.0409, 0.0539, 0.1036, 0.0741, 0.0311, 0.1086,\n",
      "        0.1746, 0.0777, 0.0689, 0.1100, 0.0489, 0.1048, 0.1097, 0.1025, 0.0448,\n",
      "        0.0675, 0.0707, 0.1364, 0.0438, 0.0346, 0.1769, 0.0667, 0.1155, 0.0628,\n",
      "        0.0873, 0.0406, 0.2890, 0.0703, 0.0428, 0.1173, 0.1049, 0.0611, 0.0469,\n",
      "        0.0400, 0.0744, 0.1003, 0.1012, 0.0599, 0.1078, 0.1512, 0.0322, 0.0430,\n",
      "        0.0977, 0.0951, 0.0838, 0.0958, 0.0448, 0.0263, 0.0425, 0.1154, 0.0771,\n",
      "        0.1781, 0.0300, 0.0699, 0.0724, 0.1600, 0.0893, 0.1130, 0.0534, 0.1359,\n",
      "        0.0375, 0.0809, 0.1145, 0.1232, 0.0942, 0.0880, 0.0346, 0.0996, 0.0461,\n",
      "        0.0694, 0.0630, 0.1590, 0.0509, 0.1254, 0.0590, 0.0744, 0.1084, 0.0514,\n",
      "        0.0931, 0.0848, 0.0240, 0.0279, 0.0993, 0.0612, 0.0599, 0.1095, 0.0508,\n",
      "        0.0658, 0.1162, 0.0833, 0.1651, 0.0505, 0.1231, 0.1228, 0.1038, 0.0369,\n",
      "        0.0756, 0.0415, 0.1192, 0.0292, 0.0839, 0.0577, 0.0951, 0.0944, 0.0309,\n",
      "        0.0390, 0.0604, 0.0672, 0.0501, 0.0383, 0.0946, 0.0958, 0.0501, 0.0243,\n",
      "        0.1074, 0.1908, 0.0693, 0.1376, 0.1151, 0.0329, 0.0647, 0.0616, 0.1106,\n",
      "        0.0358, 0.0721, 0.0851, 0.0375, 0.0368, 0.0947, 0.0464, 0.1666, 0.1049,\n",
      "        0.0755, 0.0398, 0.0249, 0.1528, 0.1167, 0.0886, 0.0540, 0.0726, 0.0736,\n",
      "        0.0797, 0.0854, 0.0609, 0.1263])\n",
      "tensor([-2.6385e-02,  9.9545e-02, -6.8362e-03, -8.7678e-02,  7.8322e-03,\n",
      "         4.0653e-02, -3.0738e-02,  5.9561e-03,  1.6938e-03,  4.7769e-02,\n",
      "         6.3004e-02,  3.5760e-02, -5.0405e-02,  2.1372e-02, -9.0327e-03,\n",
      "        -3.3702e-02, -4.5529e-02, -1.9238e-01, -6.7608e-02,  7.7464e-02,\n",
      "        -3.3957e-02, -7.9947e-02,  1.3138e-01, -1.2730e-01, -6.2825e-02,\n",
      "        -5.4973e-03, -9.1520e-02, -1.7570e-01, -8.2958e-03, -9.4468e-02,\n",
      "         2.4915e-03, -3.1894e-02, -1.5755e-02,  1.4372e-01, -3.4770e-03,\n",
      "         1.0756e-02, -5.1074e-02,  3.5848e-02,  8.7819e-02, -4.5240e-02,\n",
      "        -4.5785e-02,  1.4690e-02,  6.8685e-02,  1.6806e-02, -4.7715e-02,\n",
      "         5.6814e-02,  4.5985e-02, -5.0668e-02,  5.8914e-03, -1.0335e-01,\n",
      "         1.0281e-02, -1.0520e-01, -1.6557e-02, -1.9213e-02, -3.4522e-02,\n",
      "         2.0138e-02, -1.3621e-01,  3.9602e-02, -8.8277e-03, -1.0837e-02,\n",
      "        -2.9803e-02,  7.2119e-02, -6.6893e-02, -9.4287e-03, -3.0956e-02,\n",
      "        -2.6700e-02, -1.4184e-01,  1.1900e-01,  6.6929e-02, -2.1375e-01,\n",
      "         4.2720e-02,  4.7782e-02,  3.3905e-02,  1.3857e-04, -1.4822e-01,\n",
      "        -2.3684e-02, -7.4337e-02, -6.8427e-02, -2.0076e-02,  1.4697e-02,\n",
      "        -3.9620e-02,  1.9393e-02, -6.9640e-02, -5.5779e-02,  7.9664e-03,\n",
      "         2.3639e-02, -2.5781e-01,  6.3564e-03, -1.0041e-01,  2.8047e-02,\n",
      "         1.5223e-02, -4.8413e-02, -1.5355e-01,  1.0493e-01,  4.9892e-02,\n",
      "         6.5689e-02, -5.4139e-02,  7.7265e-03,  9.4090e-02, -1.9978e-02,\n",
      "        -2.3561e-01, -6.2284e-02,  3.3363e-02,  1.1017e-01,  7.6999e-02,\n",
      "        -3.2478e-02,  4.8089e-02, -1.4995e-01, -1.6503e-01,  1.2304e-01,\n",
      "         7.1157e-02,  5.8895e-02, -4.8155e-02, -9.7220e-02, -1.8604e-01,\n",
      "         8.5326e-02, -5.1602e-02, -3.0799e-01, -6.0410e-02, -7.7131e-02,\n",
      "        -2.7282e-01,  2.8950e-02, -1.3280e-01,  1.7264e-02, -3.9184e-02,\n",
      "         5.4153e-02, -3.7219e-02, -1.5283e-01, -1.7663e-01,  8.3937e-02,\n",
      "         6.9329e-02, -8.2645e-02,  1.1178e-01, -5.0772e-02, -4.4772e-02,\n",
      "        -3.7455e-02,  3.0361e-02, -3.7824e-01,  1.4857e-02,  6.8245e-03,\n",
      "        -5.2090e-02, -2.9501e-01,  8.9860e-02,  2.9646e-02,  1.9930e-02,\n",
      "        -8.3483e-02, -9.6416e-02, -2.3830e-02,  3.4911e-02, -2.6626e-01,\n",
      "        -1.6184e-01,  7.3552e-02,  2.7568e-02, -1.1087e-01, -1.0322e-02,\n",
      "        -9.7499e-02,  1.3995e-02,  1.0802e-02,  7.8444e-02,  1.3057e-02,\n",
      "        -3.9476e-02,  2.4805e-02, -7.7445e-02, -2.8419e-02,  1.0412e-02,\n",
      "        -4.2330e-02, -1.6634e-01, -9.4908e-02, -3.4328e-02,  4.5544e-02,\n",
      "        -3.0001e-01, -6.9376e-03,  1.4103e-02, -2.6155e-01, -7.3598e-02,\n",
      "        -1.0627e-01, -1.0544e-02, -7.1160e-02, -1.0339e-01, -2.9787e-02,\n",
      "        -1.4277e-01, -5.1710e-02, -5.7126e-02, -5.4392e-02, -4.2260e-02,\n",
      "        -8.5120e-03,  1.5887e-02, -6.5358e-02, -6.1346e-02, -1.4502e-01,\n",
      "         3.9939e-02,  8.1640e-02, -7.7597e-03, -3.4062e-02,  3.1969e-02,\n",
      "        -4.4756e-02, -7.0306e-02,  1.0213e-01, -1.7993e-01, -2.1173e-01,\n",
      "        -5.9797e-02, -1.1596e-01,  3.9271e-02, -4.5443e-02, -1.8446e-01,\n",
      "        -1.0848e-01,  5.5781e-02, -6.3649e-02,  1.6825e-02,  1.6623e-04,\n",
      "         7.9866e-02, -6.7240e-02,  7.9827e-02, -3.9905e-03, -1.9016e-01,\n",
      "         2.0026e-02,  7.3181e-02,  1.0323e-01, -2.6431e-02,  2.3963e-02,\n",
      "        -4.4247e-02,  2.2896e-02,  2.3444e-02, -2.3481e-02,  1.0516e-02,\n",
      "        -2.1494e-01, -1.2810e-01, -1.8338e-02, -6.0221e-04, -5.1624e-02,\n",
      "         5.6575e-02, -5.4297e-02,  1.4146e-02, -4.9852e-02,  6.7255e-02,\n",
      "         5.1713e-02, -4.0266e-03,  3.5057e-02,  8.2781e-02,  1.0034e-02,\n",
      "         5.9230e-02, -2.0429e-01, -7.6169e-02,  4.1432e-02,  7.7517e-02,\n",
      "         7.6005e-02, -1.5919e-01, -8.3613e-02, -1.6625e-01,  2.2968e-03,\n",
      "        -6.8489e-02,  3.8119e-02, -9.8731e-02, -2.0290e-02,  1.5394e-02,\n",
      "        -1.0548e-01])\n",
      "tensor([[[[ 4.8367e-02,  4.8045e-02,  3.8471e-02],\n",
      "          [ 4.9888e-02,  5.5208e-02,  5.6701e-02],\n",
      "          [ 2.4192e-02,  1.3436e-02,  2.4655e-02]],\n",
      "\n",
      "         [[-3.6542e-03, -3.1100e-03,  4.9227e-03],\n",
      "          [-1.2114e-03,  3.4020e-03,  1.9846e-02],\n",
      "          [-2.1704e-02, -2.1158e-02, -2.8686e-03]],\n",
      "\n",
      "         [[-1.2536e-02, -2.0486e-02, -2.3154e-02],\n",
      "          [-1.3515e-02, -2.3781e-02, -2.5515e-02],\n",
      "          [ 1.0584e-02,  7.2999e-03, -5.2329e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.3596e-02, -1.8328e-02, -5.0577e-02],\n",
      "          [ 1.6590e-02,  5.0719e-02,  2.1919e-02],\n",
      "          [-1.9203e-02, -8.8315e-03, -2.0335e-02]],\n",
      "\n",
      "         [[-7.6949e-03, -1.5848e-02,  1.5841e-03],\n",
      "          [-6.2470e-03, -1.3135e-02,  6.9092e-03],\n",
      "          [-3.3791e-03,  1.7889e-03,  3.7373e-03]],\n",
      "\n",
      "         [[-6.6310e-03,  5.8503e-03, -5.8571e-04],\n",
      "          [-2.4600e-02, -8.9747e-03, -7.2466e-03],\n",
      "          [-1.7566e-02, -8.5829e-03, -7.5220e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.3679e-02, -9.4399e-03, -1.1688e-02],\n",
      "          [-2.4777e-02, -1.7326e-02, -3.1489e-02],\n",
      "          [-3.3683e-03,  9.7571e-03, -5.1527e-03]],\n",
      "\n",
      "         [[-3.0809e-02, -4.0685e-02, -2.2731e-02],\n",
      "          [-5.1065e-03, -1.6457e-02, -1.8804e-02],\n",
      "          [ 5.0382e-02,  5.2054e-02,  3.9185e-02]],\n",
      "\n",
      "         [[-3.7790e-02, -4.2234e-02, -2.9703e-02],\n",
      "          [-6.4766e-03,  2.6967e-03, -8.1736e-03],\n",
      "          [ 3.7747e-02,  5.5416e-02,  2.5806e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7275e-02, -4.5364e-02, -3.9567e-02],\n",
      "          [ 8.9827e-03,  1.6150e-02,  1.1675e-02],\n",
      "          [-9.7209e-03, -3.6449e-02, -1.6842e-02]],\n",
      "\n",
      "         [[ 1.7824e-02,  1.5013e-02,  1.0225e-02],\n",
      "          [ 5.4044e-03,  1.1664e-02,  6.4623e-03],\n",
      "          [ 2.1803e-02,  4.1795e-02,  1.9234e-02]],\n",
      "\n",
      "         [[-2.6730e-04,  1.5218e-03, -5.0352e-03],\n",
      "          [ 2.5761e-02,  2.7110e-02, -9.3395e-04],\n",
      "          [-1.1949e-02, -7.5204e-03, -3.9370e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.7447e-02, -1.8358e-02, -2.6020e-02],\n",
      "          [-1.4074e-02, -1.1302e-02, -1.4814e-02],\n",
      "          [-3.1460e-03, -1.8674e-02, -9.3350e-03]],\n",
      "\n",
      "         [[-5.1125e-03, -4.8036e-03,  1.8139e-02],\n",
      "          [-1.0524e-02, -1.5152e-02,  2.3904e-03],\n",
      "          [ 8.7093e-03,  9.3810e-03,  2.4203e-03]],\n",
      "\n",
      "         [[-7.6392e-03, -8.1496e-03, -1.5331e-02],\n",
      "          [-8.0622e-03, -1.3383e-02, -1.3938e-02],\n",
      "          [-1.6904e-02, -3.0059e-02, -1.8659e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8390e-02, -2.6080e-03,  9.3782e-03],\n",
      "          [-6.4662e-04, -1.3146e-02,  1.0045e-02],\n",
      "          [-2.2293e-03, -1.4097e-02,  1.7385e-02]],\n",
      "\n",
      "         [[ 3.0293e-04,  2.9622e-03,  1.0030e-02],\n",
      "          [-5.7588e-03, -1.6943e-03,  6.9988e-03],\n",
      "          [ 9.8134e-03,  1.4197e-02,  5.9742e-03]],\n",
      "\n",
      "         [[ 2.8753e-03, -1.7814e-03,  1.0873e-02],\n",
      "          [ 1.5230e-02,  4.5867e-03,  1.6860e-02],\n",
      "          [ 1.9536e-03,  1.9503e-02,  1.2168e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.3983e-02,  2.4598e-03, -7.4604e-03],\n",
      "          [-2.2250e-02, -1.2757e-02, -2.8846e-03],\n",
      "          [-1.0911e-02,  7.5499e-03,  8.6910e-03]],\n",
      "\n",
      "         [[-4.8463e-03, -8.3250e-03,  1.3420e-02],\n",
      "          [-6.2502e-03, -7.3982e-03,  1.1153e-02],\n",
      "          [ 4.0391e-03, -9.0354e-03, -7.5441e-03]],\n",
      "\n",
      "         [[-5.1627e-03, -8.9529e-03, -1.2414e-02],\n",
      "          [-4.9261e-03, -3.5488e-03,  2.1501e-03],\n",
      "          [-1.1709e-02, -1.4984e-02, -1.9216e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5428e-02, -7.6036e-04, -1.3522e-03],\n",
      "          [-3.4856e-02, -7.4478e-04, -6.5064e-03],\n",
      "          [-9.1655e-03, -2.8467e-02, -4.8924e-02]],\n",
      "\n",
      "         [[ 1.2207e-02,  1.0519e-02, -8.4421e-03],\n",
      "          [-2.5495e-02,  2.8140e-03,  1.6165e-03],\n",
      "          [-1.8831e-02,  1.2268e-02,  1.5439e-02]],\n",
      "\n",
      "         [[-1.3684e-02, -4.1732e-03,  1.2609e-02],\n",
      "          [-6.8834e-04,  5.9757e-03, -1.0183e-02],\n",
      "          [ 2.1559e-04, -1.3462e-02, -3.0114e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6186e-02, -6.4926e-02, -4.3146e-02],\n",
      "          [-2.1790e-02, -4.9106e-02, -3.4568e-02],\n",
      "          [ 4.0506e-02,  4.2449e-02,  6.1562e-02]],\n",
      "\n",
      "         [[ 3.5715e-03, -1.0916e-02, -2.2922e-02],\n",
      "          [-2.4831e-03,  6.4555e-03, -1.1316e-02],\n",
      "          [ 1.6662e-03, -1.9145e-02, -2.3007e-02]],\n",
      "\n",
      "         [[-7.1243e-03, -4.2783e-05,  4.9363e-03],\n",
      "          [-1.5832e-02,  4.0474e-03,  4.5135e-04],\n",
      "          [-4.7967e-03, -7.2164e-04, -1.7230e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1589e-02,  7.7814e-04,  6.3205e-03],\n",
      "          [ 1.1360e-02, -6.2076e-03, -2.7689e-02],\n",
      "          [ 2.6392e-02,  2.3775e-03, -1.4937e-02]],\n",
      "\n",
      "         [[-1.1237e-02, -2.6285e-03,  9.1537e-03],\n",
      "          [-8.2120e-03, -2.2236e-02,  3.2917e-04],\n",
      "          [ 5.5909e-03, -1.3858e-03,  6.8947e-03]],\n",
      "\n",
      "         [[-1.4783e-02, -1.0367e-02, -2.7472e-02],\n",
      "          [-4.1090e-02, -3.8532e-02, -3.9202e-02],\n",
      "          [-2.1614e-02, -3.4340e-02, -1.8542e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9492e-02, -1.6098e-02, -3.1792e-02],\n",
      "          [ 2.5374e-02,  4.6815e-02,  2.7513e-02],\n",
      "          [ 3.5903e-02,  3.1892e-02,  2.6156e-02]],\n",
      "\n",
      "         [[ 1.6856e-02,  1.5645e-02,  1.4189e-02],\n",
      "          [ 2.2550e-02,  3.0456e-02,  1.6739e-02],\n",
      "          [-2.3615e-04, -7.9501e-03, -1.9666e-03]],\n",
      "\n",
      "         [[-7.9060e-03, -4.7390e-03,  1.6030e-03],\n",
      "          [ 1.3802e-03, -8.5837e-03,  6.9451e-03],\n",
      "          [ 1.1407e-02, -5.9877e-03,  1.3759e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0124e-03,  2.9951e-02,  1.1915e-02],\n",
      "          [-4.3412e-02, -3.1776e-03, -2.7705e-02],\n",
      "          [-1.6183e-02, -1.1247e-02, -3.5084e-02]],\n",
      "\n",
      "         [[ 2.9837e-02,  5.9935e-02,  2.4631e-02],\n",
      "          [-1.9571e-03,  2.2415e-02, -1.5499e-02],\n",
      "          [ 1.6075e-02,  1.7850e-02, -1.8412e-02]],\n",
      "\n",
      "         [[-4.3712e-03, -4.9032e-02, -2.1335e-02],\n",
      "          [-5.2598e-03, -2.8579e-02, -2.2090e-02],\n",
      "          [ 8.5126e-03,  2.0862e-03,  2.3301e-02]]]])\n",
      "tensor([0.2480, 0.1972, 0.2279, 0.2709, 0.3296, 0.2640, 0.2710, 0.3475, 0.2388,\n",
      "        0.2904, 0.2769, 0.3045, 0.2268, 0.2634, 0.2999, 0.2397, 0.2724, 0.2723,\n",
      "        0.2133, 0.3806, 0.2767, 0.2403, 0.2406, 0.2917, 0.2675, 0.2305, 0.2394,\n",
      "        0.3123, 0.2984, 0.3353, 0.2234, 0.1919, 0.3168, 0.2626, 0.2901, 0.2918,\n",
      "        0.3455, 0.2561, 0.2434, 0.2298, 0.3318, 0.3481, 0.2032, 0.2478, 0.2478,\n",
      "        0.2483, 0.3252, 0.2567, 0.2685, 0.1977, 0.2541, 0.4079, 0.2480, 0.2076,\n",
      "        0.2276, 0.2683, 0.2098, 0.2056, 0.2010, 0.3560, 0.2384, 0.3284, 0.1952,\n",
      "        0.2445, 0.2848, 0.3742, 0.2746, 0.2117, 0.3859, 0.4785, 0.3005, 0.2848,\n",
      "        0.3762, 0.2903, 0.2126, 0.1776, 0.2778, 0.3878, 0.3123, 0.1974, 0.2679,\n",
      "        0.2300, 0.2474, 0.2320, 0.2635, 0.2819, 0.2296, 0.3194, 0.3814, 0.2503,\n",
      "        0.2269, 0.2676, 0.3431, 0.3799, 0.3787, 0.2968, 0.3021, 0.2575, 0.3007,\n",
      "        0.1939, 0.1950, 0.3217, 0.3623, 0.2171, 0.2486, 0.2266, 0.2133, 0.2851,\n",
      "        0.2715, 0.2720, 0.3107, 0.2174, 0.2675, 0.2387, 0.3434, 0.2761, 0.2084,\n",
      "        0.2975, 0.3178, 0.2818, 0.2858, 0.3498, 0.2675, 0.2638, 0.3159, 0.2879,\n",
      "        0.1873, 0.2986, 0.3584, 0.2570, 0.1815, 0.2758, 0.2640, 0.2486, 0.2567,\n",
      "        0.2252, 0.3420, 0.2910, 0.2898, 0.2902, 0.2404, 0.2381, 0.3633, 0.2690,\n",
      "        0.3810, 0.2947, 0.2743, 0.4644, 0.3133, 0.2444, 0.3477, 0.3001, 0.1977,\n",
      "        0.2301, 0.2513, 0.2660, 0.3271, 0.1622, 0.2274, 0.2225, 0.3596, 0.3215,\n",
      "        0.1997, 0.2215, 0.2706, 0.2831, 0.2621, 0.3710, 0.2730, 0.2903, 0.1893,\n",
      "        0.2140, 0.2460, 0.3141, 0.2424, 0.3699, 0.2364, 0.2420, 0.2948, 0.2497,\n",
      "        0.2760, 0.2686, 0.2895, 0.3857, 0.1398, 0.2832, 0.3362, 0.2522, 0.2823,\n",
      "        0.2381, 0.2311, 0.3274, 0.4078, 0.2648, 0.2525, 0.3388, 0.3251, 0.2420,\n",
      "        0.2856, 0.3605, 0.2603, 0.2294, 0.2483, 0.2171, 0.2353, 0.4117, 0.2588,\n",
      "        0.2888, 0.1972, 0.2408, 0.2755, 0.3031, 0.2457, 0.2744, 0.3564, 0.2546,\n",
      "        0.3673, 0.2883, 0.2590, 0.3021, 0.2890, 0.3505, 0.2092, 0.2953, 0.3222,\n",
      "        0.2925, 0.2574, 0.3012, 0.3893, 0.2211, 0.2226, 0.3258, 0.3205, 0.2975,\n",
      "        0.2323, 0.3323, 0.2812, 0.2702, 0.2300, 0.2846, 0.3318, 0.2292, 0.3498,\n",
      "        0.2622, 0.3581, 0.4003, 0.2924, 0.3049, 0.3478, 0.2845, 0.2742, 0.2019,\n",
      "        0.2466, 0.2988, 0.2044, 0.2691])\n",
      "tensor([-0.1332, -0.0644, -0.3239, -0.2390, -0.3262, -0.1796, -0.2087, -0.3208,\n",
      "        -0.1874, -0.2988, -0.2099, -0.2283, -0.2141, -0.2460, -0.2768, -0.1351,\n",
      "        -0.2498, -0.2393, -0.1223, -0.4590, -0.2172, -0.1220, -0.2101, -0.1779,\n",
      "        -0.2426, -0.1546, -0.1549, -0.3716, -0.2817, -0.3886, -0.1545, -0.0687,\n",
      "        -0.3412, -0.2261, -0.1961, -0.2242, -0.2984, -0.1381, -0.2251, -0.1658,\n",
      "        -0.4534, -0.3226, -0.0977, -0.1349, -0.2619, -0.1428, -0.3960, -0.1633,\n",
      "        -0.2101, -0.1161, -0.1448, -0.5502, -0.2179, -0.1246,  0.0502, -0.1902,\n",
      "        -0.1047, -0.1000, -0.1411, -0.3124, -0.2190, -0.3062, -0.1247, -0.1557,\n",
      "        -0.2973, -0.3825, -0.1951, -0.1381, -0.5761, -0.3879, -0.2808, -0.2542,\n",
      "        -0.3470, -0.2460, -0.1091, -0.0562, -0.1833, -0.4956, -0.3059, -0.0988,\n",
      "        -0.2255, -0.1958, -0.1320, -0.1738, -0.2287, -0.1926, -0.0924, -0.3427,\n",
      "        -0.5489, -0.2431, -0.1935, -0.1641, -0.2503, -0.3274, -0.4008, -0.2824,\n",
      "        -0.2694, -0.1939, -0.2413, -0.0309, -0.0880, -0.3421, -0.3104, -0.1102,\n",
      "        -0.1539, -0.1233, -0.1780, -0.2715, -0.2005, -0.1846, -0.2843, -0.1117,\n",
      "        -0.1816, -0.2119, -0.3304, -0.2267, -0.1413, -0.3376, -0.2674, -0.2524,\n",
      "        -0.2554, -0.4735, -0.2342, -0.2130, -0.3282, -0.1966, -0.1063, -0.2615,\n",
      "        -0.4234, -0.1374, -0.0811, -0.3069, -0.1538, -0.1453, -0.1612, -0.1631,\n",
      "        -0.3759, -0.2608, -0.2382, -0.2499, -0.1485, -0.1487, -0.4328, -0.1377,\n",
      "        -0.2781, -0.2259, -0.2072, -0.4165, -0.3582, -0.1382, -0.3598, -0.2672,\n",
      "        -0.2090, -0.0177, -0.1279, -0.2812, -0.3621,  0.0476, -0.2232, -0.1272,\n",
      "        -0.3237, -0.3008, -0.1119, -0.0839, -0.2426, -0.2000, -0.1873, -0.4685,\n",
      "        -0.2000, -0.3462, -0.0706, -0.1973, -0.3548, -0.1975, -0.3537, -0.3546,\n",
      "        -0.1433, -0.2052, -0.2722, -0.1528, -0.2798, -0.1945, -0.2474, -0.4910,\n",
      "         0.1322, -0.2378, -0.5166, -0.3959, -0.2354, -0.1266, -0.0810, -0.4132,\n",
      "        -0.5576, -0.2238, -0.1563, -0.3950, -0.3283, -0.0846, -0.3103, -0.3130,\n",
      "        -0.1498, -0.1396, -0.0972, -0.1620, -0.1631, -0.6364, -0.1350, -0.2345,\n",
      "        -0.1049, -0.1625, -0.2878, -0.2450, -0.1468, -0.2035, -0.5358, -0.1683,\n",
      "        -0.5524, -0.2511, -0.1230, -0.2305, -0.1925, -0.3759, -0.1014, -0.1697,\n",
      "        -0.4002, -0.2980, -0.3035, -0.1563, -0.4660, -0.1155, -0.1665, -0.3382,\n",
      "        -0.2935, -0.3122, -0.3015, -0.3261, -0.2542, -0.2037, -0.0955, -0.2070,\n",
      "        -0.4370, -0.2051, -0.4205, -0.3125, -0.4845, -0.3528, -0.2624, -0.2894,\n",
      "        -0.3976, -0.2107, -0.1791, -0.1075, -0.1213, -0.3022,  0.0516, -0.1928])\n",
      "tensor([[[[-4.2568e-02, -2.6148e-02, -2.2019e-02],\n",
      "          [-1.7334e-02, -7.5950e-03, -7.2384e-03],\n",
      "          [-1.7876e-03,  2.3800e-02,  1.4873e-02]],\n",
      "\n",
      "         [[-2.8277e-03, -5.0644e-03, -4.9442e-03],\n",
      "          [ 1.2117e-03,  1.4908e-02,  1.6013e-02],\n",
      "          [ 1.4391e-02,  3.3109e-02,  5.0061e-02]],\n",
      "\n",
      "         [[-3.4891e-03, -4.4437e-03,  2.6589e-03],\n",
      "          [ 1.5105e-02,  2.6303e-02,  2.6802e-02],\n",
      "          [ 3.9232e-02,  5.0057e-02,  4.6637e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2877e-02,  1.5454e-02, -2.4483e-02],\n",
      "          [ 3.1145e-02,  3.4944e-02,  1.3296e-02],\n",
      "          [-1.7674e-04,  7.3297e-03, -5.7174e-03]],\n",
      "\n",
      "         [[-2.1781e-02, -3.7379e-02, -1.3382e-02],\n",
      "          [ 1.8976e-02,  1.4155e-02, -6.5395e-03],\n",
      "          [ 2.6831e-02,  3.6354e-02,  1.1450e-02]],\n",
      "\n",
      "         [[ 3.1603e-02,  3.3933e-02,  3.1575e-02],\n",
      "          [-1.0098e-02, -1.2657e-02,  1.1674e-02],\n",
      "          [ 1.0325e-02,  7.9424e-05,  1.5911e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5937e-02,  6.2590e-03,  6.0798e-03],\n",
      "          [-4.5745e-03, -3.5188e-02, -2.9249e-02],\n",
      "          [ 2.1366e-02,  2.0480e-03,  6.2699e-03]],\n",
      "\n",
      "         [[-2.9549e-03, -1.3679e-03, -8.6876e-03],\n",
      "          [ 7.9988e-03,  1.2888e-03, -5.9629e-03],\n",
      "          [-9.1481e-03, -2.1914e-02, -4.1572e-02]],\n",
      "\n",
      "         [[ 7.6390e-03,  3.0253e-03,  2.7817e-04],\n",
      "          [ 7.0329e-03,  1.1914e-02, -2.4419e-03],\n",
      "          [-8.2131e-03, -9.7848e-05, -1.9223e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.4498e-03,  5.1611e-03,  3.7416e-03],\n",
      "          [ 3.2110e-04,  8.3762e-03,  3.6612e-03],\n",
      "          [ 9.3343e-03,  8.1829e-03,  1.1234e-03]],\n",
      "\n",
      "         [[-6.6849e-02, -5.9871e-02, -3.3931e-02],\n",
      "          [ 2.2337e-02,  3.1932e-02,  3.7244e-02],\n",
      "          [ 9.3296e-03,  3.7222e-02,  1.4052e-02]],\n",
      "\n",
      "         [[-2.0643e-03,  1.2408e-02, -3.1072e-03],\n",
      "          [-8.2882e-03,  1.3917e-02, -2.0680e-02],\n",
      "          [-1.9329e-02,  1.1953e-02, -2.3436e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.7788e-03, -3.5982e-03, -1.2592e-03],\n",
      "          [-1.5320e-02, -1.0690e-02, -2.0311e-02],\n",
      "          [-3.4649e-04, -2.2188e-03, -1.5021e-02]],\n",
      "\n",
      "         [[-2.8952e-02, -3.3958e-02, -2.5437e-02],\n",
      "          [-1.5919e-04,  1.5204e-02,  3.4554e-02],\n",
      "          [ 3.6892e-02,  7.0144e-02,  7.3610e-02]],\n",
      "\n",
      "         [[ 1.0721e-02,  2.1531e-03, -5.6155e-03],\n",
      "          [ 1.1754e-02, -4.8546e-03, -5.5013e-03],\n",
      "          [-3.7388e-04, -9.7639e-03, -1.5029e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5622e-02,  9.8976e-03,  3.4725e-03],\n",
      "          [ 1.4711e-02,  7.0707e-03, -9.1826e-03],\n",
      "          [ 7.0986e-03,  6.3087e-03, -3.5893e-03]],\n",
      "\n",
      "         [[-6.4518e-03, -6.7673e-03,  1.1635e-02],\n",
      "          [ 1.4707e-02,  2.3831e-02,  4.9396e-02],\n",
      "          [ 1.8897e-02,  3.4981e-02,  4.5488e-02]],\n",
      "\n",
      "         [[ 1.5900e-02,  3.3369e-02,  2.6194e-02],\n",
      "          [ 1.0616e-02,  1.8515e-02,  3.0190e-03],\n",
      "          [ 1.1004e-02,  2.5503e-02,  1.3654e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1231e-02, -1.2804e-02, -1.5498e-02],\n",
      "          [ 7.6750e-03,  1.2120e-02,  1.5099e-02],\n",
      "          [ 1.8536e-02,  2.5110e-02,  2.5283e-02]],\n",
      "\n",
      "         [[ 7.4059e-03, -3.0540e-03, -1.5475e-03],\n",
      "          [-8.4415e-03, -2.2002e-02, -3.4099e-03],\n",
      "          [ 9.1918e-03,  2.2617e-03, -1.4260e-02]],\n",
      "\n",
      "         [[-5.2568e-03, -5.3507e-03, -3.2230e-03],\n",
      "          [-1.5805e-02,  6.0508e-03, -1.5917e-03],\n",
      "          [-8.9323e-03,  2.6483e-03,  5.0508e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9826e-02, -2.1209e-03,  1.4889e-02],\n",
      "          [ 5.7275e-02,  3.5549e-02,  6.0175e-03],\n",
      "          [ 2.3347e-02, -2.2153e-02, -2.5497e-02]],\n",
      "\n",
      "         [[-1.3985e-02, -6.4766e-02, -1.7286e-02],\n",
      "          [ 1.1704e-02,  1.0714e-02,  4.6278e-02],\n",
      "          [-1.0038e-02, -3.5707e-03,  2.2691e-02]],\n",
      "\n",
      "         [[-8.3342e-03, -1.3070e-03, -1.0049e-02],\n",
      "          [ 3.2605e-02,  5.3259e-02,  2.2172e-02],\n",
      "          [ 3.7339e-02,  6.1155e-02,  4.4555e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6584e-02, -1.3850e-02, -1.4604e-02],\n",
      "          [-1.7604e-02, -2.1268e-02, -1.6734e-02],\n",
      "          [-6.0039e-04,  3.8569e-03,  1.2837e-02]],\n",
      "\n",
      "         [[ 1.7623e-02,  2.3706e-02,  2.7633e-02],\n",
      "          [-2.2841e-02, -1.9576e-02, -1.6551e-02],\n",
      "          [-8.0822e-03,  4.3779e-03, -5.3622e-03]],\n",
      "\n",
      "         [[ 1.5582e-02,  3.7879e-02,  2.3555e-02],\n",
      "          [-6.4632e-03,  9.8620e-03,  1.2121e-02],\n",
      "          [-1.3743e-02, -6.1246e-03, -2.7332e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5037e-03, -1.2064e-02, -9.0989e-03],\n",
      "          [-4.7911e-04, -2.8339e-03,  2.1365e-03],\n",
      "          [-6.2077e-03, -2.6615e-03,  1.1215e-02]],\n",
      "\n",
      "         [[-8.1794e-03, -2.2417e-02, -3.4012e-02],\n",
      "          [-2.8553e-02, -2.9546e-02, -4.4372e-02],\n",
      "          [-5.0348e-02, -3.4973e-02, -5.2028e-02]],\n",
      "\n",
      "         [[ 1.4728e-02,  3.2834e-02,  2.6312e-02],\n",
      "          [ 1.3449e-02,  2.6407e-02,  2.6924e-02],\n",
      "          [ 2.5572e-02,  3.4316e-02,  2.6184e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.2026e-03, -2.3931e-03,  2.2182e-03],\n",
      "          [ 4.2555e-03, -6.4084e-03,  7.8548e-03],\n",
      "          [ 2.0510e-02,  1.8644e-02,  2.3280e-02]],\n",
      "\n",
      "         [[-1.2471e-02,  1.3008e-02,  1.0010e-02],\n",
      "          [-1.7496e-03,  6.1331e-03,  4.3366e-03],\n",
      "          [ 5.2269e-03,  1.5111e-02, -8.1881e-03]],\n",
      "\n",
      "         [[-3.7337e-02,  1.9923e-02, -2.4149e-02],\n",
      "          [-4.9487e-02, -1.0510e-02, -4.2107e-02],\n",
      "          [-5.7684e-03, -4.8632e-03, -1.8332e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.2013e-03, -1.5208e-02, -1.6507e-02],\n",
      "          [-8.8276e-03, -1.8698e-02, -1.6637e-03],\n",
      "          [-1.2015e-02,  2.9667e-03,  6.2300e-03]],\n",
      "\n",
      "         [[-1.8341e-02, -9.0521e-03,  2.6030e-02],\n",
      "          [ 3.5930e-02,  5.3049e-02,  5.8487e-02],\n",
      "          [-1.3661e-02, -3.6888e-03, -7.1606e-03]],\n",
      "\n",
      "         [[-1.2594e-02, -4.0898e-02,  1.7162e-03],\n",
      "          [-1.7420e-02, -4.3435e-02, -1.3183e-02],\n",
      "          [-3.7506e-02, -5.5707e-02, -3.0051e-02]]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1971, 0.1771, 0.1303, 0.1995, 0.1839, 0.0934, 0.2333, 0.2236, 0.1654,\n",
      "        0.1280, 0.0842, 0.1085, 0.3168, 0.2032, 0.3246, 0.2184, 0.3208, 0.2824,\n",
      "        0.3408, 0.3339, 0.3307, 0.5571, 0.2821, 0.3081, 0.2114, 0.2971, 0.2361,\n",
      "        0.5500, 0.1221, 0.3381, 0.1528, 0.1544, 0.1982, 0.0582, 0.1812, 0.2489,\n",
      "        0.1954, 0.0705, 0.0918, 0.1328, 0.2616, 0.2013, 0.0720, 0.1573, 0.1919,\n",
      "        0.0813, 0.1170, 0.2504, 0.2863, 0.3032, 0.1476, 0.3696, 0.1870, 0.2097,\n",
      "        0.1907, 0.2364, 0.1642, 0.1079, 0.2531, 0.1703, 0.1266, 0.0814, 0.2407,\n",
      "        0.2609, 0.2705, 0.2128, 0.5007, 0.2375, 0.0802, 0.2896, 0.1776, 0.0887,\n",
      "        0.1094, 0.1834, 0.2812, 0.1971, 0.2021, 0.3443, 0.1411, 0.1362, 0.2676,\n",
      "        0.1618, 0.2723, 0.2727, 0.2528, 0.0982, 0.4707, 0.2239, 0.3649, 0.1987,\n",
      "        0.0815, 0.2543, 0.3322, 0.1561, 0.2336, 0.1294, 0.2570, 0.1700, 0.1374,\n",
      "        0.2215, 0.5015, 0.3132, 0.1487, 0.1174, 0.0916, 0.2130, 0.1393, 0.3057,\n",
      "        0.5634, 0.1018, 0.0994, 0.0492, 0.4427, 0.3142, 0.4002, 0.1334, 0.2174,\n",
      "        0.5522, 0.2806, 0.2784, 0.4333, 0.2602, 0.3788, 0.1827, 0.2664, 0.1077,\n",
      "        0.3001, 0.2428, 0.5130, 0.0829, 0.1254, 0.1996, 0.1451, 0.2253, 0.1467,\n",
      "        0.3712, 0.0794, 0.5425, 0.2058, 0.2103, 0.1288, 0.4993, 0.1815, 0.1845,\n",
      "        0.4154, 0.3817, 0.2054, 0.2205, 0.1471, 0.4964, 0.4202, 0.0801, 0.0623,\n",
      "        0.3536, 0.2760, 0.3840, 0.1632, 0.1402, 0.2674, 0.0844, 0.2305, 0.2259,\n",
      "        0.2146, 0.4181, 0.2821, 0.2926, 0.3416, 0.4640, 0.3025, 0.3732, 0.5871,\n",
      "        0.0616, 0.2797, 0.3042, 0.2173, 0.3550, 0.2096, 0.2449, 0.3428, 0.2868,\n",
      "        0.3543, 0.4667, 0.3220, 0.3805, 0.2632, 0.2160, 0.1924, 0.4074, 0.4966,\n",
      "        0.3623, 0.1670, 0.1321, 0.2374, 0.2118, 0.1522, 0.1668, 0.3836, 0.0983,\n",
      "        0.3729, 0.3943, 0.4353, 0.2270, 0.1508, 0.3133, 0.3850, 0.5774, 0.1892,\n",
      "        0.2822, 0.0907, 0.2364, 0.0964, 0.2360, 0.0699, 0.2938, 0.5100, 0.3348,\n",
      "        0.2339, 0.1145, 0.2155, 0.2266, 0.2829, 0.2341, 0.1891, 0.2906, 0.2681,\n",
      "        0.3876, 0.3915, 0.1844, 0.1889, 0.4405, 0.1405, 0.3460, 0.2724, 0.2567,\n",
      "        0.2785, 0.1148, 0.1607, 0.1754, 0.0883, 0.1649, 0.1268, 0.2356, 0.2811,\n",
      "        0.0766, 0.1424, 0.1683, 0.3979, 0.2685, 0.6383, 0.1087, 0.3180, 0.1760,\n",
      "        0.3634, 0.2615, 0.1999, 0.2541])\n",
      "tensor([-0.0162, -0.2033,  0.0294, -0.1697, -0.1840, -0.0309, -0.2039, -0.1426,\n",
      "        -0.0443, -0.0886, -0.0647, -0.0968, -0.0380, -0.2073, -0.3061,  0.1443,\n",
      "        -0.3079, -0.1232, -0.1627, -0.0980, -0.2471, -0.2837, -0.1201, -0.2893,\n",
      "        -0.2303, -0.3562, -0.0825, -0.3483,  0.0707, -0.1321, -0.1074, -0.1451,\n",
      "         0.0235,  0.0225, -0.1885, -0.2507, -0.2461,  0.0631, -0.0023, -0.1209,\n",
      "        -0.2581, -0.1640, -0.0172, -0.1143, -0.2096, -0.0158,  0.0128, -0.1332,\n",
      "        -0.3139, -0.2294, -0.1527, -0.3503,  0.2086,  0.0785, -0.1597, -0.1990,\n",
      "         0.0346,  0.0388, -0.1269,  0.1019,  0.0981, -0.0390, -0.2537, -0.1356,\n",
      "        -0.1796, -0.2422, -0.4517, -0.3124, -0.0177, -0.2615, -0.1567,  0.0212,\n",
      "        -0.0753, -0.1426, -0.2788,  0.0062, -0.1895, -0.2327, -0.1298, -0.1200,\n",
      "        -0.1917, -0.0987, -0.1916, -0.1666, -0.2729,  0.1287, -0.4620, -0.2259,\n",
      "        -0.2270,  0.1939,  0.0230, -0.3303, -0.3202, -0.1292, -0.0716,  0.0048,\n",
      "        -0.2579, -0.0116, -0.0557, -0.1229, -0.4804, -0.2351, -0.1367, -0.0578,\n",
      "        -0.0537, -0.2743, -0.0827, -0.1922, -0.3481, -0.0358, -0.1094,  0.0138,\n",
      "        -0.1888, -0.2592, -0.3293, -0.0820, -0.1839, -0.1636, -0.3163, -0.0246,\n",
      "        -0.1667, -0.1653, -0.3076, -0.2229, -0.1834, -0.0536, -0.0621, -0.1752,\n",
      "        -0.5243, -0.1933, -0.1119, -0.2283, -0.0437, -0.1777, -0.1300, -0.2519,\n",
      "        -0.0456, -0.6305, -0.1364, -0.2138,  0.0406, -0.5287, -0.2014, -0.1442,\n",
      "        -0.1930, -0.3033,  0.1030, -0.1499, -0.2297, -0.5301, -0.2543, -0.0417,\n",
      "         0.0429, -0.3218, -0.1611, -0.2562, -0.1187, -0.1001,  0.0225,  0.0996,\n",
      "        -0.2138, -0.2019,  0.0808, -0.0121, -0.2364, -0.3247, -0.1482, -0.4846,\n",
      "        -0.3449, -0.1365, -0.6664,  0.0418, -0.2807, -0.0961, -0.2378, -0.1834,\n",
      "        -0.1890, -0.0377, -0.3056, -0.1843, -0.1357, -0.3038, -0.2680, -0.4143,\n",
      "        -0.2633, -0.1750, -0.1856, -0.2405, -0.1082, -0.2250, -0.1268, -0.1094,\n",
      "         0.0594, -0.1419, -0.1178, -0.1602, -0.0328, -0.0194, -0.1985,  0.0470,\n",
      "        -0.1887, -0.2776, -0.0930, -0.4092, -0.3378, -0.7252,  0.0260, -0.1829,\n",
      "         0.0561, -0.2227, -0.0026, -0.3218, -0.0093, -0.2843, -0.5121, -0.2337,\n",
      "        -0.0836, -0.0818, -0.1296, -0.2090,  0.0169, -0.1899, -0.1892, -0.3075,\n",
      "        -0.3108, -0.2986, -0.4712, -0.1823, -0.1893, -0.3131, -0.0876, -0.1166,\n",
      "        -0.2995, -0.0831, -0.3427, -0.0772, -0.1460, -0.1611,  0.0203, -0.0627,\n",
      "        -0.0610, -0.2574, -0.1383,  0.0470, -0.0302, -0.1638, -0.3323, -0.1741,\n",
      "        -0.6307, -0.0772, -0.2123, -0.1559, -0.0459, -0.2416, -0.0143, -0.2079])\n",
      "tensor([[[[-1.1645e-02, -1.9010e-02, -2.1876e-02],\n",
      "          [ 2.0482e-02,  2.3962e-02,  2.9161e-02],\n",
      "          [ 4.3672e-02,  3.3278e-02,  4.9908e-02]],\n",
      "\n",
      "         [[-7.4040e-03,  2.8083e-03, -4.7339e-03],\n",
      "          [ 6.9030e-03,  1.4271e-02, -3.6954e-03],\n",
      "          [-3.1341e-03,  1.3736e-02,  1.6127e-03]],\n",
      "\n",
      "         [[ 1.8676e-02, -1.0553e-02, -1.4233e-02],\n",
      "          [ 8.9944e-03, -2.5068e-03, -1.2145e-02],\n",
      "          [-4.9455e-03, -2.9206e-02, -9.6385e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2655e-02,  1.7691e-02,  9.8264e-04],\n",
      "          [ 7.4271e-03,  7.6115e-03,  1.1135e-02],\n",
      "          [ 2.3242e-02,  1.1058e-02,  4.0498e-03]],\n",
      "\n",
      "         [[ 1.8557e-02,  1.2472e-02,  1.7220e-02],\n",
      "          [-4.8544e-03,  8.3627e-03,  2.2811e-02],\n",
      "          [-5.1675e-03,  2.3264e-02,  3.4068e-02]],\n",
      "\n",
      "         [[ 2.4934e-02,  2.2373e-02,  4.2614e-02],\n",
      "          [ 1.3486e-02,  1.6760e-03,  1.3019e-02],\n",
      "          [-6.2821e-03, -1.5112e-03, -8.9229e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.8089e-04, -6.3011e-03,  5.9932e-03],\n",
      "          [ 1.5936e-02,  1.3394e-02,  2.9934e-02],\n",
      "          [ 2.3149e-02,  2.0709e-02,  2.5485e-02]],\n",
      "\n",
      "         [[-2.0015e-02, -3.3349e-02, -8.0396e-03],\n",
      "          [-7.2800e-03, -1.2187e-02, -2.0389e-04],\n",
      "          [-1.3138e-02, -2.0427e-02, -1.6286e-02]],\n",
      "\n",
      "         [[-6.7681e-03,  5.0045e-03, -2.6683e-03],\n",
      "          [-2.1073e-02,  2.8275e-04, -1.8205e-02],\n",
      "          [-1.7382e-02, -5.0244e-03, -3.0386e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1035e-02, -2.2964e-02, -1.1028e-02],\n",
      "          [-6.3256e-03, -4.1667e-03, -1.7323e-02],\n",
      "          [-1.3611e-02, -2.3468e-02, -1.6436e-02]],\n",
      "\n",
      "         [[ 7.3663e-03,  6.6219e-03,  5.2776e-03],\n",
      "          [-3.5464e-03,  3.2750e-03, -9.1126e-03],\n",
      "          [ 3.5593e-04, -1.0151e-02, -1.9123e-02]],\n",
      "\n",
      "         [[ 1.8193e-03,  8.8087e-03,  5.1361e-03],\n",
      "          [ 3.1915e-03,  2.5287e-02,  2.4939e-02],\n",
      "          [ 1.3968e-02,  1.9613e-02,  2.2382e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.1548e-03,  8.8964e-03,  2.0143e-03],\n",
      "          [ 1.1327e-02,  1.3251e-02,  1.4014e-02],\n",
      "          [ 7.2196e-03,  1.3045e-02,  2.4827e-02]],\n",
      "\n",
      "         [[-1.5025e-02,  5.0530e-03,  7.4766e-03],\n",
      "          [-2.4685e-02, -1.6732e-02, -1.0888e-02],\n",
      "          [-2.8064e-02, -1.1875e-02, -3.4120e-03]],\n",
      "\n",
      "         [[ 2.8449e-02,  1.4594e-02,  6.9441e-03],\n",
      "          [ 2.4799e-02,  1.9453e-02,  1.1294e-02],\n",
      "          [-1.0787e-02, -2.1006e-02, -1.0372e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4967e-02,  8.2449e-03,  2.0244e-03],\n",
      "          [ 1.4287e-02, -6.3867e-03, -8.0757e-03],\n",
      "          [ 2.7547e-02,  1.0791e-02,  1.6567e-02]],\n",
      "\n",
      "         [[ 3.6191e-02,  3.8918e-02,  3.9028e-02],\n",
      "          [-8.3489e-04,  1.3273e-02,  2.0172e-02],\n",
      "          [-2.0652e-02, -5.4010e-03,  1.7147e-03]],\n",
      "\n",
      "         [[ 2.0373e-04,  3.5919e-03,  8.5592e-03],\n",
      "          [ 6.2363e-03, -9.3086e-05,  1.2940e-02],\n",
      "          [ 1.3152e-02,  1.0732e-02,  1.9896e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7400e-02, -6.7019e-03, -9.1787e-03],\n",
      "          [-9.9672e-03,  2.6298e-04,  3.3439e-03],\n",
      "          [ 1.5721e-02,  1.4216e-02,  2.0509e-02]],\n",
      "\n",
      "         [[ 2.1410e-02,  3.6914e-02,  2.8239e-02],\n",
      "          [ 3.8158e-02,  4.8944e-02,  3.4652e-02],\n",
      "          [ 3.1723e-02,  4.4208e-02,  4.0035e-02]],\n",
      "\n",
      "         [[-3.3437e-03, -1.0482e-02, -5.3990e-03],\n",
      "          [-5.3186e-03,  1.1394e-02,  1.7593e-03],\n",
      "          [-5.6652e-03, -6.6373e-03, -1.3492e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7099e-02, -1.8145e-03, -1.3040e-02],\n",
      "          [-2.2750e-02, -3.6062e-03, -8.0294e-03],\n",
      "          [-1.6087e-02, -1.0175e-02, -1.3529e-02]],\n",
      "\n",
      "         [[ 4.1701e-04, -5.1785e-03, -2.1884e-02],\n",
      "          [ 2.6919e-03,  8.9139e-03, -1.4217e-04],\n",
      "          [-7.3746e-03, -6.6853e-03, -2.3725e-02]],\n",
      "\n",
      "         [[ 1.9425e-02,  1.3175e-02,  1.7511e-02],\n",
      "          [ 1.8235e-02,  4.4286e-02,  2.3767e-02],\n",
      "          [ 2.6504e-02,  3.3104e-02,  1.9696e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0177e-02, -1.0701e-02, -2.0428e-02],\n",
      "          [-1.7986e-02,  5.9928e-03, -1.0584e-03],\n",
      "          [-1.8794e-02, -1.8773e-03, -6.9449e-03]],\n",
      "\n",
      "         [[-2.8498e-03,  1.6427e-03,  1.4575e-04],\n",
      "          [-5.4403e-03,  8.3667e-03, -9.4164e-03],\n",
      "          [-4.4999e-03,  5.4902e-03,  2.4863e-03]],\n",
      "\n",
      "         [[-1.3356e-02, -2.1525e-02,  5.3421e-04],\n",
      "          [-1.9160e-02, -2.4645e-02, -1.3791e-02],\n",
      "          [-6.1991e-03, -1.3174e-02, -3.6783e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3993e-03, -2.7823e-03,  7.6715e-03],\n",
      "          [-2.0649e-02, -1.2731e-02, -9.4138e-03],\n",
      "          [-1.3678e-03, -3.4410e-02, -2.6984e-02]],\n",
      "\n",
      "         [[-3.5651e-04,  2.0102e-03,  1.4130e-02],\n",
      "          [-1.3073e-02, -1.6616e-02, -1.2690e-02],\n",
      "          [-3.5934e-02, -4.1700e-02, -3.3968e-02]],\n",
      "\n",
      "         [[ 2.0470e-02,  8.0159e-04, -1.1607e-03],\n",
      "          [ 9.5101e-03,  3.0336e-02,  2.7362e-02],\n",
      "          [ 1.5588e-02,  3.2851e-02,  1.3015e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5574e-02, -3.2971e-02, -3.1939e-02],\n",
      "          [-2.2502e-02, -5.7187e-03, -5.6729e-03],\n",
      "          [-2.7309e-02, -1.6981e-02,  1.2832e-04]],\n",
      "\n",
      "         [[-1.1925e-02, -2.9479e-02, -2.0437e-02],\n",
      "          [-2.4408e-02, -2.2069e-02, -1.9965e-03],\n",
      "          [-2.3279e-02, -5.5140e-03,  2.5630e-02]],\n",
      "\n",
      "         [[-1.6100e-02, -8.2417e-03,  1.5266e-04],\n",
      "          [-2.6195e-03, -8.2754e-03, -2.9435e-02],\n",
      "          [-2.7493e-03, -2.4889e-02, -2.3583e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7985e-02,  1.8594e-02,  8.9198e-04],\n",
      "          [-1.7319e-02,  7.8735e-03, -2.8659e-03],\n",
      "          [ 3.8596e-03,  2.9061e-02,  2.4188e-02]],\n",
      "\n",
      "         [[-2.6735e-02, -1.4391e-02, -4.0148e-02],\n",
      "          [-2.6728e-02, -2.4455e-02, -6.9176e-03],\n",
      "          [-5.7244e-02, -2.1995e-04,  5.5438e-02]],\n",
      "\n",
      "         [[ 2.3487e-02,  2.7157e-03, -8.4719e-04],\n",
      "          [ 1.7886e-02,  5.4860e-03,  2.8059e-02],\n",
      "          [ 4.6468e-03,  1.8598e-02,  1.3761e-03]]]])\n",
      "tensor([0.2427, 0.2232, 0.2511, 0.2288, 0.2074, 0.2905, 0.2482, 0.3102, 0.2749,\n",
      "        0.2892, 0.2448, 0.1759, 0.2426, 0.2780, 0.2315, 0.2631, 0.3383, 0.2785,\n",
      "        0.2536, 0.2989, 0.2335, 0.2812, 0.3486, 0.2778, 0.2280, 0.2547, 0.3032,\n",
      "        0.2468, 0.2512, 0.2973, 0.2577, 0.3200, 0.2385, 0.2714, 0.2532, 0.2625,\n",
      "        0.3344, 0.2626, 0.1838, 0.2839, 0.2187, 0.2666, 0.2858, 0.2471, 0.2915,\n",
      "        0.2332, 0.2637, 0.2691, 0.2432, 0.2384, 0.2356, 0.2525, 0.2564, 0.2451,\n",
      "        0.2529, 0.2522, 0.2800, 0.3165, 0.2340, 0.2634, 0.2569, 0.1942, 0.2621,\n",
      "        0.2205, 0.2301, 0.2323, 0.2811, 0.1897, 0.2280, 0.3472, 0.2717, 0.3191,\n",
      "        0.2440, 0.2719, 0.2781, 0.2262, 0.3444, 0.2648, 0.2725, 0.2851, 0.2039,\n",
      "        0.2935, 0.2742, 0.2774, 0.2654, 0.2430, 0.2721, 0.2708, 0.3085, 0.2895,\n",
      "        0.2596, 0.2147, 0.3119, 0.3449, 0.2262, 0.2814, 0.2326, 0.2712, 0.2637,\n",
      "        0.2323, 0.3333, 0.2714, 0.2991, 0.2747, 0.2515, 0.2394, 0.2709, 0.2836,\n",
      "        0.2866, 0.2408, 0.2560, 0.2048, 0.2394, 0.2813, 0.3267, 0.2761, 0.2123,\n",
      "        0.2715, 0.2540, 0.2771, 0.3209, 0.1905, 0.3989, 0.2676, 0.2357, 0.2169,\n",
      "        0.3216, 0.3596, 0.2838, 0.2648, 0.2702, 0.2469, 0.2442, 0.2553, 0.2599,\n",
      "        0.2693, 0.2399, 0.2700, 0.2063, 0.2711, 0.2834, 0.2781, 0.2529, 0.2013,\n",
      "        0.2343, 0.2082, 0.3063, 0.1635, 0.2673, 0.2197, 0.2787, 0.2724, 0.2744,\n",
      "        0.2287, 0.2969, 0.2662, 0.2982, 0.2396, 0.3039, 0.2319, 0.2773, 0.2661,\n",
      "        0.2898, 0.2489, 0.3060, 0.2612, 0.2937, 0.3045, 0.2999, 0.2580, 0.2093,\n",
      "        0.2714, 0.2993, 0.2679, 0.2963, 0.2754, 0.2580, 0.2566, 0.2634, 0.2325,\n",
      "        0.2442, 0.2934, 0.2398, 0.2631, 0.2851, 0.2870, 0.2239, 0.2410, 0.2676,\n",
      "        0.2681, 0.2638, 0.2732, 0.2812, 0.2203, 0.2670, 0.2764, 0.2550, 0.3160,\n",
      "        0.2888, 0.2615, 0.2178, 0.2485, 0.2414, 0.2798, 0.2872, 0.2767, 0.2551,\n",
      "        0.2429, 0.2459, 0.3288, 0.3024, 0.2912, 0.2625, 0.3019, 0.2643, 0.2721,\n",
      "        0.2108, 0.2368, 0.2269, 0.1988, 0.2830, 0.2569, 0.2349, 0.2755, 0.2442,\n",
      "        0.2717, 0.2747, 0.2785, 0.2516, 0.2227, 0.2783, 0.2465, 0.2652, 0.2641,\n",
      "        0.2960, 0.2671, 0.2679, 0.2537, 0.2847, 0.2507, 0.2525, 0.2024, 0.2311,\n",
      "        0.2618, 0.2764, 0.3031, 0.2452, 0.2716, 0.2273, 0.2295, 0.2611, 0.2329,\n",
      "        0.2690, 0.2753, 0.2737, 0.2590, 0.2421, 0.2685, 0.3392, 0.3073, 0.1371,\n",
      "        0.3650, 0.2980, 0.2460, 0.2487, 0.2912, 0.2704, 0.2560, 0.2213, 0.2569,\n",
      "        0.2661, 0.2367, 0.2742, 0.2847, 0.3055, 0.2671, 0.2819, 0.2791, 0.2401,\n",
      "        0.2549, 0.2210, 0.3507, 0.2852, 0.2162, 0.2821, 0.2369, 0.2905, 0.2826,\n",
      "        0.2300, 0.2745, 0.2437, 0.2522, 0.2489, 0.2395, 0.2851, 0.2887, 0.2621,\n",
      "        0.2500, 0.2689, 0.2427, 0.3010, 0.3067, 0.2861, 0.2387, 0.2462, 0.2859,\n",
      "        0.2550, 0.2630, 0.2442, 0.2145, 0.2898, 0.2282, 0.2327, 0.2242, 0.2738,\n",
      "        0.2485, 0.2379, 0.3058, 0.2798, 0.2761, 0.2252, 0.2866, 0.2660, 0.3250,\n",
      "        0.2612, 0.2767, 0.3205, 0.2932, 0.3183, 0.2939, 0.3103, 0.2553, 0.2981,\n",
      "        0.3667, 0.3086, 0.2254, 0.2352, 0.2348, 0.2555, 0.2597, 0.2369, 0.3017,\n",
      "        0.2776, 0.2728, 0.3174, 0.2785, 0.2721, 0.2637, 0.2702, 0.3633, 0.2869,\n",
      "        0.2675, 0.3405, 0.2587, 0.2732, 0.2747, 0.2821, 0.2750, 0.2630, 0.2018,\n",
      "        0.2358, 0.3034, 0.3155, 0.3013, 0.2775, 0.2511, 0.2945, 0.1605, 0.2825,\n",
      "        0.2964, 0.2194, 0.2061, 0.2332, 0.2348, 0.2663, 0.2543, 0.2927, 0.2215,\n",
      "        0.2521, 0.2827, 0.1993, 0.2453, 0.2597, 0.2654, 0.2757, 0.2650, 0.2444,\n",
      "        0.2949, 0.2308, 0.3071, 0.1904, 0.3024, 0.2786, 0.3659, 0.2966, 0.2746,\n",
      "        0.2449, 0.2201, 0.2564, 0.2853, 0.2392, 0.2457, 0.2467, 0.2374, 0.2664,\n",
      "        0.2460, 0.3182, 0.1793, 0.2379, 0.2596, 0.2847, 0.2452, 0.1974, 0.2388,\n",
      "        0.2949, 0.2879, 0.2786, 0.2765, 0.3296, 0.2530, 0.2690, 0.2547, 0.2333,\n",
      "        0.2348, 0.2690, 0.2718, 0.2679, 0.2516, 0.2710, 0.2366, 0.2601, 0.2764,\n",
      "        0.2880, 0.2008, 0.2637, 0.2263, 0.2511, 0.2604, 0.2805, 0.2989, 0.2965,\n",
      "        0.2597, 0.2767, 0.2553, 0.2959, 0.2512, 0.2925, 0.3008, 0.2423, 0.2394,\n",
      "        0.2708, 0.3704, 0.2879, 0.2532, 0.2248, 0.2023, 0.2279, 0.2366, 0.3082,\n",
      "        0.2980, 0.2909, 0.2777, 0.4293, 0.2658, 0.2940, 0.2418, 0.2816, 0.3247,\n",
      "        0.2647, 0.2216, 0.2758, 0.2421, 0.2078, 0.2332, 0.2271, 0.2611, 0.3650,\n",
      "        0.2017, 0.2598, 0.2160, 0.2641, 0.1408, 0.2664, 0.2502, 0.2553, 0.2227,\n",
      "        0.2417, 0.2696, 0.2388, 0.2833, 0.2333, 0.2667, 0.2224, 0.2691, 0.2710,\n",
      "        0.2459, 0.2674, 0.2430, 0.2593, 0.1851, 0.2950, 0.3664, 0.2212, 0.3026,\n",
      "        0.1840, 0.3443, 0.2140, 0.3717, 0.2360, 0.3081, 0.2638, 0.2233])\n",
      "tensor([-0.1986, -0.1593, -0.2054, -0.1598, -0.1268, -0.3226, -0.1597, -0.3477,\n",
      "        -0.2497, -0.2730, -0.2319, -0.0286, -0.1899, -0.2813, -0.1733, -0.2412,\n",
      "        -0.3712, -0.2747, -0.2053, -0.2585, -0.1535, -0.2748, -0.3241, -0.2525,\n",
      "        -0.1906, -0.2252, -0.3436, -0.2202, -0.1664, -0.2716, -0.1920, -0.3399,\n",
      "        -0.2026, -0.2972, -0.2616, -0.2238, -0.2486, -0.2606, -0.0893, -0.3572,\n",
      "        -0.1283, -0.2583, -0.2450, -0.1523, -0.3165, -0.1445, -0.2522, -0.1963,\n",
      "        -0.1794, -0.1071, -0.1662, -0.2053, -0.2530, -0.1447, -0.2517, -0.2062,\n",
      "        -0.2817, -0.3376, -0.1382, -0.2389, -0.2557, -0.0156, -0.2169, -0.1763,\n",
      "        -0.1486, -0.2122, -0.2002, -0.0716, -0.2089, -0.3580, -0.2588, -0.3599,\n",
      "        -0.1528, -0.2107, -0.2925, -0.1855, -0.3970, -0.1257, -0.2574, -0.2412,\n",
      "        -0.0863, -0.3065, -0.2701, -0.3380, -0.2485, -0.1935, -0.2987, -0.2279,\n",
      "        -0.3600, -0.2764, -0.2480, -0.1208, -0.3378, -0.2661, -0.1677, -0.2470,\n",
      "        -0.2152, -0.2591, -0.1936, -0.1543, -0.4117, -0.1570, -0.2372, -0.2997,\n",
      "        -0.2124, -0.2034, -0.1848, -0.3070, -0.3438, -0.1839, -0.1937, -0.0916,\n",
      "        -0.2338, -0.3558, -0.1967, -0.3303, -0.1398, -0.2177, -0.1665, -0.1857,\n",
      "        -0.3115, -0.1049, -0.4229, -0.2408, -0.1320, -0.1631, -0.3378, -0.3300,\n",
      "        -0.3183, -0.2268, -0.2787, -0.1950, -0.1950, -0.1463, -0.2437, -0.2297,\n",
      "        -0.1282, -0.2164, -0.1179, -0.2437, -0.2611, -0.2656, -0.1948, -0.1208,\n",
      "        -0.1668, -0.1351, -0.2713, -0.0560, -0.2243, -0.1318, -0.2356, -0.2720,\n",
      "        -0.2051, -0.1736, -0.2891, -0.2627, -0.3358, -0.1779, -0.2309, -0.1477,\n",
      "        -0.2685, -0.1882, -0.2629, -0.1983, -0.3522, -0.1905, -0.2778, -0.3395,\n",
      "        -0.2895, -0.2240, -0.1150, -0.2462, -0.2426, -0.2581, -0.3133, -0.2315,\n",
      "        -0.2271, -0.2077, -0.2109, -0.1371, -0.1323, -0.2529, -0.1716, -0.2532,\n",
      "        -0.2277, -0.2084, -0.1803, -0.1868, -0.2404, -0.2166, -0.2197, -0.2870,\n",
      "        -0.3062, -0.1507, -0.1054, -0.2199, -0.2415, -0.3310, -0.2700, -0.1568,\n",
      "        -0.1449, -0.2610, -0.1828, -0.2648, -0.3134, -0.2937, -0.2687, -0.2115,\n",
      "        -0.2164, -0.4522, -0.2999, -0.3032, -0.2292, -0.3099, -0.2642, -0.2695,\n",
      "        -0.1441, -0.1671, -0.1570, -0.1415, -0.2222, -0.1736, -0.1481, -0.2573,\n",
      "        -0.2060, -0.1703, -0.2360, -0.1770, -0.2132, -0.2016, -0.3001, -0.1518,\n",
      "        -0.2086, -0.2805, -0.2698, -0.2292, -0.1293, -0.2514, -0.2600, -0.2454,\n",
      "        -0.1744, -0.1029, -0.1679, -0.2353, -0.2007, -0.3363, -0.1640, -0.2430,\n",
      "        -0.1699, -0.1697, -0.1837, -0.1625, -0.2415, -0.2687, -0.2305, -0.2029,\n",
      "        -0.2209, -0.2240, -0.2675, -0.3233,  0.1462, -0.4777, -0.2376, -0.1489,\n",
      "        -0.1462, -0.3055, -0.2234, -0.1697, -0.1952, -0.2131, -0.2340, -0.2039,\n",
      "        -0.3054, -0.2596, -0.3470, -0.2176, -0.2706, -0.2897, -0.1729, -0.2300,\n",
      "        -0.1066, -0.3556, -0.2912, -0.1777, -0.2007, -0.1699, -0.3009, -0.3046,\n",
      "        -0.1693, -0.2602, -0.2053, -0.1810, -0.1808, -0.1730, -0.3757, -0.1808,\n",
      "        -0.1805, -0.1895, -0.2643, -0.2075, -0.2365, -0.1975, -0.3064, -0.1984,\n",
      "        -0.1811, -0.3676, -0.1198, -0.1485, -0.1770, -0.0781, -0.2052, -0.1360,\n",
      "        -0.1417, -0.1691, -0.2395, -0.1785, -0.1747, -0.2484, -0.2717, -0.3096,\n",
      "        -0.1465, -0.2239, -0.2584, -0.3572, -0.2311, -0.2878, -0.3841, -0.3475,\n",
      "        -0.3896, -0.1891, -0.2861, -0.2431, -0.2837, -0.4365, -0.3353, -0.1802,\n",
      "        -0.1976, -0.1529, -0.1978, -0.2535, -0.1954, -0.2667, -0.2813, -0.2487,\n",
      "        -0.3070, -0.2339, -0.2212, -0.1925, -0.2224, -0.4178, -0.3151, -0.2663,\n",
      "        -0.3581, -0.1935, -0.2385, -0.2424, -0.1850, -0.2265, -0.1803, -0.0777,\n",
      "        -0.1492, -0.3361, -0.4133, -0.3123, -0.2745, -0.1247, -0.3102,  0.0041,\n",
      "        -0.1981, -0.3301, -0.2047, -0.1053, -0.1653, -0.1634, -0.1116, -0.2314,\n",
      "        -0.3191, -0.1818, -0.2657, -0.2220, -0.1029, -0.1999, -0.2702, -0.2139,\n",
      "        -0.2256, -0.2653, -0.1630, -0.3322, -0.1617, -0.3446,  0.0288, -0.2456,\n",
      "        -0.3171, -0.3580, -0.2857, -0.2520, -0.2031, -0.1522, -0.2203, -0.3490,\n",
      "        -0.1685, -0.1424, -0.1602, -0.1553, -0.3057, -0.2420, -0.3536, -0.0551,\n",
      "        -0.0987, -0.2272, -0.2619, -0.2035, -0.0906, -0.1976, -0.3040, -0.2732,\n",
      "        -0.3161, -0.2102, -0.3384, -0.1740, -0.1475, -0.1842, -0.1823, -0.1151,\n",
      "        -0.2183, -0.2010, -0.2659, -0.2205, -0.2567, -0.1633, -0.2213, -0.2658,\n",
      "        -0.2938, -0.1069, -0.2522, -0.1103, -0.2216, -0.2244, -0.2908, -0.2176,\n",
      "        -0.3605, -0.2374, -0.2391, -0.2251, -0.2256, -0.1339, -0.1970, -0.2970,\n",
      "        -0.2206, -0.2051, -0.2229, -0.3602, -0.2923, -0.2498, -0.1466, -0.0979,\n",
      "        -0.1686, -0.2158, -0.2881, -0.3002, -0.2760, -0.2496, -0.3536, -0.2868,\n",
      "        -0.3251, -0.1847, -0.3062, -0.3861, -0.2650, -0.1339, -0.1846, -0.1630,\n",
      "        -0.0630, -0.1717, -0.1415, -0.1906, -0.4611, -0.1391, -0.1920, -0.1369,\n",
      "        -0.1647, -0.0055, -0.2598, -0.2653, -0.2319, -0.1780, -0.1913, -0.2055,\n",
      "        -0.1891, -0.2625, -0.1633, -0.2497, -0.1696, -0.1907, -0.2431, -0.1825,\n",
      "        -0.2607, -0.1943, -0.2361, -0.0581, -0.2758, -0.2593, -0.1466, -0.3589,\n",
      "        -0.0439, -0.3440, -0.1089, -0.4219, -0.1503, -0.2792, -0.3035, -0.1156])\n",
      "tensor([[[[ 1.6218e-04, -1.4720e-02, -1.7000e-02],\n",
      "          [-1.2850e-02, -3.3085e-02, -3.6656e-02],\n",
      "          [ 2.7812e-02,  1.7691e-02, -1.8369e-02]],\n",
      "\n",
      "         [[ 1.0528e-02,  3.1379e-02,  2.4801e-02],\n",
      "          [-1.2698e-02, -2.9453e-02, -1.1834e-02],\n",
      "          [-9.4094e-03, -8.9462e-03, -3.1349e-02]],\n",
      "\n",
      "         [[-7.8447e-03, -2.9256e-02,  5.3590e-03],\n",
      "          [-1.3791e-02, -1.1116e-02,  5.0388e-03],\n",
      "          [-2.4919e-03,  7.3514e-03,  5.4013e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0276e-03, -1.0275e-02, -2.9986e-02],\n",
      "          [-3.8465e-03,  1.9549e-03, -1.6291e-02],\n",
      "          [-1.8100e-03,  8.3778e-03, -8.5481e-03]],\n",
      "\n",
      "         [[-1.8196e-02, -1.3533e-02, -1.7457e-02],\n",
      "          [ 2.2457e-02,  5.7402e-02,  1.9325e-02],\n",
      "          [-2.4977e-02, -3.2113e-02, -8.1780e-03]],\n",
      "\n",
      "         [[ 3.6550e-03,  4.9358e-03, -5.7597e-03],\n",
      "          [-1.6875e-02,  1.3999e-04,  3.7629e-04],\n",
      "          [-2.6272e-03,  1.0947e-03,  1.1145e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4018e-02,  3.9198e-03, -1.7189e-03],\n",
      "          [-1.3175e-03,  4.3503e-04, -1.1798e-02],\n",
      "          [-9.8003e-03, -1.7693e-02, -1.9910e-02]],\n",
      "\n",
      "         [[-1.4957e-02, -1.9796e-02, -2.8724e-02],\n",
      "          [ 5.8908e-03, -1.5228e-02, -5.6715e-03],\n",
      "          [ 2.9284e-03, -1.8028e-02, -7.1433e-03]],\n",
      "\n",
      "         [[-1.1625e-02, -3.3804e-02, -1.0025e-02],\n",
      "          [-1.6606e-02, -5.5716e-02, -2.3204e-02],\n",
      "          [-2.5758e-02, -4.3135e-02, -2.5901e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5007e-02, -1.4333e-02, -2.5937e-03],\n",
      "          [-2.3078e-02, -1.5820e-02, -2.2818e-03],\n",
      "          [-4.1318e-03, -8.0353e-03, -2.3236e-03]],\n",
      "\n",
      "         [[-1.8531e-02, -1.8004e-02, -2.8084e-02],\n",
      "          [-3.6680e-02, -6.8641e-02, -5.2469e-02],\n",
      "          [-1.1712e-02, -2.4334e-02, -1.6733e-02]],\n",
      "\n",
      "         [[-2.2078e-02, -2.9163e-02, -3.8717e-03],\n",
      "          [-7.0301e-03,  1.6718e-02,  5.4339e-03],\n",
      "          [-1.3131e-02,  1.1999e-02, -1.7480e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.2378e-03, -3.4890e-03, -2.0851e-03],\n",
      "          [ 1.5306e-02, -2.1752e-02, -8.7682e-03],\n",
      "          [ 2.2460e-02,  9.9175e-03, -3.3635e-03]],\n",
      "\n",
      "         [[ 7.4677e-03, -9.1762e-03, -9.2569e-05],\n",
      "          [ 1.9441e-04,  1.2344e-03, -8.9978e-03],\n",
      "          [-5.1243e-04,  2.1850e-04, -4.8828e-03]],\n",
      "\n",
      "         [[ 1.7078e-02,  3.3955e-03,  9.3503e-03],\n",
      "          [ 2.0334e-02, -1.0621e-04, -8.2017e-05],\n",
      "          [ 1.0706e-02, -1.8414e-03,  1.0828e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.2008e-02,  2.3494e-02,  2.5386e-02],\n",
      "          [ 1.9307e-02,  2.3924e-02,  2.8972e-02],\n",
      "          [ 9.9003e-03,  2.0158e-02,  2.2655e-02]],\n",
      "\n",
      "         [[-9.8395e-03, -1.1114e-02, -3.7696e-03],\n",
      "          [-2.9508e-02, -3.6956e-02, -1.8228e-02],\n",
      "          [-1.3663e-03, -2.5845e-03,  1.0352e-02]],\n",
      "\n",
      "         [[-7.3867e-03, -2.5413e-02, -2.1942e-02],\n",
      "          [-1.6699e-02, -1.5133e-02, -1.3030e-02],\n",
      "          [-2.0090e-02,  3.7970e-03, -1.0341e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.6157e-02, -1.6883e-02, -2.8328e-04],\n",
      "          [-7.7759e-03, -2.4465e-03, -1.4641e-02],\n",
      "          [ 2.4639e-02,  3.9862e-02,  2.1048e-02]],\n",
      "\n",
      "         [[ 2.4491e-03, -9.3885e-03, -1.1786e-02],\n",
      "          [ 2.5301e-02,  2.5625e-04,  7.1335e-03],\n",
      "          [ 2.2342e-02,  1.9042e-02,  7.2526e-03]],\n",
      "\n",
      "         [[-1.4652e-02, -2.7802e-02, -4.3564e-03],\n",
      "          [-1.7961e-02, -4.3846e-02,  2.7409e-03],\n",
      "          [-4.7968e-03, -8.4231e-03,  1.2070e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.0171e-02, -3.3546e-02, -1.6728e-02],\n",
      "          [-1.7847e-02, -5.1713e-02, -2.6780e-02],\n",
      "          [-1.3145e-03, -4.3181e-03, -9.6373e-03]],\n",
      "\n",
      "         [[-5.3917e-03, -2.0410e-04,  2.7798e-03],\n",
      "          [-9.6882e-04, -2.5141e-02,  1.4804e-02],\n",
      "          [ 2.8748e-02,  9.0832e-03,  4.2548e-02]],\n",
      "\n",
      "         [[-1.5698e-02, -1.9303e-02, -9.1469e-03],\n",
      "          [-2.0025e-02, -1.1131e-02, -3.3902e-02],\n",
      "          [-5.7436e-03, -7.3640e-03, -1.0044e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.8612e-03, -4.5370e-03, -1.2354e-02],\n",
      "          [-5.9245e-03, -1.7058e-02, -2.8041e-02],\n",
      "          [-1.0435e-02,  7.6695e-04, -1.0578e-02]],\n",
      "\n",
      "         [[ 9.5200e-03, -5.1975e-03,  1.2947e-02],\n",
      "          [ 4.4305e-03, -2.3992e-02, -8.4569e-04],\n",
      "          [ 4.6608e-03,  9.6787e-03,  8.2174e-03]],\n",
      "\n",
      "         [[ 5.1559e-03,  4.4635e-04, -7.9934e-03],\n",
      "          [ 3.3069e-03,  1.4450e-02,  8.9234e-03],\n",
      "          [ 6.3402e-03,  1.9043e-02,  1.9021e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.6964e-03, -1.3777e-02,  6.0539e-03],\n",
      "          [-1.5745e-03, -2.3391e-02, -1.0052e-02],\n",
      "          [ 9.5183e-03, -1.2251e-02,  2.2436e-03]],\n",
      "\n",
      "         [[ 1.0375e-02,  3.5875e-03, -5.7940e-04],\n",
      "          [ 7.0412e-03, -1.0673e-02, -4.9120e-03],\n",
      "          [-2.6034e-03,  1.1306e-02,  7.0696e-03]],\n",
      "\n",
      "         [[-1.7509e-02, -2.3182e-02, -1.7897e-02],\n",
      "          [-1.7769e-03,  1.9672e-03, -7.3220e-03],\n",
      "          [-6.6833e-03,  9.8286e-03,  2.0653e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8375e-02, -8.1936e-03,  1.8009e-02],\n",
      "          [ 1.5829e-02, -1.3571e-02, -1.9335e-02],\n",
      "          [ 4.0766e-03, -1.5722e-02, -5.0620e-02]],\n",
      "\n",
      "         [[-5.5310e-03, -1.8996e-02, -7.9436e-03],\n",
      "          [ 1.3825e-03, -4.9608e-02,  1.7256e-03],\n",
      "          [ 7.6629e-03, -7.6101e-03,  1.2541e-02]],\n",
      "\n",
      "         [[ 1.8052e-02,  3.1718e-02,  4.2556e-03],\n",
      "          [-3.6760e-03,  3.0490e-03, -1.2264e-02],\n",
      "          [-8.9404e-03, -1.6604e-02,  1.6348e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.3192e-03,  1.8204e-02,  1.8114e-02],\n",
      "          [-6.1202e-03,  1.5905e-03,  2.0264e-02],\n",
      "          [-1.1471e-02, -1.5697e-02,  9.0871e-03]],\n",
      "\n",
      "         [[ 3.7707e-03,  8.0599e-03,  1.8290e-02],\n",
      "          [ 1.7257e-02,  6.9638e-03,  1.8746e-02],\n",
      "          [ 1.0751e-02,  1.3663e-02, -1.0081e-03]],\n",
      "\n",
      "         [[ 1.9711e-02, -1.4569e-02, -2.4663e-02],\n",
      "          [ 2.5966e-03, -2.4807e-02,  9.3861e-03],\n",
      "          [-1.2876e-03,  1.3974e-03,  1.3434e-02]]]])\n",
      "tensor([0.4474, 0.5138, 0.4335, 0.3421, 0.3855, 0.3495, 0.3741, 0.5836, 0.4327,\n",
      "        0.5043, 0.4618, 0.3866, 0.3498, 0.4798, 0.3310, 0.3913, 0.3880, 0.5225,\n",
      "        0.3975, 0.3292, 0.4151, 0.4458, 0.3970, 0.3614, 0.3914, 0.4633, 0.3463,\n",
      "        0.3644, 0.3272, 0.4584, 0.4280, 0.4538, 0.4030, 0.4673, 0.4209, 0.3987,\n",
      "        0.4233, 0.3876, 0.4212, 0.3460, 0.3522, 0.3744, 0.4550, 0.2888, 0.4590,\n",
      "        0.4817, 0.4450, 0.5110, 0.4052, 0.4247, 0.3558, 0.3075, 0.4462, 0.4724,\n",
      "        0.4253, 0.3884, 0.4492, 0.3727, 0.4630, 0.3985, 0.3512, 0.3665, 0.3860,\n",
      "        0.5082, 0.4022, 0.3458, 0.4805, 0.5390, 0.4223, 0.4275, 0.4590, 0.4736,\n",
      "        0.3673, 0.5405, 0.3243, 0.5178, 0.4743, 0.3506, 0.3759, 0.4328, 0.3867,\n",
      "        0.4591, 0.3843, 0.4982, 0.5288, 0.3946, 0.4589, 0.3197, 0.4676, 0.4806,\n",
      "        0.4308, 0.4235, 0.3284, 0.3877, 0.4140, 0.4469, 0.4041, 0.4407, 0.4356,\n",
      "        0.5120, 0.5059, 0.4628, 0.4585, 0.3311, 0.3424, 0.4150, 0.5170, 0.4593,\n",
      "        0.5228, 0.4252, 0.4214, 0.4995, 0.4098, 0.5380, 0.4874, 0.3719, 0.4649,\n",
      "        0.4320, 0.3277, 0.3743, 0.4360, 0.4838, 0.4399, 0.3763, 0.4150, 0.5147,\n",
      "        0.5012, 0.4382, 0.3655, 0.4037, 0.4498, 0.4720, 0.3914, 0.3237, 0.3208,\n",
      "        0.3224, 0.4291, 0.4009, 0.3947, 0.3779, 0.4349, 0.4120, 0.3274, 0.4334,\n",
      "        0.3740, 0.4189, 0.4288, 0.3071, 0.4260, 0.3410, 0.4375, 0.4407, 0.3750,\n",
      "        0.5853, 0.4518, 0.5045, 0.3005, 0.4968, 0.4155, 0.3755, 0.5514, 0.4146,\n",
      "        0.4677, 0.1404, 0.5001, 0.4193, 0.4246, 0.4452, 0.5109, 0.4488, 0.4574,\n",
      "        0.3896, 0.4145, 0.4497, 0.4245, 0.3971, 0.3957, 0.4072, 0.5305, 0.4986,\n",
      "        0.3733, 0.4280, 0.3469, 0.4178, 0.3766, 0.4029, 0.3814, 0.4493, 0.5132,\n",
      "        0.4080, 0.4155, 0.3635, 0.4391, 0.3489, 0.4228, 0.4833, 0.3494, 0.4406,\n",
      "        0.3795, 0.4298, 0.4910, 0.3878, 0.6299, 0.4322, 0.5436, 0.4140, 0.4312,\n",
      "        0.3161, 0.3612, 0.3597, 0.4281, 0.4506, 0.4294, 0.3646, 0.4110, 0.4038,\n",
      "        0.4098, 0.3901, 0.3928, 0.5421, 0.3629, 0.4078, 0.4586, 0.4217, 0.3953,\n",
      "        0.3997, 0.3838, 0.4374, 0.3576, 0.4217, 0.4128, 0.3904, 0.4137, 0.5145,\n",
      "        0.4039, 0.3577, 0.4429, 0.5639, 0.3848, 0.6104, 0.4482, 0.6203, 0.5336,\n",
      "        0.3480, 0.5401, 0.6044, 0.4077, 0.3469, 0.4281, 0.4631, 0.5948, 0.3479,\n",
      "        0.3689, 0.3658, 0.3191, 0.5492, 0.3410, 0.5386, 0.4041, 0.3373, 0.4186,\n",
      "        0.5187, 0.3933, 0.3188, 0.3502, 0.3736, 0.4238, 0.4752, 0.3322, 0.5078,\n",
      "        0.4317, 0.5318, 0.4413, 0.5510, 0.5648, 0.4130, 0.4017, 0.4304, 0.4077,\n",
      "        0.4285, 0.4360, 0.3749, 0.4261, 0.3905, 0.3030, 0.3412, 0.3768, 0.4507,\n",
      "        0.3127, 0.4592, 0.4298, 0.3936, 0.3106, 0.3869, 0.3594, 0.4046, 0.4722,\n",
      "        0.4373, 0.3902, 0.3515, 0.4448, 0.4299, 0.4347, 0.4693, 0.4807, 0.2549,\n",
      "        0.4171, 0.4387, 0.4156, 0.3976, 0.4092, 0.4953, 0.4824, 0.3468, 0.4382,\n",
      "        0.4179, 0.4668, 0.3299, 0.5986, 0.4949, 0.4167, 0.4996, 0.4528, 0.4550,\n",
      "        0.4945, 0.3415, 0.4658, 0.4356, 0.3976, 0.5439, 0.4643, 0.5122, 0.4669,\n",
      "        0.4463, 0.4810, 0.3492, 0.3961, 0.3593, 0.4053, 0.3878, 0.3959, 0.5001,\n",
      "        0.2808, 0.5470, 0.4448, 0.4894, 0.4621, 0.3417, 0.3485, 0.5060, 0.3637,\n",
      "        0.3774, 0.3248, 0.4520, 0.3936, 0.3403, 0.4660, 0.4114, 0.3643, 0.4196,\n",
      "        0.3903, 0.5128, 0.4221, 0.4115, 0.4240, 0.3610, 0.4999, 0.3672, 0.4721,\n",
      "        0.4252, 0.5590, 0.4694, 0.7322, 0.5849, 0.4749, 0.4426, 0.3934, 0.3909,\n",
      "        0.4576, 0.3636, 0.4146, 0.4129, 0.5081, 0.3681, 0.3652, 0.4254, 0.2945,\n",
      "        0.4142, 0.3145, 0.4304, 0.4252, 0.3493, 0.4257, 0.5133, 0.3261, 0.4367,\n",
      "        0.3637, 0.3712, 0.4183, 0.3772, 0.4418, 0.4231, 0.4133, 0.4731, 0.4955,\n",
      "        0.4046, 0.4079, 0.4719, 0.3875, 0.4673, 0.4129, 0.4569, 0.3530, 0.4793,\n",
      "        0.3844, 0.3785, 0.3343, 0.4351, 0.6512, 0.4295, 0.4122, 0.3788, 0.3692,\n",
      "        0.4343, 0.4214, 0.3873, 0.4566, 0.4456, 0.4107, 0.4596, 0.7082, 0.4452,\n",
      "        0.3515, 0.4785, 0.4217, 0.5756, 0.4312, 0.4047, 0.4043, 0.4764, 0.5489,\n",
      "        0.4430, 0.5559, 0.3744, 0.3951, 0.4376, 0.4752, 0.4340, 0.4399, 0.3586,\n",
      "        0.4161, 0.3930, 0.4599, 0.4354, 0.3448, 0.4649, 0.4442, 0.4275, 0.3881,\n",
      "        0.3247, 0.4909, 0.3426, 0.3989, 0.4320, 0.3363, 0.3991, 0.4732, 0.3514,\n",
      "        0.4736, 0.4244, 0.4603, 0.3298, 0.4357, 0.4353, 0.3742, 0.4191, 0.3880,\n",
      "        0.4212, 0.4527, 0.7213, 0.3969, 0.5217, 0.3786, 0.3512, 0.5318, 0.4138,\n",
      "        0.3243, 0.3244, 0.3652, 0.4774, 0.3997, 0.2800, 0.4562, 0.4463, 0.4816,\n",
      "        0.4290, 0.4399, 0.4633, 0.3575, 0.4774, 0.3105, 0.4356, 0.3797, 0.4304,\n",
      "        0.4261, 0.3740, 0.3370, 0.3917, 0.3637, 0.4347, 0.5235, 0.3845])\n",
      "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
      "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
      "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
      "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
      "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
      "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
      "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
      "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
      "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
      "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
      "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
      "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
      "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
      "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
      "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
      "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
      "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
      "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
      "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
      "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
      "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
      "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
      "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
      "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
      "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
      "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
      "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
      "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
      "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
      "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
      "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
      "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
      "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
      "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
      "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
      "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
      "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
      "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
      "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
      "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
      "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
      "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
      "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
      "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
      "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
      "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
      "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
      "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
      "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
      "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
      "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
      "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
      "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
      "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
      "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
      "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
      "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
      "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
      "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
      "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
      "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
      "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
      "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
      "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713])\n",
      "tensor([[[[ 0.0057]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[ 0.0167]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0085]],\n",
      "\n",
      "         [[-0.0274]],\n",
      "\n",
      "         [[ 0.0097]]],\n",
      "\n",
      "\n",
      "        [[[-0.0271]],\n",
      "\n",
      "         [[-0.0157]],\n",
      "\n",
      "         [[ 0.0543]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0206]],\n",
      "\n",
      "         [[-0.0308]],\n",
      "\n",
      "         [[ 0.0013]]],\n",
      "\n",
      "\n",
      "        [[[-0.0523]],\n",
      "\n",
      "         [[-0.0353]],\n",
      "\n",
      "         [[ 0.0394]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0382]],\n",
      "\n",
      "         [[-0.0264]],\n",
      "\n",
      "         [[-0.0443]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0494]],\n",
      "\n",
      "         [[ 0.0436]],\n",
      "\n",
      "         [[ 0.0103]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0669]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0533]],\n",
      "\n",
      "         [[-0.0148]],\n",
      "\n",
      "         [[-0.0480]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0429]],\n",
      "\n",
      "         [[ 0.0129]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0300]],\n",
      "\n",
      "         [[-0.0092]],\n",
      "\n",
      "         [[ 0.0090]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[-0.0111]],\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [[-0.0080]]]])\n",
      "tensor([ 0.1694,  0.3368,  0.2993,  0.3745,  0.1513,  0.1781,  0.3167,  0.3947,\n",
      "         0.1858,  0.2068,  0.1090,  0.2042,  0.2955,  0.0765,  0.2023,  0.2487,\n",
      "         0.3295,  0.3349,  0.2532,  0.2739,  0.1661,  0.3432,  0.3424,  0.2969,\n",
      "         0.2226,  0.0993,  0.3328,  0.2349,  0.2894,  0.2296,  0.2719,  0.3945,\n",
      "         0.1990,  0.2564,  0.2557,  0.3541,  0.1848,  0.2513,  0.3101,  0.2782,\n",
      "         0.2109,  0.2441,  0.3282,  0.3248,  0.2499,  0.1873,  0.2643,  0.3949,\n",
      "         0.1962,  0.2587,  0.1708,  0.3381,  0.2238,  0.2498,  0.2787,  0.3783,\n",
      "         0.3445,  0.2681,  0.2956,  0.1146,  0.2688,  0.3479,  0.1295,  0.2843,\n",
      "         0.1552,  0.3026,  0.2738,  0.1891,  0.3568,  0.2302,  0.2199,  0.2070,\n",
      "         0.2119,  0.0971,  0.2482,  0.2264,  0.3555,  0.3113,  0.2386,  0.2654,\n",
      "         0.2975,  0.2666,  0.2180,  0.1451,  0.2460,  0.1734,  0.2358,  0.2891,\n",
      "         0.2091,  0.1971,  0.2185,  0.2008,  0.2461,  0.3726,  0.2028,  0.1993,\n",
      "         0.3652,  0.2258,  0.2606,  0.1900,  0.2764,  0.2011,  0.1973,  0.2958,\n",
      "         0.3222,  0.4117,  0.1475,  0.2674,  0.1928,  0.3615,  0.2774,  0.2143,\n",
      "         0.2688,  0.4286,  0.2560,  0.2777,  0.1339,  0.5103,  0.3238,  0.2417,\n",
      "         0.1529,  0.1843,  0.0579,  0.2288,  0.1797,  0.2803,  0.2279,  0.1579,\n",
      "         0.3196,  0.1842,  0.3378,  0.1688,  0.1654,  0.3049,  0.3533,  0.2948,\n",
      "         0.1140,  0.2503,  0.1892,  0.2647,  0.2405,  0.3880,  0.1933,  0.1918,\n",
      "         0.2511,  0.2901,  0.3151,  0.3252,  0.1296,  0.2491,  0.1417,  0.1295,\n",
      "         0.3062,  0.2836,  0.3483,  0.2306,  0.2741,  0.2700,  0.1873,  0.2431,\n",
      "         0.3526,  0.3546,  0.2721,  0.2708,  0.3065,  0.0832,  0.2968,  0.2286,\n",
      "         0.3276,  0.2695,  0.2452,  0.2444,  0.2857,  0.3365,  0.2784,  0.2933,\n",
      "         0.3397,  0.2231,  0.2330,  0.1486,  0.3846,  0.3104,  0.1724,  0.1724,\n",
      "         0.3466,  0.2978,  0.2582,  0.1879,  0.2419,  0.2249,  0.2720,  0.3735,\n",
      "         0.4259,  0.3754,  0.1731,  0.3698,  0.2349,  0.2694,  0.3148,  0.1658,\n",
      "         0.1181,  0.2994,  0.4018,  0.2126,  0.3864,  0.2955,  0.1848,  0.3686,\n",
      "         0.1972,  0.3265,  0.2319,  0.1676,  0.1756,  0.2367,  0.2139,  0.1974,\n",
      "         0.2561,  0.2619,  0.2170,  0.2284,  0.3486,  0.4500,  0.2563,  0.2559,\n",
      "         0.2814,  0.1797,  0.1736,  0.2013,  0.3411,  0.2245,  0.1385,  0.2284,\n",
      "         0.2230,  0.2566,  0.2301,  0.3639,  0.1380,  0.2381,  0.2590,  0.0830,\n",
      "         0.1863,  0.1267,  0.4501,  0.2741,  0.2590,  0.2782,  0.2248,  0.2718,\n",
      "         0.1949,  0.1815,  0.2969,  0.3168,  0.3389,  0.2790,  0.1594,  0.2752,\n",
      "         0.2947,  0.2909,  0.1418,  0.3336,  0.1953,  0.2646,  0.0879,  0.2553,\n",
      "         0.3335,  0.1943,  0.2777,  0.2386,  0.3676,  0.3042,  0.1234,  0.2615,\n",
      "         0.2548,  0.3224,  0.3462,  0.2090,  0.2142,  0.2054,  0.2115,  0.2153,\n",
      "         0.2163,  0.2509,  0.2429,  0.3326, -0.0527,  0.2244,  0.2319,  0.2674,\n",
      "         0.1103,  0.2320,  0.2822,  0.3234,  0.2818,  0.2093,  0.2261,  0.2900,\n",
      "         0.3127,  0.3456,  0.2592,  0.1677,  0.3924,  0.2694,  0.1997,  0.2973,\n",
      "         0.3324,  0.2270,  0.0656,  0.2964,  0.1948,  0.2383,  0.3021,  0.2510,\n",
      "         0.3117,  0.3185,  0.1721,  0.1867,  0.1665,  0.2851,  0.3512, -0.0486,\n",
      "         0.1558,  0.2213,  0.3281,  0.3861,  0.2375,  0.3057,  0.1178,  0.2681,\n",
      "         0.1921,  0.2211,  0.1679,  0.2877,  0.2495,  0.2451,  0.2678,  0.2393,\n",
      "         0.0988,  0.2778,  0.2465,  0.1747,  0.1005,  0.0502,  0.2809,  0.2810,\n",
      "         0.1716,  0.2114,  0.2213,  0.2817,  0.1506,  0.0769,  0.2381,  0.2411,\n",
      "         0.2942,  0.2543,  0.2556,  0.3451,  0.2948,  0.3040,  0.3204,  0.2757,\n",
      "         0.1657,  0.2941,  0.1301,  0.1854,  0.2866,  0.3198,  0.2127,  0.3608,\n",
      "         0.3440,  0.0954,  0.2586,  0.1709,  0.2007,  0.1967,  0.1972,  0.1942,\n",
      "         0.3201,  0.3484,  0.3437,  0.3153,  0.2020,  0.3251,  0.3227,  0.3038,\n",
      "         0.2634,  0.2364,  0.2492,  0.3080,  0.2591,  0.2391,  0.2720,  0.2601,\n",
      "         0.3210,  0.1818,  0.3526,  0.3579,  0.2861,  0.2526,  0.1642,  0.2897,\n",
      "         0.3996,  0.2651,  0.2031,  0.2502,  0.3694,  0.2085,  0.2804,  0.2233,\n",
      "         0.2309,  0.1609,  0.2369,  0.2116,  0.3549,  0.1635,  0.1642,  0.3072,\n",
      "         0.3077,  0.2152,  0.2821,  0.2857,  0.1701,  0.2305,  0.2134,  0.3189,\n",
      "         0.1061,  0.2628,  0.2608,  0.1749,  0.0820,  0.1815,  0.3566,  0.1204,\n",
      "         0.3159,  0.1595,  0.3790,  0.3272,  0.2086,  0.3096,  0.2253,  0.1456,\n",
      "         0.1346,  0.2304,  0.2913,  0.2727,  0.2027,  0.2688,  0.1958,  0.2277,\n",
      "         0.3036,  0.3250,  0.3000,  0.3328,  0.2417,  0.2665,  0.2473,  0.0913,\n",
      "         0.2503,  0.2543,  0.3710,  0.3321,  0.3693,  0.1099,  0.1701,  0.1758,\n",
      "         0.3888,  0.2206,  0.2766,  0.2813,  0.1755,  0.2616,  0.1544,  0.2519,\n",
      "         0.1945,  0.2452,  0.3405,  0.2446,  0.2426,  0.1822,  0.3002,  0.3037,\n",
      "         0.3118,  0.2414,  0.2326,  0.1303,  0.3081,  0.0979,  0.2776,  0.2918,\n",
      "         0.3848,  0.1789,  0.3622,  0.3005,  0.1923,  0.2672,  0.1663,  0.2998,\n",
      "         0.2710,  0.2040,  0.2565,  0.2289,  0.2552,  0.2121,  0.3532,  0.2293,\n",
      "         0.2510,  0.3085,  0.2368,  0.3000,  0.2111,  0.3456,  0.3422,  0.1576])\n",
      "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
      "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
      "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
      "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
      "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
      "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
      "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
      "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
      "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
      "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
      "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
      "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
      "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
      "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
      "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
      "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
      "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
      "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
      "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
      "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
      "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
      "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
      "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
      "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
      "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
      "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
      "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
      "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
      "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
      "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
      "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
      "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
      "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
      "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
      "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
      "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
      "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
      "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
      "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
      "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
      "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
      "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
      "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
      "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
      "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
      "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
      "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
      "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
      "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
      "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
      "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
      "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
      "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
      "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
      "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
      "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
      "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
      "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
      "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
      "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
      "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
      "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
      "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
      "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713])\n",
      "tensor([[[[-8.0284e-03, -5.7776e-03,  6.4154e-03],\n",
      "          [ 5.0498e-03, -6.7796e-03,  1.2691e-02],\n",
      "          [ 1.3331e-02,  1.4523e-02,  2.4522e-02]],\n",
      "\n",
      "         [[-1.9876e-03,  1.2466e-02,  1.0494e-02],\n",
      "          [-1.9364e-02, -1.6696e-02, -1.1857e-02],\n",
      "          [-1.1569e-02, -3.7674e-03, -3.4679e-03]],\n",
      "\n",
      "         [[-1.1440e-02, -1.3884e-02,  1.1559e-03],\n",
      "          [-1.7906e-02, -2.9349e-02, -1.3876e-02],\n",
      "          [-1.4057e-02, -2.6989e-02, -2.3963e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.3040e-03, -3.1167e-03, -1.3304e-02],\n",
      "          [ 7.1623e-03,  6.4669e-03,  1.6063e-02],\n",
      "          [-1.0750e-02, -1.0480e-02, -6.1070e-03]],\n",
      "\n",
      "         [[ 7.4484e-03,  6.3878e-03, -1.2579e-02],\n",
      "          [-7.7356e-03,  1.8112e-03, -1.7890e-02],\n",
      "          [-2.9142e-03,  7.7705e-03, -9.7314e-03]],\n",
      "\n",
      "         [[ 2.1760e-02,  2.2364e-02,  2.2731e-02],\n",
      "          [ 2.6681e-02,  2.9127e-02,  3.3356e-02],\n",
      "          [ 1.2892e-02, -3.5818e-03,  5.3022e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0597e-02, -9.1551e-03, -2.3418e-02],\n",
      "          [-1.0768e-02, -3.3171e-03, -1.8559e-02],\n",
      "          [-1.8607e-02, -4.2634e-03, -1.5591e-02]],\n",
      "\n",
      "         [[-2.6090e-02, -2.2517e-02, -3.0593e-02],\n",
      "          [-3.9406e-02, -2.6639e-02, -2.8202e-02],\n",
      "          [-2.6143e-02, -1.9647e-02, -2.1466e-02]],\n",
      "\n",
      "         [[-3.5259e-03,  1.6623e-03, -6.5624e-03],\n",
      "          [-5.0597e-03, -8.7162e-04, -5.3742e-03],\n",
      "          [-7.9651e-03, -9.7778e-03, -1.0736e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8492e-02, -3.6799e-03,  1.0043e-02],\n",
      "          [-5.2974e-03, -2.0757e-02, -1.5120e-02],\n",
      "          [ 2.1435e-02,  6.4916e-03,  4.7660e-03]],\n",
      "\n",
      "         [[-1.8810e-02, -6.0469e-04, -7.6999e-03],\n",
      "          [-1.7697e-02, -7.8692e-03, -1.6543e-02],\n",
      "          [-1.7206e-02, -2.4746e-02, -3.0270e-02]],\n",
      "\n",
      "         [[-3.1191e-02, -1.4363e-02,  2.2032e-03],\n",
      "          [-1.2033e-02, -2.3699e-03, -1.6630e-02],\n",
      "          [-1.2905e-02, -1.5363e-02, -3.6297e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.2648e-02, -4.8158e-03, -2.0476e-02],\n",
      "          [-2.5846e-02, -1.4660e-03, -2.8170e-02],\n",
      "          [-2.6640e-02,  4.3022e-03, -2.7636e-02]],\n",
      "\n",
      "         [[-6.3289e-03, -1.5401e-02, -1.3096e-03],\n",
      "          [-1.7499e-02, -2.6212e-02, -2.3646e-02],\n",
      "          [-7.3207e-03, -1.5592e-02, -8.9578e-03]],\n",
      "\n",
      "         [[ 8.9701e-04, -6.6914e-03, -5.3129e-03],\n",
      "          [-1.1727e-03, -1.0726e-02, -9.0103e-03],\n",
      "          [ 3.2311e-03, -4.5854e-03,  4.3512e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1822e-02, -3.6889e-02, -2.2588e-02],\n",
      "          [-1.3054e-02, -3.4191e-02, -2.7238e-02],\n",
      "          [-1.2383e-02, -2.3452e-02, -2.2486e-02]],\n",
      "\n",
      "         [[ 6.8177e-03,  2.1561e-02,  1.3674e-02],\n",
      "          [ 3.1192e-03,  1.0660e-02,  1.0409e-02],\n",
      "          [ 8.0477e-03, -4.6817e-03, -4.3912e-03]],\n",
      "\n",
      "         [[-1.1983e-02, -1.6201e-02, -2.2626e-02],\n",
      "          [-1.3461e-02, -7.0928e-03, -1.4384e-02],\n",
      "          [-2.4456e-02,  1.4885e-02,  1.2247e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.6347e-02, -2.9923e-02, -3.7810e-02],\n",
      "          [-1.5663e-02, -4.1126e-03, -1.1482e-02],\n",
      "          [-1.3415e-02, -1.5432e-02, -1.8204e-02]],\n",
      "\n",
      "         [[-3.8392e-03, -1.1093e-02, -8.0841e-04],\n",
      "          [-5.9634e-03, -5.9165e-03, -9.3332e-03],\n",
      "          [-2.2761e-03,  5.4781e-03, -5.6050e-03]],\n",
      "\n",
      "         [[-1.8406e-03, -2.8134e-03,  8.3246e-03],\n",
      "          [-1.2453e-03,  2.1453e-04,  7.4868e-03],\n",
      "          [ 1.3450e-02,  3.0599e-02,  2.6405e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5268e-04,  2.3897e-03,  6.2558e-03],\n",
      "          [-1.4338e-02, -2.3146e-02, -1.9024e-02],\n",
      "          [-2.7306e-02, -3.0079e-02, -3.1762e-02]],\n",
      "\n",
      "         [[ 1.4584e-02,  4.3430e-03,  1.2053e-02],\n",
      "          [-6.1130e-03, -2.8539e-02, -1.8268e-02],\n",
      "          [-1.6844e-02, -4.7816e-02, -2.6274e-02]],\n",
      "\n",
      "         [[-1.8850e-02, -9.3396e-03,  7.8905e-03],\n",
      "          [-1.5322e-03,  8.3153e-03,  1.7783e-02],\n",
      "          [-8.3318e-03, -1.5759e-02, -1.2061e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.9578e-03,  7.4573e-03, -1.8738e-03],\n",
      "          [-1.7752e-03, -6.8015e-04, -7.4443e-03],\n",
      "          [-1.8319e-02, -1.4264e-02, -7.1446e-03]],\n",
      "\n",
      "         [[ 7.8524e-03, -2.6520e-03, -1.7556e-02],\n",
      "          [ 4.5240e-03, -4.8661e-03, -1.5215e-02],\n",
      "          [-5.0211e-03, -1.1864e-02, -1.4846e-02]],\n",
      "\n",
      "         [[ 2.9163e-02,  1.0344e-02,  2.4736e-02],\n",
      "          [ 1.2012e-02, -1.0346e-02,  3.5472e-03],\n",
      "          [ 8.2238e-03, -1.8237e-02, -5.4892e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.8434e-03, -4.3184e-03, -5.7536e-03],\n",
      "          [ 7.7230e-03, -4.1936e-04,  7.7260e-03],\n",
      "          [ 1.3536e-02,  1.5705e-02,  2.0893e-02]],\n",
      "\n",
      "         [[ 1.6743e-03,  1.9720e-03,  2.1567e-02],\n",
      "          [-8.0074e-03, -4.6606e-03,  4.0560e-03],\n",
      "          [-1.6688e-02, -1.3754e-02, -1.1708e-02]],\n",
      "\n",
      "         [[-9.7959e-03, -9.4502e-03, -9.3443e-03],\n",
      "          [ 6.9547e-03, -3.9134e-05,  6.2691e-03],\n",
      "          [-1.3193e-02,  9.3272e-04,  1.4579e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.4963e-03,  5.5133e-04,  1.1571e-02],\n",
      "          [ 1.0174e-02,  1.7889e-03,  1.1035e-02],\n",
      "          [ 7.0212e-03,  1.4651e-03,  1.2769e-03]],\n",
      "\n",
      "         [[-1.3021e-02,  6.4109e-03, -1.5199e-02],\n",
      "          [ 2.4775e-02,  2.1926e-02,  3.3679e-02],\n",
      "          [ 2.6471e-04, -3.0235e-03,  1.1690e-02]],\n",
      "\n",
      "         [[-2.9665e-02, -1.5314e-02, -1.7500e-02],\n",
      "          [-1.8339e-02, -2.0845e-02, -1.5494e-02],\n",
      "          [-1.6086e-03,  1.0831e-02, -1.4309e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.7044e-03, -2.1100e-02, -2.2816e-02],\n",
      "          [ 5.7688e-03,  1.9362e-04,  7.7105e-04],\n",
      "          [-6.1357e-03,  9.7275e-03, -2.5464e-03]],\n",
      "\n",
      "         [[ 1.1043e-02,  2.4205e-02,  3.4213e-02],\n",
      "          [ 2.9181e-02,  2.6904e-02,  4.5372e-02],\n",
      "          [-2.1594e-02, -1.1072e-03, -7.8312e-03]],\n",
      "\n",
      "         [[-8.3287e-03, -7.9521e-03, -5.3358e-03],\n",
      "          [-6.2527e-04, -5.3243e-03, -8.6296e-03],\n",
      "          [ 3.6094e-03, -1.2544e-03, -4.3801e-03]]]])\n",
      "tensor([0.2587, 0.3073, 0.2595, 0.3223, 0.2662, 0.2652, 0.2575, 0.2660, 0.2766,\n",
      "        0.2414, 0.3045, 0.2853, 0.2821, 0.2880, 0.3094, 0.3444, 0.3155, 0.4129,\n",
      "        0.2110, 0.2903, 0.2496, 0.2601, 0.2967, 0.3033, 0.4152, 0.2719, 0.3661,\n",
      "        0.3251, 0.3898, 0.3346, 0.2753, 0.2712, 0.2414, 0.3351, 0.3394, 0.3167,\n",
      "        0.3360, 0.2666, 0.2109, 0.2705, 0.2587, 0.3070, 0.2720, 0.2316, 0.2885,\n",
      "        0.2884, 0.2955, 0.3057, 0.3043, 0.2596, 0.2673, 0.1929, 0.3136, 0.3593,\n",
      "        0.2622, 0.2931, 0.3295, 0.2514, 0.3208, 0.2798, 0.3259, 0.2939, 0.2390,\n",
      "        0.3105, 0.3471, 0.2812, 0.2148, 0.2997, 0.3061, 0.2740, 0.2791, 0.3790,\n",
      "        0.3592, 0.3247, 0.2995, 0.2735, 0.3356, 0.2703, 0.3255, 0.3127, 0.2783,\n",
      "        0.2702, 0.3900, 0.2942, 0.2899, 0.3461, 0.3432, 0.4685, 0.2634, 0.2553,\n",
      "        0.3019, 0.3961, 0.2742, 0.2995, 0.3858, 0.2785, 0.3212, 0.3109, 0.3642,\n",
      "        0.2193, 0.2643, 0.2333, 0.3151, 0.3102, 0.2936, 0.2374, 0.2419, 0.2976,\n",
      "        0.3335, 0.2619, 0.3984, 0.2721, 0.2718, 0.2678, 0.2757, 0.2445, 0.3508,\n",
      "        0.2174, 0.3309, 0.2653, 0.2564, 0.1748, 0.3177, 0.2751, 0.2067, 0.2905,\n",
      "        0.2762, 0.3329, 0.2738, 0.3224, 0.2199, 0.2997, 0.2206, 0.3213, 0.2760,\n",
      "        0.3927, 0.3174, 0.2698, 0.2988, 0.2610, 0.2550, 0.2788, 0.4445, 0.2862,\n",
      "        0.3606, 0.3279, 0.2869, 0.3294, 0.2244, 0.2338, 0.1754, 0.2318, 0.3186,\n",
      "        0.3322, 0.2255, 0.3041, 0.2837, 0.3276, 0.2392, 0.3668, 0.1971, 0.2946,\n",
      "        0.3613, 0.2736, 0.2554, 0.2860, 0.2511, 0.3490, 0.3253, 0.2934, 0.2027,\n",
      "        0.2580, 0.2200, 0.3089, 0.3074, 0.3332, 0.2943, 0.3375, 0.2330, 0.2611,\n",
      "        0.3383, 0.2837, 0.3546, 0.3093, 0.3791, 0.2197, 0.2648, 0.2830, 0.2587,\n",
      "        0.3588, 0.2830, 0.3971, 0.3194, 0.3066, 0.2754, 0.2647, 0.0970, 0.2182,\n",
      "        0.2334, 0.2624, 0.1829, 0.2933, 0.2747, 0.3001, 0.2996, 0.3107, 0.3256,\n",
      "        0.2940, 0.3901, 0.2790, 0.3030, 0.2838, 0.3010, 0.3044, 0.3479, 0.3087,\n",
      "        0.2611, 0.1958, 0.2941, 0.2558, 0.2889, 0.3148, 0.2516, 0.2664, 0.2862,\n",
      "        0.3940, 0.2933, 0.2781, 0.3796, 0.3022, 0.2583, 0.3021, 0.2784, 0.2967,\n",
      "        0.2994, 0.3856, 0.3277, 0.2587, 0.2539, 0.2824, 0.2634, 0.1489, 0.2205,\n",
      "        0.3929, 0.3401, 0.2717, 0.2789, 0.2917, 0.3177, 0.1992, 0.3684, 0.3120,\n",
      "        0.3201, 0.2810, 0.2302, 0.2779, 0.2865, 0.2858, 0.2713, 0.1601, 0.2496,\n",
      "        0.2895, 0.3154, 0.3443, 0.3285, 0.3444, 0.3251, 0.3235, 0.3375, 0.2282,\n",
      "        0.2128, 0.1795, 0.3077, 0.3005, 0.2775, 0.3054, 0.2914, 0.3535, 0.2871,\n",
      "        0.2669, 0.3961, 0.2674, 0.3898, 0.3183, 0.3242, 0.2789, 0.1911, 0.2569,\n",
      "        0.3427, 0.2464, 0.2778, 0.2098, 0.3019, 0.3145, 0.3271, 0.2914, 0.2619,\n",
      "        0.2643, 0.3039, 0.2520, 0.2099, 0.3643, 0.2915, 0.1957, 0.3286, 0.2355,\n",
      "        0.3210, 0.2982, 0.3388, 0.3450, 0.3716, 0.2898, 0.2846, 0.2805, 0.2219,\n",
      "        0.2910, 0.2681, 0.3163, 0.1964, 0.3176, 0.3092, 0.2706, 0.2505, 0.2508,\n",
      "        0.3166, 0.3583, 0.1563, 0.2608, 0.2892, 0.3401, 0.2891, 0.3126, 0.2172,\n",
      "        0.2459, 0.2651, 0.4052, 0.2986, 0.3026, 0.3773, 0.2262, 0.2675, 0.2900,\n",
      "        0.3759, 0.3201, 0.2567, 0.3443, 0.2348, 0.3057, 0.2347, 0.3277, 0.2938,\n",
      "        0.2746, 0.2805, 0.2421, 0.3590, 0.2622, 0.2773, 0.2396, 0.2134, 0.2727,\n",
      "        0.2984, 0.2744, 0.2591, 0.2628, 0.3568, 0.2009, 0.3220, 0.2868, 0.2561,\n",
      "        0.3113, 0.2138, 0.3136, 0.2745, 0.3046, 0.3042, 0.1972, 0.2815, 0.2542,\n",
      "        0.2983, 0.2613, 0.2668, 0.3142, 0.2930, 0.3800, 0.1966, 0.2948, 0.3363,\n",
      "        0.2713, 0.3625, 0.2909, 0.2695, 0.3111, 0.3242, 0.3009, 0.3231, 0.3051,\n",
      "        0.2012, 0.2716, 0.3692, 0.2694, 0.1481, 0.2858, 0.2819, 0.2391, 0.2867,\n",
      "        0.3466, 0.3431, 0.2365, 0.3357, 0.1685, 0.2925, 0.3092, 0.3127, 0.1883,\n",
      "        0.2561, 0.3086, 0.1732, 0.2989, 0.3235, 0.2693, 0.2630, 0.2913, 0.2786,\n",
      "        0.3124, 0.3098, 0.2695, 0.2403, 0.2906, 0.2784, 0.2654, 0.3485, 0.3939,\n",
      "        0.3033, 0.3145, 0.2622, 0.1540, 0.2790, 0.2967, 0.1954, 0.2632, 0.2957,\n",
      "        0.2581, 0.3231, 0.2795, 0.2859, 0.3139, 0.2488, 0.2404, 0.3714, 0.2649,\n",
      "        0.2267, 0.2878, 0.3462, 0.3063, 0.3180, 0.1726, 0.3153, 0.2625, 0.3020,\n",
      "        0.2996, 0.3632, 0.1541, 0.3192, 0.2200, 0.2894, 0.2622, 0.2534, 0.2935,\n",
      "        0.3208, 0.2231, 0.2743, 0.3023, 0.2829, 0.2394, 0.2506, 0.3512, 0.3366,\n",
      "        0.2666, 0.2930, 0.3049, 0.2321, 0.3397, 0.2727, 0.2900, 0.3146, 0.2682,\n",
      "        0.3094, 0.3718, 0.3387, 0.3202, 0.2423, 0.2745, 0.2966, 0.2500, 0.2329,\n",
      "        0.3419, 0.2928, 0.3536, 0.3739, 0.1935, 0.2670, 0.2846, 0.2583, 0.3783,\n",
      "        0.2826, 0.2929, 0.2728, 0.3645, 0.2770, 0.2756, 0.2523, 0.2500])\n",
      "tensor([-0.1668, -0.3019, -0.2187, -0.2917, -0.1971, -0.2325, -0.1869, -0.1857,\n",
      "        -0.2474, -0.1629, -0.2448, -0.2508, -0.1895, -0.2651, -0.3250, -0.3811,\n",
      "        -0.2953, -0.4963, -0.0294, -0.2724, -0.2007, -0.2220, -0.2945, -0.2579,\n",
      "        -0.5152, -0.1994, -0.5016, -0.2736, -0.4528, -0.3968, -0.2281, -0.1772,\n",
      "        -0.1293, -0.2655, -0.3252, -0.3232, -0.3337, -0.1901, -0.0692, -0.2196,\n",
      "        -0.2132, -0.2565, -0.1646, -0.1567, -0.2087, -0.2178, -0.2480, -0.2767,\n",
      "        -0.3071, -0.1988, -0.1985, -0.0235, -0.2458, -0.4156, -0.1660, -0.1923,\n",
      "        -0.3328, -0.1481, -0.3047, -0.2277, -0.3182, -0.2744, -0.1643, -0.3365,\n",
      "        -0.4050, -0.2082, -0.0621, -0.2671, -0.2809, -0.2185, -0.2148, -0.4465,\n",
      "        -0.3376, -0.3213, -0.2921, -0.1998, -0.3369, -0.2092, -0.2831, -0.2893,\n",
      "        -0.1719, -0.2189, -0.4016, -0.2484, -0.2070, -0.3849, -0.3753, -0.5874,\n",
      "        -0.1637, -0.1748, -0.2217, -0.5067, -0.2496, -0.2117, -0.4291, -0.1944,\n",
      "        -0.3089, -0.2621, -0.4096, -0.0602, -0.2009, -0.1316, -0.3336, -0.2627,\n",
      "        -0.2320, -0.0910, -0.1560, -0.2889, -0.3286, -0.1628, -0.5128, -0.2036,\n",
      "        -0.1726, -0.1844, -0.2285, -0.1925, -0.3432, -0.0929, -0.3138, -0.1912,\n",
      "        -0.1926, -0.0342, -0.3268, -0.1699, -0.0828, -0.2417, -0.2069, -0.3870,\n",
      "        -0.2210, -0.2867, -0.0526, -0.3092, -0.0655, -0.2594, -0.2160, -0.5062,\n",
      "        -0.2905, -0.2125, -0.3124, -0.2128, -0.1946, -0.2520, -0.5475, -0.2321,\n",
      "        -0.3350, -0.3473, -0.2158, -0.3603, -0.0759, -0.1472, -0.0327, -0.1404,\n",
      "        -0.3128, -0.3063, -0.1120, -0.2664, -0.2700, -0.3112, -0.1519, -0.3843,\n",
      "        -0.0645, -0.2373, -0.4227, -0.2546, -0.1611, -0.2350, -0.1524, -0.3494,\n",
      "        -0.3453, -0.2081, -0.0918, -0.2025, -0.1246, -0.2533, -0.2768, -0.3156,\n",
      "        -0.2530, -0.3957, -0.0981, -0.1257, -0.3697, -0.2333, -0.3664, -0.2829,\n",
      "        -0.4320, -0.0836, -0.1583, -0.2395, -0.1818, -0.4408, -0.2376, -0.4450,\n",
      "        -0.3232, -0.2787, -0.1858, -0.2137,  0.0481, -0.1058, -0.1093, -0.2035,\n",
      "        -0.0496, -0.2117, -0.1598, -0.2389, -0.2830, -0.2878, -0.3406, -0.2560,\n",
      "        -0.4468, -0.2444, -0.2492, -0.2222, -0.2792, -0.3005, -0.4180, -0.2568,\n",
      "        -0.1872, -0.0270, -0.2645, -0.1873, -0.3022, -0.3400, -0.1803, -0.1810,\n",
      "        -0.2079, -0.4775, -0.2047, -0.1878, -0.4504, -0.2516, -0.1657, -0.2765,\n",
      "        -0.2329, -0.2446, -0.2956, -0.4163, -0.2816, -0.1571, -0.2199, -0.2125,\n",
      "        -0.1684,  0.0356, -0.0914, -0.4484, -0.3535, -0.2212, -0.2550, -0.2509,\n",
      "        -0.2702, -0.0599, -0.3505, -0.2924, -0.2360, -0.2339, -0.1259, -0.2597,\n",
      "        -0.2267, -0.1978, -0.1371, -0.0129, -0.1175, -0.2527, -0.3099, -0.3231,\n",
      "        -0.3468, -0.3553, -0.3537, -0.3315, -0.3713, -0.1091, -0.0959, -0.0258,\n",
      "        -0.2756, -0.2808, -0.2012, -0.2812, -0.1991, -0.3948, -0.2257, -0.2469,\n",
      "        -0.4211, -0.2110, -0.4670, -0.3069, -0.3549, -0.2337, -0.0612, -0.1321,\n",
      "        -0.2968, -0.1870, -0.2316, -0.0686, -0.3113, -0.2895, -0.3149, -0.2686,\n",
      "        -0.2081, -0.2096, -0.3011, -0.1810, -0.0227, -0.3873, -0.2665, -0.0225,\n",
      "        -0.2973, -0.0973, -0.2980, -0.3219, -0.2926, -0.3196, -0.4332, -0.1980,\n",
      "        -0.2117, -0.2302, -0.0980, -0.2344, -0.2154, -0.2921, -0.0350, -0.3361,\n",
      "        -0.2620, -0.2188, -0.1566, -0.1795, -0.2726, -0.4103,  0.0413, -0.1507,\n",
      "        -0.2552, -0.3137, -0.2466, -0.2961, -0.0938, -0.1481, -0.2129, -0.5480,\n",
      "        -0.2915, -0.2802, -0.5077, -0.1306, -0.1862, -0.2400, -0.4362, -0.3017,\n",
      "        -0.1633, -0.3447, -0.1047, -0.2846, -0.1244, -0.3036, -0.2404, -0.2333,\n",
      "        -0.2494, -0.1866, -0.3294, -0.1677, -0.2540, -0.1295, -0.0512, -0.1966,\n",
      "        -0.2801, -0.1702, -0.1879, -0.1850, -0.3274, -0.0369, -0.2979, -0.2612,\n",
      "        -0.1889, -0.3270, -0.1377, -0.2787, -0.2201, -0.2417, -0.2834, -0.0555,\n",
      "        -0.2538, -0.1040, -0.2660, -0.1644, -0.1723, -0.2672, -0.2797, -0.4214,\n",
      "        -0.0378, -0.2386, -0.3498, -0.2435, -0.4348, -0.2554, -0.1719, -0.2836,\n",
      "        -0.3316, -0.2787, -0.2879, -0.2640, -0.0560, -0.1789, -0.4195, -0.2152,\n",
      "         0.0567, -0.2359, -0.2249, -0.0911, -0.2644, -0.3875, -0.3317, -0.1415,\n",
      "        -0.3425, -0.0020, -0.1941, -0.2821, -0.2809, -0.0965, -0.1841, -0.2971,\n",
      "        -0.0173, -0.3043, -0.3013, -0.1729, -0.1872, -0.2683, -0.2033, -0.3059,\n",
      "        -0.2939, -0.2163, -0.1889, -0.2581, -0.2296, -0.2066, -0.3462, -0.4298,\n",
      "        -0.2600, -0.3095, -0.1800, -0.0116, -0.2124, -0.2552, -0.0523, -0.2216,\n",
      "        -0.2605, -0.2134, -0.2867, -0.2556, -0.2275, -0.3437, -0.1698, -0.1560,\n",
      "        -0.4120, -0.2067, -0.1159, -0.2408, -0.3093, -0.2621, -0.2593, -0.0135,\n",
      "        -0.3099, -0.2179, -0.2766, -0.2400, -0.3934,  0.0072, -0.2982, -0.0930,\n",
      "        -0.2166, -0.1635, -0.1827, -0.2308, -0.2525, -0.0991, -0.2325, -0.2938,\n",
      "        -0.2480, -0.0934, -0.1911, -0.3772, -0.3369, -0.1606, -0.2752, -0.3005,\n",
      "        -0.1372, -0.2990, -0.2156, -0.2622, -0.3160, -0.1342, -0.2903, -0.3865,\n",
      "        -0.2916, -0.3243, -0.2051, -0.2656, -0.2359, -0.1508, -0.1063, -0.3595,\n",
      "        -0.2312, -0.3046, -0.4178, -0.0276, -0.2204, -0.2426, -0.1616, -0.4789,\n",
      "        -0.1713, -0.2802, -0.2305, -0.4327, -0.2413, -0.1862, -0.1486, -0.1507])\n",
      "tensor([[[[ 2.8729e-04,  4.2632e-03, -2.0266e-03],\n",
      "          [ 1.9513e-04,  2.4381e-03, -5.8632e-03],\n",
      "          [ 4.4803e-03,  8.6577e-03,  8.5538e-04]],\n",
      "\n",
      "         [[-1.1335e-02, -1.3195e-02, -1.0305e-02],\n",
      "          [-4.9507e-03, -4.5898e-03, -3.1041e-03],\n",
      "          [-7.5883e-03, -8.3795e-03, -8.9239e-03]],\n",
      "\n",
      "         [[-1.1914e-02, -1.2104e-02, -1.0167e-02],\n",
      "          [-1.2093e-02, -1.1557e-02, -8.9600e-03],\n",
      "          [-1.2515e-02, -9.3296e-03, -6.4079e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.3573e-03, -1.0662e-02, -1.2672e-02],\n",
      "          [-8.0600e-03, -8.5423e-03, -1.2121e-02],\n",
      "          [-8.1498e-03, -8.8037e-03, -1.0611e-02]],\n",
      "\n",
      "         [[ 4.2632e-03,  5.6461e-03,  2.8460e-03],\n",
      "          [ 4.7070e-03,  6.2550e-03,  7.5862e-03],\n",
      "          [ 1.1504e-02,  1.1518e-02,  1.0728e-02]],\n",
      "\n",
      "         [[-6.2455e-03, -9.1693e-03, -9.6664e-03],\n",
      "          [-4.2935e-03, -6.5311e-03, -5.0513e-03],\n",
      "          [-3.1141e-03, -5.0124e-03, -5.8122e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7483e-03,  3.7146e-04,  3.3262e-05],\n",
      "          [-4.5675e-03, -6.6689e-03, -6.4447e-03],\n",
      "          [-6.7610e-03, -7.3204e-03, -9.5855e-03]],\n",
      "\n",
      "         [[-1.4630e-02, -1.2320e-02, -1.4457e-02],\n",
      "          [-8.6197e-03, -5.8059e-03, -1.1075e-02],\n",
      "          [-6.2154e-03, -6.8218e-03, -9.3805e-03]],\n",
      "\n",
      "         [[ 1.0879e-03,  4.3850e-04, -1.9456e-03],\n",
      "          [-1.2517e-03,  3.2917e-04, -2.1435e-03],\n",
      "          [ 4.8136e-03,  2.5333e-03,  5.1504e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4644e-02,  1.7434e-02,  2.0734e-02],\n",
      "          [ 2.3101e-02,  1.3487e-02,  2.0728e-02],\n",
      "          [ 1.9381e-02,  1.5243e-02,  1.7340e-02]],\n",
      "\n",
      "         [[ 1.2212e-02,  1.2448e-02,  1.5048e-02],\n",
      "          [ 5.2993e-03,  4.0090e-03,  9.3927e-03],\n",
      "          [ 6.6766e-03,  2.4941e-03,  8.3288e-03]],\n",
      "\n",
      "         [[ 3.1040e-02,  2.8243e-02,  3.2319e-02],\n",
      "          [ 3.8608e-02,  3.3099e-02,  3.8652e-02],\n",
      "          [ 2.5839e-02,  2.6524e-02,  2.4995e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.1761e-03,  4.5553e-03,  2.0612e-03],\n",
      "          [ 4.9747e-03,  1.1420e-02,  8.5734e-03],\n",
      "          [ 4.8583e-03,  1.1469e-02,  1.0039e-02]],\n",
      "\n",
      "         [[-6.2547e-05,  6.5336e-04,  9.4747e-04],\n",
      "          [ 5.0603e-03,  7.7136e-03,  6.5484e-03],\n",
      "          [-4.8432e-04,  2.3057e-03,  2.9219e-03]],\n",
      "\n",
      "         [[-3.2788e-02, -2.7615e-02, -3.2608e-02],\n",
      "          [-3.6296e-02, -2.8170e-02, -3.0277e-02],\n",
      "          [-3.6814e-02, -3.1547e-02, -3.0231e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.2998e-03, -2.8590e-04, -4.9266e-03],\n",
      "          [-7.0530e-03, -2.3684e-04, -1.5838e-03],\n",
      "          [-6.9291e-03,  4.8084e-04, -3.1548e-03]],\n",
      "\n",
      "         [[ 1.1854e-02,  8.4836e-03,  1.3839e-02],\n",
      "          [ 2.8741e-03, -9.7358e-05,  4.4888e-03],\n",
      "          [-2.5515e-03, -2.7788e-03, -3.2464e-03]],\n",
      "\n",
      "         [[-1.2408e-02, -1.5001e-02, -1.3377e-02],\n",
      "          [-1.4540e-02, -1.8537e-02, -1.7392e-02],\n",
      "          [-6.7315e-03, -9.5205e-03, -9.0692e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.0369e-03,  1.9542e-03,  1.7140e-03],\n",
      "          [-7.6240e-03, -2.8765e-03, -5.1760e-03],\n",
      "          [-9.3019e-03, -4.8800e-03, -4.2932e-03]],\n",
      "\n",
      "         [[ 4.4836e-03,  2.4909e-03,  1.5746e-03],\n",
      "          [ 1.2065e-02,  1.2936e-02,  1.0344e-02],\n",
      "          [ 1.9010e-02,  1.7459e-02,  1.5988e-02]],\n",
      "\n",
      "         [[-1.4914e-03, -8.1727e-03, -8.0671e-03],\n",
      "          [-6.6247e-03, -6.2421e-03, -9.2717e-03],\n",
      "          [-8.7991e-03, -7.7528e-03, -8.6336e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8040e-02, -1.5366e-02, -1.5334e-02],\n",
      "          [-1.3148e-02, -1.2180e-02, -1.0915e-02],\n",
      "          [-1.4545e-02, -1.4756e-02, -1.1787e-02]],\n",
      "\n",
      "         [[ 3.5762e-03,  6.6073e-03, -1.4055e-03],\n",
      "          [ 4.3975e-03,  7.8375e-03,  8.8085e-05],\n",
      "          [-5.0697e-03, -5.6633e-04, -5.9284e-03]],\n",
      "\n",
      "         [[-1.9234e-03, -8.8012e-03, -5.8821e-03],\n",
      "          [ 3.6685e-03, -1.3784e-03, -3.2117e-03],\n",
      "          [-4.7037e-04,  1.5340e-04, -3.4046e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.8305e-02, -1.7735e-02, -2.1683e-02],\n",
      "          [-1.6598e-02, -1.2508e-02, -2.0530e-02],\n",
      "          [-1.0800e-02, -9.8670e-03, -1.7195e-02]],\n",
      "\n",
      "         [[ 2.0721e-02,  2.2466e-02,  2.5049e-02],\n",
      "          [ 1.8682e-02,  1.3160e-02,  2.3696e-02],\n",
      "          [ 2.2104e-02,  1.7261e-02,  2.4877e-02]],\n",
      "\n",
      "         [[-5.7091e-03, -2.6876e-03, -9.2260e-04],\n",
      "          [-9.4530e-03, -7.0543e-03, -6.2770e-03],\n",
      "          [-4.5806e-03, -2.7182e-03, -2.5823e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4150e-02,  1.4002e-02,  1.6559e-02],\n",
      "          [ 2.1363e-02,  1.4359e-02,  1.5854e-02],\n",
      "          [ 2.5786e-02,  2.7233e-02,  2.5104e-02]],\n",
      "\n",
      "         [[-4.6450e-03,  1.2419e-03, -1.8768e-03],\n",
      "          [ 1.3005e-03,  4.0888e-03, -6.5483e-04],\n",
      "          [-7.9783e-03, -6.6539e-03, -8.9957e-03]],\n",
      "\n",
      "         [[ 1.1494e-02,  2.6621e-02,  1.5649e-02],\n",
      "          [ 6.5960e-03,  1.7290e-02,  7.5466e-03],\n",
      "          [-8.0256e-03,  4.6246e-03, -5.7808e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4232e-02,  1.1769e-02,  9.4342e-03],\n",
      "          [ 6.2592e-03,  5.1087e-03,  2.3311e-03],\n",
      "          [-1.9694e-03,  2.7110e-03, -2.8945e-03]],\n",
      "\n",
      "         [[-7.0772e-03,  1.0365e-03, -5.8451e-03],\n",
      "          [-9.1879e-03, -3.1388e-03, -8.1517e-03],\n",
      "          [-8.0300e-03, -5.1313e-03, -9.5734e-03]],\n",
      "\n",
      "         [[ 2.4314e-02,  1.8942e-02,  2.4256e-02],\n",
      "          [ 2.0090e-02,  1.1472e-02,  1.5993e-02],\n",
      "          [ 2.2910e-02,  2.0622e-02,  2.3820e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6375e-02, -1.6928e-02, -1.9019e-02],\n",
      "          [-9.7367e-03, -1.1274e-02, -1.0261e-02],\n",
      "          [-1.2310e-02, -1.5931e-02, -1.4151e-02]],\n",
      "\n",
      "         [[ 4.7098e-03, -4.5205e-04,  2.8042e-03],\n",
      "          [ 2.1428e-03, -4.6175e-03, -1.6818e-03],\n",
      "          [-1.3336e-03, -5.5009e-03, -2.6237e-03]],\n",
      "\n",
      "         [[-1.4367e-02, -1.3520e-02, -1.1387e-02],\n",
      "          [-4.7420e-03, -1.7309e-03, -2.6426e-03],\n",
      "          [ 5.1448e-03,  7.0428e-03,  5.0202e-03]]]])\n",
      "tensor([1.8419, 1.8307, 1.7650, 1.8288, 1.9505, 1.8026, 1.9536, 2.2790, 1.7662,\n",
      "        1.8902, 1.7768, 1.7749, 1.9055, 1.7328, 1.8762, 1.8211, 1.7967, 2.3428,\n",
      "        1.7985, 1.7271, 1.7915, 1.9512, 1.8928, 1.9017, 1.8784, 1.9809, 1.8569,\n",
      "        1.7830, 1.8911, 1.8859, 1.7764, 1.9832, 1.8389, 1.7616, 1.8728, 1.8753,\n",
      "        1.9008, 1.8209, 1.7039, 1.7377, 1.7786, 1.6944, 1.7829, 1.7815, 1.7594,\n",
      "        1.8428, 1.9238, 2.0871, 1.8980, 1.8413, 1.8471, 1.8584, 1.7640, 1.8453,\n",
      "        1.7606, 1.9504, 1.9620, 1.8755, 1.9424, 1.8731, 1.8674, 1.9422, 1.8750,\n",
      "        1.9208, 1.7464, 1.8558, 1.6539, 2.0660, 2.0298, 1.9174, 1.8972, 1.7589,\n",
      "        1.7551, 1.9560, 1.7909, 1.7971, 1.7851, 1.7733, 1.8061, 1.7949, 1.8169,\n",
      "        1.8089, 1.8641, 2.1542, 1.7739, 1.7913, 1.8022, 1.7155, 1.7679, 1.7704,\n",
      "        1.6266, 1.8645, 1.9076, 1.8576, 1.6924, 1.8020, 1.7100, 1.7713, 1.8572,\n",
      "        1.7103, 2.0664, 1.9054, 1.9422, 1.8078, 1.7412, 1.6061, 1.9105, 1.8947,\n",
      "        1.7954, 1.8989, 1.8239, 1.7619, 1.7951, 1.8149, 1.8539, 1.8502, 1.7095,\n",
      "        2.1831, 1.8599, 1.8252, 1.8193, 1.8460, 1.7968, 1.6229, 1.8450, 1.8290,\n",
      "        1.8706, 1.9293, 1.6881, 1.9725, 1.8981, 1.8925, 1.8851, 1.8445, 1.9764,\n",
      "        2.0674, 1.8384, 1.8414, 1.8762, 1.7931, 1.7131, 1.9644, 1.7854, 1.9369,\n",
      "        1.8972, 1.8940, 1.8700, 1.7967, 1.8775, 1.9409, 1.7391, 1.7944, 1.9678,\n",
      "        1.7678, 1.6851, 1.9414, 1.9663, 1.9882, 1.7915, 1.8141, 1.8325, 2.1200,\n",
      "        1.9256, 2.3592, 2.0304, 1.9594, 1.7334, 1.9048, 1.8221, 1.7811, 1.9084,\n",
      "        1.8053, 1.9171, 1.9644, 1.8256, 1.6432, 1.9173, 1.9094, 1.9923, 1.7963,\n",
      "        1.9077, 1.7619, 2.1724, 1.7931, 1.7564, 1.8889, 1.9832, 1.9136, 1.8035,\n",
      "        1.8419, 1.8278, 1.8057, 1.9063, 1.8646, 1.7848, 1.8230, 1.7986, 1.7091,\n",
      "        1.7724, 1.7939, 1.7611, 1.9325, 2.0162, 1.7295, 2.0196, 1.8876, 1.8325,\n",
      "        1.8225, 1.7870, 1.9160, 1.7197, 1.7170, 1.9133, 1.7770, 1.9943, 1.8389,\n",
      "        1.8070, 1.8516, 1.7857, 1.9648, 1.9553, 1.9232, 1.8086, 1.8114, 1.7141,\n",
      "        1.8058, 1.8532, 1.9255, 1.7682, 1.8314, 1.8495, 1.8296, 1.8278, 1.8819,\n",
      "        1.7698, 1.7838, 1.7807, 1.9974, 1.6994, 1.9483, 1.7793, 1.8029, 2.2210,\n",
      "        1.6455, 1.8357, 2.1706, 1.9204, 1.7414, 1.7809, 1.8648, 1.9145, 1.8849,\n",
      "        1.8346, 1.9368, 1.8169, 2.2302, 1.8262, 2.0651, 1.9888, 1.8169, 1.8462,\n",
      "        1.9681, 1.8083, 1.8595, 1.8539, 1.7699, 1.9001, 1.7285, 1.7553, 1.8924,\n",
      "        1.7829, 1.9428, 1.8724, 1.7228, 2.0548, 1.7732, 1.8561, 1.7699, 1.9269,\n",
      "        1.8171, 2.4075, 1.7257, 1.7819, 1.7244, 1.8521, 1.8302, 1.8797, 1.7617,\n",
      "        1.9650, 1.9807, 1.7102, 1.7486, 1.8350, 1.9919, 1.8505, 1.9000, 1.8269,\n",
      "        1.9787, 1.7635, 1.6071, 1.7998, 1.9545, 1.7348, 1.7140, 1.8851, 1.7981,\n",
      "        1.9100, 1.8315, 1.7864, 1.9165, 1.8839, 1.9017, 1.9334, 1.7405, 1.7661,\n",
      "        1.8015, 1.9987, 1.7622, 1.9107, 1.8444, 1.7128, 1.8726, 1.8529, 1.9270,\n",
      "        1.8769, 1.7261, 1.8393, 1.9075, 1.7953, 1.8246, 1.7605, 2.0470, 1.9221,\n",
      "        1.9205, 1.8910, 1.7666, 1.6801, 1.8308, 1.8845, 1.8339, 1.8238, 1.7616,\n",
      "        1.6114, 1.8411, 1.7437, 1.8423, 1.9540, 1.7465, 1.7741, 1.8746, 1.8856,\n",
      "        1.7740, 1.7603, 1.7682, 1.8396, 1.6869, 1.8080, 1.8836, 1.8283, 1.8341,\n",
      "        1.8522, 1.9749, 1.8707, 1.7719, 1.8993, 1.8108, 1.8480, 1.8267, 1.8731,\n",
      "        1.9576, 1.8347, 1.9509, 1.9641, 1.7997, 1.7652, 1.9253, 1.7126, 1.7551,\n",
      "        1.9427, 1.8559, 1.9163, 1.7681, 1.7803, 1.8500, 1.8535, 1.8865, 1.7599,\n",
      "        2.0692, 1.8021, 1.7077, 1.8890, 1.9457, 1.8516, 1.7882, 1.8356, 1.8472,\n",
      "        1.6708, 1.7435, 1.9080, 1.9653, 2.0401, 1.8935, 1.8450, 1.7536, 1.7733,\n",
      "        1.8135, 1.8534, 1.9368, 1.7348, 1.8738, 1.9632, 1.9033, 1.7422, 1.7842,\n",
      "        1.8516, 2.0218, 1.7044, 1.8793, 1.8655, 1.8516, 1.8002, 1.8687, 1.8460,\n",
      "        1.7589, 1.8174, 1.9830, 1.9034, 2.1222, 1.8460, 1.9209, 1.8893, 1.9422,\n",
      "        1.8489, 1.8396, 1.9953, 2.0865, 1.8253, 1.7700, 1.8035, 1.7535, 1.8923,\n",
      "        1.8620, 1.8627, 1.7264, 1.8140, 1.9613, 1.8812, 1.8729, 2.0050, 1.7092,\n",
      "        1.7726, 1.9410, 1.8381, 1.8366, 1.7276, 1.8796, 1.7548, 1.9536, 1.8062,\n",
      "        1.8883, 2.0278, 1.8775, 1.9446, 1.8676, 1.8423, 1.7798, 1.9403, 1.8375,\n",
      "        2.0473, 1.9507, 1.8337, 1.8184, 1.7791, 1.8993, 1.8781, 1.8691, 1.8493,\n",
      "        1.7623, 1.9458, 1.7564, 1.7448, 1.8633, 1.6863, 1.8062, 1.8702, 2.0048,\n",
      "        1.8504, 1.8964, 1.9489, 1.8264, 1.9019, 1.8196, 1.9712, 1.8969, 1.8652,\n",
      "        1.8709, 1.6984, 1.8677, 1.8846, 1.9256, 1.8620, 1.6366, 1.8434, 1.7506,\n",
      "        1.8438, 1.5788, 1.9316, 1.9535, 1.7878, 1.7354, 2.0920, 1.9456])\n",
      "tensor([0.2371, 0.3433, 0.3279, 0.4642, 0.2233, 0.2370, 0.2176, 0.3793, 0.3140,\n",
      "        0.2803, 0.2434, 0.2116, 0.2478, 0.2435, 0.2298, 0.3172, 0.2725, 0.6511,\n",
      "        0.2925, 0.2281, 0.2279, 0.4254, 0.2342, 0.3328, 0.2632, 0.2176, 0.3180,\n",
      "        0.3893, 0.1387, 0.2274, 0.3379, 0.0767, 0.2253, 0.2504, 0.1990, 0.1951,\n",
      "        0.2566, 0.3253, 0.2797, 0.3149, 0.2373, 0.2533, 0.1956, 0.3236, 0.2093,\n",
      "        0.2333, 0.2300, 0.5019, 0.2830, 0.1885, 0.3264, 0.2722, 0.2369, 0.2430,\n",
      "        0.3625, 0.2165, 0.4700, 0.3047, 0.3675, 0.2641, 0.1979, 0.2664, 0.3448,\n",
      "        0.2005, 0.2450, 0.4351, 0.2689, 0.1632, 0.3087, 0.1209, 0.2153, 0.1592,\n",
      "        0.2960, 0.1423, 0.2951, 0.2706, 0.2007, 0.2939, 0.2210, 0.2243, 0.2465,\n",
      "        0.3910, 0.4599, 0.5417, 0.2147, 0.3469, 0.2703, 0.2229, 0.3645, 0.2647,\n",
      "        0.2421, 0.2492, 0.1666, 0.2763, 0.2560, 0.2151, 0.3363, 0.2767, 0.2516,\n",
      "        0.2988, 0.2622, 0.3499, 0.3001, 0.3907, 0.3184, 0.2233, 0.2649, 0.2110,\n",
      "        0.2034, 0.2752, 0.2314, 0.3480, 0.2238, 0.2892, 0.1991, 0.2923, 0.3259,\n",
      "        0.0722, 0.3039, 0.3041, 0.3803, 0.2568, 0.2382, 0.3057, 0.2652, 0.1532,\n",
      "        0.2110, 0.2567, 0.3148, 0.2746, 0.1833, 0.1950, 0.1116, 0.2279, 0.3705,\n",
      "        0.2477, 0.2000, 0.3060, 0.2548, 0.2468, 0.3028, 0.1921, 0.2952, 0.1980,\n",
      "        0.2135, 0.1583, 0.1586, 0.3944, 0.2352, 0.3947, 0.2740, 0.2861, 0.1856,\n",
      "        0.2702, 0.2986, 0.1728, 0.2658, 0.2696, 0.2028, 0.1838, 0.3176, 0.6246,\n",
      "        0.2631, 0.3855, 0.2074, 0.2317, 0.4171, 0.2044, 0.2926, 0.3506, 0.2305,\n",
      "        0.2400, 0.1420, 0.1093, 0.2757, 0.3253, 0.2334, 0.1650, 0.4026, 0.2066,\n",
      "        0.1790, 0.3032, 0.5658, 0.3246, 0.3834, 0.3254, 0.1772, 0.2909, 0.2350,\n",
      "        0.2519, 0.1968, 0.2003, 0.3213, 0.4802, 0.2543, 0.2578, 0.3280, 0.2270,\n",
      "        0.3044, 0.2273, 0.2447, 0.2527, 0.4136, 0.2588, 0.3589, 0.2688, 0.2115,\n",
      "        0.2022, 0.3186, 0.3740, 0.1785, 0.2074, 0.2346, 0.3566, 0.2623, 0.2620,\n",
      "        0.2880, 0.1462, 0.1896, 0.2777, 0.1852, 0.3240, 0.2748, 0.2164, 0.3066,\n",
      "        0.1845, 0.3992, 0.1695, 0.4411, 0.2812, 0.2730, 0.2784, 0.1861, 0.3589,\n",
      "        0.1934, 0.3320, 0.3350, 0.2655, 0.2740, 0.3185, 0.2633, 0.2458, 0.2003,\n",
      "        0.2809, 0.3049, 0.2050, 0.2904, 0.2381, 0.3278, 0.3484, 0.4293, 0.2422,\n",
      "        0.2859, 0.1864, 0.2954, 0.5634, 0.2081, 0.3743, 0.2902, 0.3820, 0.3069,\n",
      "        0.2101, 0.2750, 0.2878, 0.1870, 0.3015, 0.1661, 0.2998, 0.3101, 0.2522,\n",
      "        0.2419, 0.1758, 0.2681, 0.2812, 0.1495, 0.2868, 0.3157, 0.2587, 0.2437,\n",
      "        0.1467, 0.5416, 0.2490, 0.2831, 0.2783, 0.1614, 0.1963, 0.2034, 0.2364,\n",
      "        0.2527, 0.1573, 0.3184, 0.2841, 0.1613, 0.1489, 0.2850, 0.1625, 0.3277,\n",
      "        0.4936, 0.2780, 0.3178, 0.1743, 0.2158, 0.2222, 0.2821, 0.4267, 0.2713,\n",
      "        0.1778, 0.3067, 0.2270, 0.1772, 0.3897, 0.2923, 0.4843, 0.2345, 0.2327,\n",
      "        0.2740, 0.2700, 0.2804, 0.4035, 0.1501, 0.3329, 0.3286, 0.2803, 0.2309,\n",
      "        0.1738, 0.3270, 0.3097, 0.1808, 0.2384, 0.2107, 0.3240, 0.3346, 0.2236,\n",
      "        0.2061, 0.2687, 0.2360, 0.3338, 0.2694, 0.3203, 0.2895, 0.1884, 0.1491,\n",
      "        0.3957, 0.5167, 0.3407, 0.1854, 0.1816, 0.2626, 0.1855, 0.2219, 0.1482,\n",
      "        0.2584, 0.2458, 0.2616, 0.2396, 0.2402, 0.2423, 0.3463, 0.2731, 0.1524,\n",
      "        0.2514, 0.2760, 0.1734, 0.2715, 0.4052, 0.2252, 0.3676, 0.3070, 0.3127,\n",
      "        0.1836, 0.4330, 0.2203, 0.2073, 0.2803, 0.2984, 0.2191, 0.3272, 0.2267,\n",
      "        0.2749, 0.3056, 0.4566, 0.2962, 0.3528, 0.3236, 0.4220, 0.2715, 0.2256,\n",
      "        0.2903, 0.1829, 0.3994, 0.2820, 0.2471, 0.1647, 0.3654, 0.4504, 0.2685,\n",
      "        0.2992, 0.2825, 0.2435, 0.2212, 0.4300, 0.4342, 0.1988, 0.2863, 0.3398,\n",
      "        0.2444, 0.2905, 0.2559, 0.2586, 0.1702, 0.1906, 0.2536, 0.2978, 0.2498,\n",
      "        0.3777, 0.2252, 0.2472, 0.2243, 0.1732, 0.2194, 0.2091, 0.2820, 0.2898,\n",
      "        0.2887, 0.3292, 0.1644, 0.2962, 0.3279, 0.2535, 0.2795, 0.2238, 0.2607,\n",
      "        0.1937, 0.2680, 0.2418, 0.5193, 0.2502, 0.3147, 0.2166, 0.2313, 0.2027,\n",
      "        0.1880, 0.2180, 0.3826, 0.3871, 0.2358, 0.3556, 0.2272, 0.3272, 0.3442,\n",
      "        0.3154, 0.1993, 0.3135, 0.2254, 0.3048, 0.2658, 0.3337, 0.2679, 0.2670,\n",
      "        0.2363, 0.4347, 0.1931, 0.1995, 0.2072, 0.3202, 0.2667, 0.2305, 0.2383,\n",
      "        0.2246, 0.2562, 0.2837, 0.4046, 0.2786, 0.2243, 0.1591, 0.1923, 0.1894,\n",
      "        0.2496, 0.1140, 0.3128, 0.3197, 0.3530, 0.2999, 0.2115, 0.4718, 0.2979,\n",
      "        0.3472, 0.2890, 0.4740, 0.2230, 0.3630, 0.4015, 0.2446, 0.1897, 0.1460,\n",
      "        0.1874, 0.2734, 0.2366, 0.3001, 0.2359, 0.2688, 0.3256, 0.2749, 0.2848,\n",
      "        0.2299, 0.3001, 0.4818, 0.3074, 0.3164, 0.3114, 0.3549, 0.2859])\n",
      "tensor([[-0.0162,  0.0060,  0.0305,  ..., -0.0199,  0.0270,  0.0342],\n",
      "        [-0.0019, -0.0315,  0.0344,  ..., -0.0348,  0.0427,  0.0050],\n",
      "        [-0.0108, -0.0412, -0.0379,  ..., -0.0103,  0.0046,  0.0374],\n",
      "        ...,\n",
      "        [-0.0174,  0.0131,  0.0355,  ...,  0.0360,  0.0267,  0.0094],\n",
      "        [ 0.0114,  0.0093, -0.0392,  ..., -0.0344,  0.0203, -0.0432],\n",
      "        [ 0.0335,  0.0311, -0.0098,  ..., -0.0374,  0.0307, -0.0224]])\n",
      "tensor([-0.0003,  0.0393,  0.0341, -0.0085, -0.0322,  0.0322,  0.0219, -0.0243,\n",
      "        -0.0185, -0.0256,  0.0115, -0.0373,  0.0172,  0.0245,  0.0106, -0.0275,\n",
      "        -0.0236, -0.0060,  0.0345, -0.0004,  0.0436, -0.0188, -0.0419,  0.0305,\n",
      "        -0.0163,  0.0219,  0.0111,  0.0245, -0.0274, -0.0197, -0.0263, -0.0131,\n",
      "        -0.0043,  0.0102, -0.0029, -0.0345, -0.0142,  0.0354,  0.0224,  0.0400,\n",
      "        -0.0177,  0.0319, -0.0126, -0.0337, -0.0175,  0.0364,  0.0171,  0.0415,\n",
      "         0.0105, -0.0323, -0.0182,  0.0106,  0.0179, -0.0174,  0.0303, -0.0268,\n",
      "         0.0014,  0.0294, -0.0049, -0.0247,  0.0142, -0.0245,  0.0164, -0.0181,\n",
      "        -0.0089, -0.0300,  0.0320,  0.0132, -0.0079, -0.0188,  0.0306, -0.0014,\n",
      "         0.0346, -0.0308, -0.0214,  0.0051, -0.0098,  0.0284,  0.0056,  0.0104,\n",
      "         0.0047, -0.0377, -0.0384, -0.0385, -0.0130, -0.0076, -0.0336, -0.0125,\n",
      "        -0.0111, -0.0204,  0.0282, -0.0138, -0.0362,  0.0399, -0.0193,  0.0120,\n",
      "         0.0136, -0.0019,  0.0059,  0.0018])\n",
      "tensor([[ 0.0180,  0.0826, -0.0597,  0.0808,  0.0372,  0.0079, -0.0707, -0.0076,\n",
      "         -0.0073,  0.0610, -0.0504,  0.0846,  0.0984, -0.0594,  0.0664,  0.0218,\n",
      "          0.0027, -0.0431,  0.0738, -0.0547, -0.0102, -0.0990,  0.0449, -0.0548,\n",
      "         -0.0941,  0.0220, -0.0457, -0.0228,  0.0689, -0.0867,  0.0023,  0.0494,\n",
      "          0.0636,  0.0378,  0.0476,  0.0049,  0.0933, -0.0899,  0.0953, -0.0378,\n",
      "          0.0674,  0.0843, -0.0757, -0.0574,  0.0140, -0.0794, -0.0670,  0.0265,\n",
      "          0.0889,  0.0880, -0.0516,  0.0876, -0.0562, -0.0958,  0.0656, -0.0075,\n",
      "          0.0049, -0.0447,  0.0453,  0.0067, -0.0097, -0.0896,  0.0874, -0.0492,\n",
      "         -0.0380,  0.0613,  0.0346, -0.0749, -0.0657, -0.0988,  0.0117, -0.0133,\n",
      "          0.0977,  0.0963,  0.0928, -0.0970, -0.0881,  0.0505,  0.0856,  0.0836,\n",
      "          0.0008,  0.0047, -0.0617,  0.0789, -0.0939, -0.0520,  0.0583,  0.0332,\n",
      "          0.0379, -0.0725,  0.0920, -0.0220, -0.0444, -0.0982, -0.0920,  0.0648,\n",
      "         -0.0167,  0.0019, -0.0651,  0.0356],\n",
      "        [-0.0894, -0.0886,  0.0250, -0.0254,  0.0743, -0.0159, -0.0769, -0.0080,\n",
      "          0.0223, -0.0907,  0.0430, -0.0756, -0.0518, -0.0899, -0.0181,  0.0648,\n",
      "         -0.0555, -0.0104,  0.0608, -0.0483,  0.0075, -0.0098, -0.0700,  0.0039,\n",
      "         -0.0743,  0.0419, -0.0929, -0.0291,  0.0887,  0.0153, -0.0165, -0.0131,\n",
      "          0.0571,  0.0237, -0.0735,  0.0100,  0.0110, -0.0647,  0.0405, -0.0379,\n",
      "          0.0092, -0.0403, -0.0197, -0.0290, -0.0928, -0.0129,  0.0900,  0.0931,\n",
      "          0.0541,  0.0872, -0.0155, -0.0377, -0.0965,  0.0383,  0.0403, -0.0948,\n",
      "          0.0474, -0.0484, -0.0831, -0.0465, -0.0357,  0.0661, -0.0450, -0.0910,\n",
      "          0.0201, -0.0956, -0.0079,  0.0104,  0.0880,  0.0413,  0.0032,  0.0685,\n",
      "         -0.0436,  0.0945, -0.0152,  0.0206,  0.0141, -0.0358, -0.0470, -0.0395,\n",
      "          0.0735,  0.0447,  0.0489, -0.0568, -0.0283,  0.0503, -0.0807, -0.0974,\n",
      "         -0.0085, -0.0074, -0.0311, -0.0565, -0.0827, -0.0197,  0.0720,  0.0556,\n",
      "          0.0463,  0.0020, -0.0127,  0.0495],\n",
      "        [-0.0013,  0.0802,  0.0495,  0.0527,  0.0782,  0.0310, -0.0818,  0.0969,\n",
      "         -0.0078, -0.0530, -0.0228,  0.0621, -0.0471, -0.0921,  0.0155,  0.0909,\n",
      "         -0.0926, -0.0184, -0.0092, -0.0839, -0.0293, -0.0552, -0.0341,  0.0364,\n",
      "         -0.0628,  0.0364,  0.0315, -0.0091,  0.0386,  0.0115,  0.0537, -0.0913,\n",
      "         -0.0831,  0.0459,  0.0552,  0.0655, -0.0169, -0.0333,  0.0311,  0.0425,\n",
      "         -0.0090, -0.0632, -0.0251,  0.0958,  0.0713,  0.0867,  0.0916,  0.0018,\n",
      "          0.0568,  0.0020, -0.0480, -0.0271, -0.0185,  0.0875, -0.0906, -0.0065,\n",
      "          0.0118,  0.0522,  0.0868,  0.0629,  0.0049, -0.0041, -0.0731,  0.0194,\n",
      "         -0.0281, -0.0432, -0.0025, -0.0671,  0.0338,  0.0071,  0.0743, -0.0661,\n",
      "          0.0263,  0.0954, -0.0004, -0.0065,  0.0675,  0.0225, -0.0366, -0.0317,\n",
      "          0.0221, -0.0743, -0.0313, -0.0078,  0.0345, -0.0251,  0.0147,  0.0803,\n",
      "          0.0451,  0.0337, -0.0555,  0.0462,  0.0642, -0.0468,  0.0019, -0.0604,\n",
      "          0.0957, -0.0692,  0.0629, -0.0552],\n",
      "        [ 0.0206, -0.0149,  0.0488, -0.0604, -0.0858, -0.0672, -0.0104, -0.0689,\n",
      "         -0.0407,  0.0911, -0.0147, -0.0415,  0.0430,  0.0744, -0.0217,  0.0105,\n",
      "         -0.0453, -0.0143, -0.0430,  0.0383,  0.0060, -0.0919, -0.0856,  0.0706,\n",
      "         -0.0228, -0.0394, -0.0582,  0.0393,  0.0074, -0.0260,  0.0850, -0.0476,\n",
      "         -0.0772,  0.0962, -0.0451, -0.0284, -0.0582,  0.0513,  0.0923,  0.0873,\n",
      "          0.0468, -0.0561,  0.0529,  0.0386,  0.0892,  0.0237,  0.0114, -0.0839,\n",
      "         -0.0773, -0.0086,  0.0849,  0.0509, -0.0053,  0.0248, -0.0865, -0.0006,\n",
      "         -0.0832, -0.0929,  0.0167, -0.0279, -0.0236,  0.0661, -0.0090,  0.0247,\n",
      "         -0.0067,  0.0209,  0.0173, -0.0728,  0.0260, -0.0794,  0.0154,  0.0039,\n",
      "          0.0856,  0.0006,  0.0327,  0.0666, -0.0183,  0.0719, -0.0549, -0.0297,\n",
      "          0.0132, -0.0937,  0.0350,  0.0382, -0.0081,  0.0774,  0.0057,  0.0818,\n",
      "          0.0773, -0.0626,  0.0244, -0.0833,  0.0062, -0.0998,  0.0449,  0.0943,\n",
      "          0.0476, -0.0702, -0.0547, -0.0326],\n",
      "        [-0.0739,  0.0649, -0.0638,  0.0177,  0.0536, -0.0482, -0.0105,  0.0680,\n",
      "         -0.0019, -0.0796, -0.0890, -0.0317, -0.0993,  0.0869, -0.0761, -0.0672,\n",
      "          0.0636, -0.0001,  0.0067, -0.0089,  0.0784, -0.0796,  0.0752, -0.0770,\n",
      "         -0.0238, -0.0090,  0.0742, -0.0388,  0.0731, -0.0768, -0.0653,  0.0512,\n",
      "          0.0518,  0.0545,  0.0561,  0.0727,  0.0332, -0.0366,  0.0127,  0.0962,\n",
      "         -0.0663,  0.0584,  0.0137,  0.0187, -0.0244,  0.0184, -0.0714, -0.0004,\n",
      "          0.0990,  0.0078, -0.0511,  0.0234,  0.0298, -0.0182,  0.0113,  0.0489,\n",
      "          0.0623,  0.0273, -0.0955,  0.0215, -0.0266,  0.0350, -0.0246, -0.0707,\n",
      "         -0.0404, -0.0071,  0.0684,  0.0579, -0.0691, -0.0633, -0.0052,  0.0798,\n",
      "          0.0218,  0.0764, -0.0399, -0.0703,  0.0377, -0.0301,  0.0590, -0.0911,\n",
      "         -0.0191, -0.0605,  0.0373, -0.0538, -0.0950, -0.0263,  0.0320,  0.0435,\n",
      "         -0.0166,  0.0940, -0.0421,  0.0866,  0.0019,  0.0522, -0.0961, -0.0052,\n",
      "         -0.0734,  0.0207, -0.0911,  0.0387],\n",
      "        [ 0.0187,  0.0634,  0.0287, -0.0663, -0.0241,  0.0692, -0.0804,  0.0419,\n",
      "          0.0180,  0.0497, -0.0340,  0.0328, -0.0397,  0.0670, -0.0596, -0.0335,\n",
      "         -0.0108, -0.0468, -0.0544,  0.0665, -0.0246, -0.0950, -0.0216,  0.0017,\n",
      "         -0.0066, -0.0390, -0.0861,  0.0457,  0.0948,  0.0442,  0.0294,  0.0420,\n",
      "          0.0177,  0.0683,  0.0933,  0.0198, -0.0649, -0.0090,  0.0424,  0.0376,\n",
      "         -0.0140,  0.0256,  0.0986,  0.0615,  0.0431,  0.0054,  0.0062, -0.0305,\n",
      "          0.0521,  0.0879, -0.0364, -0.0719,  0.0615,  0.0545, -0.0034, -0.0172,\n",
      "         -0.0291,  0.0286,  0.0476,  0.0238,  0.0251, -0.0442,  0.0799, -0.0198,\n",
      "          0.0586, -0.0999,  0.0124, -0.0675,  0.0679,  0.0573, -0.0348,  0.0606,\n",
      "         -0.0850, -0.0275,  0.0970,  0.0594, -0.0551,  0.0979,  0.0553, -0.0483,\n",
      "         -0.0927,  0.0756, -0.0713,  0.0611,  0.0060, -0.0526,  0.0646, -0.0749,\n",
      "          0.0199, -0.0688, -0.0747,  0.0139,  0.0676,  0.0126, -0.0499,  0.0928,\n",
      "         -0.0430, -0.0360, -0.0880, -0.0486],\n",
      "        [ 0.0197,  0.0115, -0.0768, -0.0554,  0.0234, -0.0121, -0.0091, -0.0199,\n",
      "         -0.0926, -0.0185,  0.0911, -0.0752,  0.0597,  0.0167, -0.0465,  0.0104,\n",
      "         -0.0203,  0.0728, -0.0276,  0.0984, -0.0215,  0.0545,  0.0335,  0.0423,\n",
      "          0.0750,  0.0261,  0.0968,  0.0946,  0.0315, -0.0717,  0.0112, -0.0757,\n",
      "          0.0397,  0.0186,  0.0502,  0.0185,  0.0563, -0.0720,  0.0442, -0.0859,\n",
      "          0.0222, -0.0067, -0.0155,  0.0951, -0.0370,  0.0692, -0.0681,  0.0807,\n",
      "          0.0080,  0.0103,  0.0314,  0.0484, -0.0933,  0.0603,  0.0390, -0.0725,\n",
      "          0.0694, -0.0751, -0.0699, -0.0766,  0.0308,  0.0355,  0.0563,  0.0164,\n",
      "          0.0719, -0.0223, -0.0049,  0.0512,  0.0786, -0.0952, -0.0691,  0.0844,\n",
      "          0.0601, -0.0141,  0.0569, -0.0266,  0.0639,  0.0336,  0.0169, -0.0686,\n",
      "          0.0995, -0.0859,  0.0089, -0.0573, -0.0150,  0.0139, -0.0759, -0.0498,\n",
      "          0.0365, -0.0084, -0.0100,  0.0980, -0.0603,  0.0119,  0.0742, -0.0403,\n",
      "          0.0076,  0.0060, -0.0072, -0.0537],\n",
      "        [-0.0349,  0.0948, -0.0851, -0.0556, -0.0784,  0.0570, -0.0540, -0.0074,\n",
      "         -0.0976, -0.0754, -0.0679, -0.0464,  0.0792,  0.0981, -0.0172,  0.0045,\n",
      "         -0.0293,  0.0057,  0.0773, -0.0773, -0.0030, -0.0170, -0.0807, -0.0280,\n",
      "         -0.0684, -0.0515,  0.0401,  0.0604, -0.0870, -0.0944, -0.0510,  0.0818,\n",
      "         -0.0997,  0.0854,  0.0192, -0.0821, -0.0066,  0.0380, -0.0571, -0.0884,\n",
      "          0.0770, -0.0674, -0.0755,  0.0415,  0.0191,  0.0972, -0.0921, -0.0674,\n",
      "          0.0313,  0.0361,  0.0372,  0.0814, -0.0721, -0.0701, -0.0771, -0.0001,\n",
      "         -0.0359, -0.0928, -0.0700,  0.0281, -0.0791, -0.0445,  0.0546, -0.0633,\n",
      "          0.0412, -0.0014, -0.0622, -0.0368, -0.0077,  0.0449, -0.0645,  0.0753,\n",
      "         -0.0797,  0.0685,  0.0466, -0.0466,  0.0991,  0.0938,  0.0277,  0.0183,\n",
      "         -0.0831, -0.0426, -0.0577,  0.0626, -0.0420, -0.0713, -0.0350,  0.0385,\n",
      "         -0.0562, -0.0067,  0.0008,  0.0389,  0.0927, -0.0702, -0.0109, -0.0282,\n",
      "         -0.0522, -0.0330, -0.0809,  0.0236],\n",
      "        [-0.0713,  0.0272, -0.0897,  0.0848,  0.0807,  0.0312,  0.0577,  0.0241,\n",
      "         -0.0433,  0.0145,  0.0058,  0.0593,  0.0348,  0.0194, -0.0934,  0.0878,\n",
      "         -0.0135,  0.0283, -0.0511,  0.0357,  0.0221,  0.0503,  0.0158,  0.0948,\n",
      "         -0.0116, -0.0564, -0.0129, -0.0695,  0.0329,  0.0024, -0.0153, -0.0617,\n",
      "         -0.0039,  0.0562, -0.0762,  0.0644, -0.0249,  0.0224, -0.0532, -0.0670,\n",
      "         -0.0069,  0.0842, -0.0232,  0.0097, -0.0620, -0.0455,  0.0920,  0.0568,\n",
      "          0.0307,  0.0892,  0.0993,  0.0996,  0.0756,  0.0690, -0.0884,  0.0141,\n",
      "         -0.0890, -0.0785, -0.0419,  0.0622, -0.0388,  0.0811, -0.0856,  0.0863,\n",
      "          0.0079, -0.0720,  0.0953, -0.0154, -0.0830,  0.0974, -0.0418,  0.0403,\n",
      "         -0.0257,  0.0968, -0.0738,  0.0622,  0.0600,  0.0053,  0.0195,  0.0895,\n",
      "          0.0488, -0.0178,  0.0661,  0.0734, -0.0415,  0.0183,  0.0007, -0.0379,\n",
      "          0.0055,  0.0956, -0.0051,  0.0680, -0.0027, -0.0964,  0.0393, -0.0823,\n",
      "         -0.0102,  0.0209, -0.0666, -0.0331],\n",
      "        [ 0.0708, -0.0572,  0.0243,  0.0642, -0.0939,  0.0062,  0.0992,  0.0543,\n",
      "         -0.0077,  0.0844,  0.0312,  0.0817,  0.0193,  0.0739, -0.0153, -0.0520,\n",
      "          0.0777,  0.0289,  0.0617, -0.0090,  0.0326, -0.0898,  0.0319,  0.0282,\n",
      "          0.0961, -0.0513,  0.0437,  0.0066,  0.0596, -0.0680, -0.0980, -0.0046,\n",
      "         -0.0073, -0.0142, -0.0759,  0.0085, -0.0141, -0.0950, -0.0396,  0.0484,\n",
      "          0.0033,  0.0156,  0.0547,  0.0388, -0.0128,  0.0601, -0.0731,  0.0170,\n",
      "         -0.0566,  0.0757, -0.0238, -0.0720, -0.0317,  0.0310, -0.0082,  0.0703,\n",
      "         -0.0099,  0.0176,  0.0508,  0.0685, -0.0135, -0.0596,  0.0133, -0.0127,\n",
      "         -0.0864,  0.0868, -0.0030,  0.0559, -0.0290, -0.0725,  0.0293,  0.0247,\n",
      "          0.0941,  0.0309, -0.0713, -0.0888,  0.0288,  0.0495, -0.0125, -0.0225,\n",
      "         -0.0550, -0.0160, -0.0491,  0.0577, -0.0186, -0.0827,  0.0092,  0.0761,\n",
      "         -0.0207, -0.0677, -0.0326,  0.0320,  0.0768,  0.0921,  0.0594,  0.0374,\n",
      "          0.0631, -0.0122,  0.0883, -0.0043]])\n",
      "tensor([ 0.0164, -0.0391, -0.0130,  0.0791,  0.0526, -0.0018, -0.0186,  0.0602,\n",
      "        -0.0698,  0.0128])\n",
      "tensor([[-0.0058, -0.2753,  0.1046, -0.0289, -0.2073, -0.1514, -0.3037, -0.2461,\n",
      "         -0.0223,  0.2676]])\n",
      "tensor([-0.2985])\n",
      "total params: 11228833\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for p in net.parameters():\n",
    "    n_params = np.prod(list(p.data.shape)).item()\n",
    "    count += n_params\n",
    "    print(p.data)\n",
    "print(f'total params: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.RMSprop(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On epoch: 0\n",
      "Outputs tensor([-0.3081, -0.3038, -0.2854, -0.2862, -0.2710, -0.2576, -0.2739, -0.2688,\n",
      "        -0.3055, -0.2413, -0.2815, -0.2743, -0.2689, -0.3155, -0.3266, -0.2784,\n",
      "        -0.2816, -0.2836, -0.2847, -0.3128], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 77873.671875\n",
      "Outputs tensor([-114.5681, -107.0456, -107.5486, -104.4761,  -93.4040, -104.5260,\n",
      "        -106.2856, -104.0927,  -96.6535,  -94.2634, -106.9238, -100.0984,\n",
      "        -106.1375, -101.9846, -105.1749, -103.9042,  -95.7550, -101.0633,\n",
      "         -90.0632,  -97.1842], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 73945.796875\n",
      "Outputs tensor([ -988.4918, -1011.7834,  -961.2576, -1150.4128, -1049.1892, -1068.3984,\n",
      "         -954.0379, -1171.3805, -1079.7764, -1045.2427, -1123.2295, -1118.2269,\n",
      "        -1054.9429, -1044.4857,  -937.7114, -1159.4387, -1157.4197, -1023.9839,\n",
      "        -1044.1715, -1099.8535], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 920752.0\n",
      "Outputs tensor([-149.4324, -158.1689, -166.9077, -157.4520, -163.6767, -139.2818,\n",
      "        -171.3093, -170.5473, -155.4789, -139.5676, -166.3291, -158.9444,\n",
      "        -161.2505, -148.5421, -160.0346, -179.9677, -161.5349, -166.6678,\n",
      "        -156.4107, -196.8818], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 64783.3046875\n",
      "Outputs tensor([-111.9703, -123.4200, -119.1214, -133.2484, -137.9006, -139.4174,\n",
      "        -140.8008, -119.6574, -144.5576, -145.9288, -117.9238, -122.6568,\n",
      "        -126.2972, -144.0009, -130.0592, -148.9488, -137.4328, -131.2281,\n",
      "        -124.5303, -128.6199], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 85373.3671875\n",
      "Outputs tensor([-233.9386, -147.7255, -176.7917, -188.8563, -176.1498, -209.7872,\n",
      "        -194.5073, -192.9635, -199.3508, -168.2373, -201.9642, -198.9579,\n",
      "        -205.2420, -171.0785, -167.2638, -178.3801, -210.4709, -219.3488,\n",
      "        -205.7059, -192.3897], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 92193.734375\n",
      "Epoch training loss: 219153.64583333334\n",
      "Epoch validation loss: tensor([[83846.8750]])\n",
      "\n",
      "On epoch: 1\n",
      "Outputs tensor([ -92.3997, -100.4472, -111.3365,  -97.1811,  -83.7350,  -87.4706,\n",
      "         -87.3908,  -94.3202,  -86.3320,  -89.7344,  -93.5526,  -93.0988,\n",
      "         -98.7689,  -89.6892, -102.9741,  -96.3948, -106.4912,  -77.0592,\n",
      "        -124.9272,  -85.8540], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 87049.734375\n",
      "Outputs tensor([-103.4986, -132.1903,  -94.0953, -124.7165, -100.1366, -139.2754,\n",
      "        -113.4283,  -84.4381, -117.2734,  -98.0868,  -90.8630, -124.9957,\n",
      "        -104.8886,  -97.1683, -111.5765, -129.8227, -117.9304, -107.8014,\n",
      "        -103.2050,  -97.4018], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 73824.796875\n",
      "Outputs tensor([-140.3300, -151.0805, -128.2298, -160.4217, -174.6329, -142.0551,\n",
      "        -179.2724, -203.2582, -198.3938, -168.2513, -158.7938, -173.2100,\n",
      "        -134.7718, -133.0493, -152.2610, -175.9364, -165.2033, -159.3910,\n",
      "        -168.7970, -177.8676], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44283.5625\n",
      "Outputs tensor([-143.5177, -160.0902, -147.5127, -146.3797, -149.5856, -163.6533,\n",
      "        -124.2710, -135.4522, -144.3498, -140.4781, -149.5046, -176.5967,\n",
      "        -150.1672, -172.1581, -140.4721, -146.3169, -149.9531, -156.1242,\n",
      "        -138.0758, -191.6494], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 52856.4296875\n",
      "Outputs tensor([-113.7091, -137.1293, -109.2792, -145.5591, -118.0296, -113.6679,\n",
      "         -93.5452, -126.0090, -121.3349, -128.7219,  -82.1837, -122.3888,\n",
      "        -110.5247, -115.7842, -129.9046, -143.1778,  -94.6609,  -90.9684,\n",
      "        -143.6640, -131.0641], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 83557.359375\n",
      "Outputs tensor([ -86.4599, -113.5832, -113.7660,  -83.4518,  -81.4647, -105.7213,\n",
      "         -58.2243, -101.7263, -120.5619,  -59.8968,  -59.1939,  -85.2051,\n",
      "         -85.7713,  -65.0986,  -53.5790,  -77.5447,  -98.5925,  -65.8300,\n",
      "        -105.4655,  -64.3167], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 80920.734375\n",
      "Epoch training loss: 70415.43619791667\n",
      "Epoch validation loss: tensor([[78948.3594]])\n",
      "\n",
      "On epoch: 2\n",
      "Outputs tensor([ -88.6377, -178.3648, -138.5586, -167.9967,  -96.3494, -141.3106,\n",
      "        -113.5957, -127.0663, -109.5185, -103.5658, -122.9127, -149.3720,\n",
      "         -98.7465, -107.8157, -170.6625, -152.4320,  -99.1861, -108.1803,\n",
      "        -103.9263, -111.8733], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 56731.5703125\n",
      "Outputs tensor([-198.2984, -113.6944, -135.0987, -190.8942, -169.3862, -155.6537,\n",
      "        -174.6596, -193.8235, -170.8460, -200.4205, -190.9057, -185.8187,\n",
      "        -126.7918, -139.6950, -188.6861, -126.8245, -129.4200, -115.7107,\n",
      "        -225.8757, -114.2904], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 95376.7578125\n",
      "Outputs tensor([ -73.1070, -112.6871,  -32.3384,  -91.7609,  -79.6749,  -58.4704,\n",
      "         -48.4855,  -33.5358,  -42.6327,  -43.4195,  -88.5104,  -67.8604,\n",
      "         -52.1874,  -38.8257,  -62.7389,  -81.2842, -106.4868,  -57.6296,\n",
      "         -45.9790,  -50.3972], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 54800.4375\n",
      "Outputs tensor([ -94.0885,  -56.9409,  -71.6060,  -95.8850,  -67.7447,  -56.8098,\n",
      "        -113.4405, -160.5813, -134.1754,  -93.2583, -144.4178, -144.2784,\n",
      "         -92.2019,  -81.8771,  -65.8904,  -95.6590,  -69.3208,  -97.0328,\n",
      "         -65.3070,  -86.3920], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 76033.6953125\n",
      "Outputs tensor([ -73.1104,  -49.6240,  -40.4471,  -34.6906, -106.9758,  -50.5857,\n",
      "         -43.9236, -148.0804,  -42.6726,  -52.7112,  -60.7765, -140.3321,\n",
      "         -45.3898,  -54.9305, -116.4375,  -73.8755,  -86.5569, -102.9517,\n",
      "         -34.5812,  -81.6915], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 59583.9296875\n",
      "Outputs tensor([-126.6502, -208.0076, -190.3719, -152.0412, -185.3056, -111.9651,\n",
      "        -200.1344, -197.5166, -101.1056, -108.3703, -136.4672, -100.8474,\n",
      "        -103.8284, -127.3914, -164.2456, -199.3433, -204.4570, -180.2961,\n",
      "        -126.7377,  -91.9611], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 58693.15625\n",
      "Epoch training loss: 66869.92447916667\n",
      "Epoch validation loss: tensor([[71902.1094]])\n",
      "\n",
      "On epoch: 3\n",
      "Outputs tensor([-136.1294, -173.7026, -233.0447, -173.3858, -144.3763, -141.2599,\n",
      "        -296.0061, -148.2075, -170.1633, -191.5068, -244.4355, -138.9056,\n",
      "        -183.0027, -146.9626, -255.2208, -149.1162, -198.1908, -228.7491,\n",
      "        -244.7303, -271.3440], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 68347.40625\n",
      "Outputs tensor([ -77.9024,  -59.6561, -122.8169, -184.8078, -123.5664,  -74.3179,\n",
      "         -63.5198,  -90.5291,  -82.3015, -153.1354,  -86.4934, -124.8942,\n",
      "        -177.8382,  -76.8866, -129.3111,  -71.0350,  -81.2255, -137.0582,\n",
      "         -83.6966, -166.4662], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42646.2421875\n",
      "Outputs tensor([-177.4206, -107.7497, -191.0289, -152.8205, -209.6098, -218.7007,\n",
      "        -128.3908, -139.6589, -102.6483, -147.4152, -160.0125, -165.6888,\n",
      "         -93.5839, -157.8307,  -96.8191,  -95.4986, -128.0559, -112.8646,\n",
      "        -192.3948,  -96.4549], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 64319.17578125\n",
      "Outputs tensor([ -62.0483, -200.4097, -106.2964,  -84.4730,  -86.2756,  -90.1334,\n",
      "         -81.8302, -199.9563, -183.0233, -178.3395,  -82.5893, -182.0940,\n",
      "        -198.2786,  -92.1374,  -58.6493, -158.8018,  -64.3274, -188.6373,\n",
      "        -200.8192,  -78.3559], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 69017.078125\n",
      "Outputs tensor([-188.5838, -236.5594, -177.3310, -140.3887, -188.3265, -313.3280,\n",
      "        -304.2854, -198.5686, -303.5558, -238.0417, -191.1489, -343.2178,\n",
      "        -195.4502, -295.1438, -243.6128, -203.6344, -230.8034, -342.4841,\n",
      "        -305.8187, -242.4969], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 117948.5\n",
      "Outputs tensor([ -39.2964, -161.5103,  -76.8552,  -85.4801, -121.2732,  -43.2831,\n",
      "         -35.9394, -131.0765,  -30.4817, -151.6566,  -47.4920,  -17.2073,\n",
      "         -40.4519,  -63.2990,  -60.6471,  -19.7307,  -32.6528, -153.0404,\n",
      "        -148.0554,  -54.2093], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 65996.6328125\n",
      "Epoch training loss: 71379.17252604167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch validation loss: tensor([[70921.1328]])\n",
      "\n",
      "On epoch: 4\n",
      "Outputs tensor([ -63.0962,  -55.5741,  -57.7723,  -57.9302, -192.9997,  -90.4519,\n",
      "         -74.3669,  -82.3068,  -88.9918,  -87.6199, -208.6293, -185.3494,\n",
      "         -71.1429, -137.4691,  -84.7143, -182.0281, -154.0600, -102.0044,\n",
      "        -156.2486, -124.7181], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 58524.71875\n",
      "Outputs tensor([ -74.8123, -163.7251, -196.1156, -117.4501, -187.4494,  -80.8288,\n",
      "         -65.5899,  -72.3043,  -98.2622,  -46.8704,  -80.1129, -249.2979,\n",
      "        -154.5309, -103.5497, -239.2301, -107.1727,  -66.5352, -252.8975,\n",
      "         -93.9458, -148.5485], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39648.80859375\n",
      "Outputs tensor([-104.7965, -189.1286, -190.3096, -227.1131, -119.6425, -218.9950,\n",
      "         -96.9258,  -80.1604, -137.0454,  -48.8676,  -38.8747,  -65.4113,\n",
      "        -122.9130,  -68.4709,  -65.0894, -187.3584, -235.9258, -168.1657,\n",
      "         -56.3085, -135.5975], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 57634.48828125\n",
      "Outputs tensor([ -70.5681, -127.8859,  -80.4309,  -76.5688,  -62.6727, -143.3846,\n",
      "         -50.1388, -272.2686, -125.7934, -173.0679,  -61.1498, -143.5975,\n",
      "        -236.9769,  -98.9967,  -61.2921, -123.9691, -164.4565, -211.7684,\n",
      "         -65.3765,  -61.9995], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40554.00390625\n",
      "Outputs tensor([ -94.4037,  -84.8118, -168.5823,  -19.5761,  -33.8389, -117.4406,\n",
      "         -15.1748, -226.5178,  -35.0687, -112.8419, -225.3484,  -32.7692,\n",
      "         -38.3726,   -7.1721, -112.4174, -205.3418,  -35.8990, -184.5407,\n",
      "         -76.0943,  -28.9865], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 78291.7109375\n",
      "Outputs tensor([ -27.7897,   -3.6915,  -53.9746, -123.7557, -190.1357,  -75.3797,\n",
      "         -27.2386,  -83.3307,  -15.1798,  -14.1620,  -13.7392, -101.9091,\n",
      "        -190.4558, -132.5804,  -92.5866, -146.7861,  -35.4188, -187.5853,\n",
      "         -20.7803, -187.4414], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 77007.515625\n",
      "Epoch training loss: 58610.207682291664\n",
      "Epoch validation loss: tensor([[64235.7227]])\n",
      "\n",
      "On epoch: 5\n",
      "Outputs tensor([-173.1362, -145.5782, -116.1646,  -86.8889, -189.2098, -304.6424,\n",
      "         -84.0796,  -86.9510, -230.8940, -289.1913,  -96.0841, -322.9045,\n",
      "        -111.1405,  -65.0682,  -72.2615, -125.7037, -325.7598,  -73.6384,\n",
      "        -258.1402, -252.3097], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49330.1484375\n",
      "Outputs tensor([-114.3437, -259.9638, -273.9704,  -72.1785,  -67.7322,  -48.5702,\n",
      "         -74.6878, -234.6777, -275.3064, -150.3747, -148.2753,  -53.9268,\n",
      "         -45.5925,  -28.1493,  -35.4397, -274.5204,  -52.9512, -159.6030,\n",
      "        -125.5359, -306.2884], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38954.93359375\n",
      "Outputs tensor([-129.2385,  -89.6803, -159.1041, -142.3812,  -88.5451, -275.0305,\n",
      "         -93.7644,  -80.8628,  -82.5277, -121.8602, -321.6573, -192.5593,\n",
      "        -258.5721, -159.8182, -180.9515,  -83.1150, -194.3577, -145.3306,\n",
      "        -240.9762, -328.0028], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 62094.875\n",
      "Outputs tensor([ -25.9697, -150.5858, -306.2463, -322.1193,  -59.2010,  -98.9491,\n",
      "         -15.4188,  -33.8903, -149.7516, -252.5108, -128.9367, -221.1727,\n",
      "         -51.1737, -136.1139,  -98.7933,  -41.1814,  -20.9313, -231.5089,\n",
      "         -10.9250, -159.6676], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 61443.23046875\n",
      "Outputs tensor([ -45.2918,  -18.0455,  -18.9489, -173.8432,   -1.1562,  -47.1458,\n",
      "        -125.9276,  -11.7130,  -73.9398,  -24.1048,  -18.3257, -266.0979,\n",
      "        -247.2722,  -15.9038,   -6.6677,  -33.4726, -230.4501,  -16.0197,\n",
      "         -73.8466, -292.2490], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 60312.42578125\n",
      "Outputs tensor([ -46.4225,  -55.6481,  -18.0681,  -36.2839,  -39.1981, -335.1136,\n",
      "        -118.3200,  -21.3744,  -34.9839,  -30.4446, -233.3831, -329.5957,\n",
      "         -14.4637,  -21.8463,  -12.8325,  -39.1405, -211.2777, -261.2352,\n",
      "        -156.8419,  -31.5590], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 21936.986328125\n",
      "Epoch training loss: 49012.099934895836\n",
      "Epoch validation loss: tensor([[55450.4844]])\n",
      "\n",
      "On epoch: 6\n",
      "Outputs tensor([ -67.6743,  -20.5470, -175.1330, -370.1471,  -62.4030,  -45.1524,\n",
      "         -93.0013, -142.4712, -129.7034, -274.5322, -115.5323,  -64.2386,\n",
      "        -387.6451, -315.5991, -272.5370, -177.8083,  -44.9838, -112.2953,\n",
      "         -82.5036, -325.3733], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 20246.92578125\n",
      "Outputs tensor([-157.4764, -191.8235, -444.2030, -117.1996, -144.2939, -261.7578,\n",
      "        -528.0254, -228.0089, -137.4453, -464.7167, -176.7835, -267.1831,\n",
      "        -288.5085, -203.0003, -535.9504, -115.0352,  -73.2612, -314.9940,\n",
      "         -97.6334, -160.6330], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 64878.9296875\n",
      "Outputs tensor([   0.9458,  -46.5130,   -2.6996, -174.6838,   11.4686,  -14.3179,\n",
      "           8.2725,   12.3982, -147.0750,    6.8034,   11.6604, -146.8432,\n",
      "        -199.3569,   -0.6917,   -2.5540,  -97.9793, -133.0636, -119.1184,\n",
      "          -7.1947,  -26.4456], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 109040.625\n",
      "Outputs tensor([ -83.6399,  -78.1509, -112.9626,  -87.7078,  -86.2651, -394.8282,\n",
      "        -128.7694,  -51.8659, -247.0320,  -80.5871, -441.2785, -161.3311,\n",
      "        -288.2506, -142.8354,  -98.1596, -167.1173, -105.5816,  -63.6699,\n",
      "        -298.4042, -435.1068], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 58046.0234375\n",
      "Outputs tensor([-2.6166e+01, -2.6496e+02, -2.5300e-01,  1.1080e+00, -2.3317e+02,\n",
      "        -2.1327e+01, -1.7213e+01, -5.7170e+00, -8.6380e+01, -1.0955e+02,\n",
      "        -9.6471e+00, -1.6422e+01, -2.9528e+02, -1.0015e+01, -1.8546e+01,\n",
      "         1.5254e+00, -2.3097e+01, -2.9123e+02, -2.2750e+01, -1.7500e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46861.6328125\n",
      "Outputs tensor([  -3.4437,    9.5852, -202.5230, -161.2679,    2.7768,  -21.6574,\n",
      "           3.1178,   -2.5887,    8.3216,    9.1582,    1.8866, -242.6273,\n",
      "        -144.0908,   -6.4712,    4.0430,  -74.4045,    5.5700, -122.9939,\n",
      "        -126.1316, -116.8475], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 65427.7265625\n",
      "Epoch training loss: 60750.310546875\n",
      "Epoch validation loss: tensor([[57495.9805]])\n",
      "\n",
      "On epoch: 7\n",
      "Outputs tensor([ -13.0015, -151.3510,  -39.3203, -363.2121, -248.9152,  -15.9512,\n",
      "        -104.7047,  -16.5431,  -69.2139, -301.2417, -329.8550,  -10.0465,\n",
      "         -10.4888,   -6.8981,  -10.2830, -174.4513, -181.3750, -183.2703,\n",
      "         -13.0653,  -22.9628], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40352.51953125\n",
      "Outputs tensor([-239.5591, -107.0708,  -17.1914, -230.0842, -375.0302,  -13.9964,\n",
      "         -27.2905, -184.8400, -227.5053,  -12.4115,  -79.0392,  -35.5764,\n",
      "        -262.5536,   -8.5727, -359.4363, -363.6990,  -69.7797,  -13.1418,\n",
      "        -276.7208,  -14.5266], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45593.078125\n",
      "Outputs tensor([ -79.3654, -123.0658,  -51.7841,  -87.7626, -181.7699,  -65.8340,\n",
      "         -24.7007, -269.6973, -174.1711,  -25.5738, -390.9218, -186.1033,\n",
      "        -405.3352, -171.6675,  -35.6308, -304.5161,  -60.7830, -119.5089,\n",
      "        -174.7742, -245.0168], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51218.0546875\n",
      "Outputs tensor([ -88.9324,  -52.0667, -447.8227,  -21.9074,  -39.4017, -173.1027,\n",
      "         -50.9565,  -38.5035,  -19.9341, -179.9503, -390.3774,  -61.3089,\n",
      "        -263.6190,  -60.9613, -150.8060, -162.4185,  -58.5958,  -24.6482,\n",
      "        -105.4123, -226.1185], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49764.14453125\n",
      "Outputs tensor([  -4.0327, -207.6695, -218.2319,  -54.3661,  -41.1132, -256.8777,\n",
      "         -16.8533,  -53.9426,  -62.7193,    1.1160, -297.9647, -188.3692,\n",
      "        -159.6305,  -11.8154,    3.5397,  -10.7197, -312.7449,  -49.7721,\n",
      "         -19.6252,  -15.2216], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39668.7890625\n",
      "Outputs tensor([ -51.7728, -331.8367, -366.2079,  -13.3065,  -44.3684, -255.1824,\n",
      "        -299.4702,   -5.9896,  -11.8175,    2.9142,   -6.6873, -293.6685,\n",
      "        -325.9702,  -21.8993,    2.9927,   -7.0900,  -15.9639,  -81.0794,\n",
      "          -5.8987,    5.8781], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40479.76953125\n",
      "Epoch training loss: 44512.725911458336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch validation loss: tensor([[52532.6445]])\n",
      "\n",
      "On epoch: 8\n",
      "Outputs tensor([  -1.3896,  -75.6702,  -48.9624, -482.9067,  -17.7339, -187.8246,\n",
      "        -100.7405, -289.9716,    1.9537,    1.8509,   -2.5025, -408.6460,\n",
      "          -7.0467, -289.0361,   -2.3640, -121.8157,    3.4328, -274.0158,\n",
      "        -297.0957,  -67.2784], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30039.037109375\n",
      "Outputs tensor([-101.4118, -243.7786, -248.3032, -349.5772, -107.6574, -232.1910,\n",
      "        -598.8016,  -83.3304, -356.0197, -116.8767, -130.9306,  -15.7841,\n",
      "         -78.6949,  -30.7212,  -76.0439, -376.4280, -205.3689,  -65.5845,\n",
      "         -59.8236,  -71.3241], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 74047.3125\n",
      "Outputs tensor([-213.5966,  -41.1011,   -2.8870,   -3.2117,  -59.4592,    1.6149,\n",
      "           3.6625,    6.1700,  -65.1441, -325.2875,    9.2202, -267.7357,\n",
      "           6.0326,    6.7263,    3.9668, -246.4511,    2.6654,    8.6785,\n",
      "           3.2953,    7.0843], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40850.0859375\n",
      "Outputs tensor([   7.4928,   24.0938,    7.3763,    7.0024,   -8.9257, -260.6224,\n",
      "        -240.1135, -170.9971,   15.6528,   13.8533,   14.1046,  -87.6871,\n",
      "        -254.4132,  -91.0752,   13.1610, -110.6301,  -31.8294, -215.7123,\n",
      "           9.1498,   -0.3395], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 52598.1953125\n",
      "Outputs tensor([-1.9806e+01, -2.5746e+01,  2.9735e+00, -3.6320e+02, -4.4500e+02,\n",
      "        -3.4456e+02,  5.1716e+00, -6.5549e+01, -2.6757e+02, -4.0358e+02,\n",
      "        -5.0297e+01, -2.7696e+02,  4.6936e+00,  7.6834e+00,  1.7284e+00,\n",
      "        -1.9921e-01, -3.3015e+02,  3.7822e+00, -2.3005e+02, -6.5594e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31490.59765625\n",
      "Outputs tensor([ -99.2420, -366.7840,  -14.7100, -100.7183,  -43.3516, -215.8345,\n",
      "         -31.1893,  -18.2618, -131.3672, -170.3515, -494.7643,  -73.0088,\n",
      "        -267.0679,  -34.6047, -493.6864, -462.0096, -178.1209,  -38.9605,\n",
      "        -141.3849, -158.5037], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 60202.3984375\n",
      "Epoch training loss: 48204.6044921875\n",
      "Epoch validation loss: tensor([[49691.5156]])\n",
      "\n",
      "On epoch: 9\n",
      "Outputs tensor([ -21.3931,    2.5076,    6.9953, -300.9617,  -54.8727,  -34.0420,\n",
      "           5.4095,    6.5974, -272.5391,    4.2444,  -92.9780, -241.7759,\n",
      "           4.1322, -420.0961,  -46.4454,    7.2304,   -2.1185,    1.6923,\n",
      "        -396.0782,  -64.9800], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46454.16015625\n",
      "Outputs tensor([ 3.1480e+00, -3.9025e+01, -1.4575e+02,  3.6995e+00, -8.5543e+01,\n",
      "        -3.2424e+02, -5.1998e+02, -1.4337e-01, -8.8296e+00, -2.1863e+02,\n",
      "        -2.9543e+02, -5.0162e+02,  1.0591e+00, -4.6611e+00, -2.1061e+01,\n",
      "        -1.6456e+01, -2.0211e+02,  3.2735e+00, -6.6257e+01, -3.4038e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37285.3359375\n",
      "Outputs tensor([ -15.7710, -393.6347,  -26.7152,    1.3752,  -82.1125, -287.6971,\n",
      "           3.4310,   -4.1608,    0.7348, -277.7113,    2.7900,   -5.1696,\n",
      "        -410.1430, -153.0399,    7.8545,    5.0752,    2.7545,    0.7929,\n",
      "         -32.5699, -470.1727], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35481.62109375\n",
      "Outputs tensor([   3.4005,  -33.1336,    4.0588,  -23.6124,  -12.6922, -193.2054,\n",
      "          -7.7978, -376.7722,  -91.0013, -139.3578, -254.1091, -447.6054,\n",
      "         -15.9669,   -5.5302, -298.4175,  -48.5717,  -15.7435,  -12.5223,\n",
      "          -7.2572, -454.7187], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34185.375\n",
      "Outputs tensor([ -57.1922,  -90.1999, -287.9885, -382.1387, -529.3119,  -71.6267,\n",
      "        -120.5012, -724.0034, -200.1224, -122.0988, -353.9588,  -43.5896,\n",
      "        -120.0187, -127.5161, -194.6365, -330.7602, -159.9874, -616.6219,\n",
      "        -427.2997,  -44.3968], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 90961.625\n",
      "Outputs tensor([ -11.6297,    8.6012, -237.4202, -285.5428,   10.5954, -172.0353,\n",
      "           8.7116,  -11.9478,   -8.6986, -169.6167,   10.0176,    6.2587,\n",
      "         -16.2742, -145.8136,    7.6514,   10.1173, -183.0872,    6.1798,\n",
      "         -61.7865,    5.3785], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 57566.1171875\n",
      "Epoch training loss: 50322.372395833336\n",
      "Epoch validation loss: tensor([[48562.0156]])\n",
      "\n",
      "On epoch: 10\n",
      "Outputs tensor([  -0.8722,  -17.4117,  -48.9846,    9.6294,    2.8443,    8.1771,\n",
      "        -412.6418, -348.4653, -325.9617,    4.8128,    9.8532,    7.5920,\n",
      "           8.7469, -226.5487,    7.1765, -174.2059,    4.8580, -146.8588,\n",
      "           3.0892,  -10.4097], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39101.28125\n",
      "Outputs tensor([-220.4460,   12.3440, -336.1353, -355.2256,  -30.4781,   11.6495,\n",
      "        -115.3611, -366.8094,   13.0697,    6.8821, -268.0708,   -6.4897,\n",
      "           3.8880,    6.3419,    4.2355,  -31.9725, -321.9475,    2.5362,\n",
      "         -64.6045,   -9.0588], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40210.7734375\n",
      "Outputs tensor([   2.5209, -228.1733,  -11.6615,   -7.1000, -569.1827,  -33.2730,\n",
      "        -119.5538, -230.5839,    4.6074,    5.6233, -218.8152,  -11.1249,\n",
      "           3.9876, -352.0644, -423.8557,    6.7911, -276.7977,    5.9436,\n",
      "         -86.7918,    9.1494], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37731.88671875\n",
      "Outputs tensor([-2.7826e+01, -9.4789e+01, -4.5243e+02,  4.4941e+00, -2.5643e+02,\n",
      "        -8.3362e+00, -4.6241e+02,  4.0485e+00, -5.6962e+01, -1.1881e+02,\n",
      "        -2.1448e+02, -8.5524e+00, -7.0321e+00, -1.1188e+02, -9.1806e+00,\n",
      "        -4.9748e+02,  1.6646e+00, -9.6881e+01, -2.9347e-01, -1.5064e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 48224.91015625\n",
      "Outputs tensor([  -1.6287, -328.6863,   -0.9921,    3.6188, -154.5555, -595.5298,\n",
      "        -116.6305,    7.2683, -211.4632,  -82.0328, -178.8866,  -26.2758,\n",
      "          -3.6376, -393.9655,  -10.8196,   -6.1080,    5.9848,    6.9739,\n",
      "           7.5084, -273.8362], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 18726.44140625\n",
      "Outputs tensor([-162.4233,  -30.1044,   -4.8295,   -3.7423, -475.0188,    4.4673,\n",
      "          -4.0989,    2.0049,   -3.3716,    4.7836, -169.6771,  -43.2091,\n",
      "         -35.9662, -456.4135,  -11.0982, -487.2573, -448.7042, -327.5094,\n",
      "        -238.2617,  -39.1527], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35883.93359375\n",
      "Epoch training loss: 36646.537760416664\n",
      "Epoch validation loss: tensor([[38262.9844]])\n",
      "\n",
      "On epoch: 11\n",
      "Outputs tensor([ -31.2088,  -86.8577, -126.3628,  -92.1374, -269.8913, -154.1709,\n",
      "         -23.9285, -184.1798,   -4.8601,  -28.0822,    5.9924,    3.8314,\n",
      "        -410.6722, -407.7237, -183.1978,  -52.3651,    3.9996,    3.9612,\n",
      "        -299.4424,  -21.7141], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 67851.4921875\n",
      "Outputs tensor([-270.3977,   13.2108,    6.5737,   10.0230,  -34.9633,    9.0929,\n",
      "        -410.2982,   10.5741,   12.5371,   10.5075,   10.0204,    3.8510,\n",
      "           7.3906,    4.4700,    5.2152,  -60.0560,    9.1945,  -91.8809,\n",
      "          11.5837,    2.8216], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37043.2421875\n",
      "Outputs tensor([ -39.5604, -171.9578, -179.5207,   15.2938,    5.9638,   15.5711,\n",
      "          -8.6910,   11.8139,   17.2513, -280.1719,    8.0007, -146.6490,\n",
      "          22.1713,   11.1590, -150.5559,   11.7784, -210.2356,    8.7012,\n",
      "          17.1106,    5.5566], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 78451.53125\n",
      "Outputs tensor([-260.4547, -191.2735,  -14.1687, -424.6123,  -23.0008,   -7.0072,\n",
      "           5.2874,    8.8164,  -15.6171,   -0.7469, -384.4516,    5.0158,\n",
      "         -10.3974,  -28.1286, -359.3586, -263.2522,    7.7203,   -1.2556,\n",
      "        -527.6976, -255.9748], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37983.36328125\n",
      "Outputs tensor([-382.1035, -423.9196,   12.9039, -296.0738, -265.0443,    7.6500,\n",
      "           3.7686,    3.2585,   10.2356,    5.6526,    6.6878,   -1.8150,\n",
      "           5.7524,    4.7311,    5.2275, -372.0632,   -5.7261, -208.9428,\n",
      "        -302.1768,   11.1352], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26370.947265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([   5.7030, -108.9512,  -16.4755,    2.2089,  -85.2866,    3.9034,\n",
      "        -567.4727, -456.4246, -118.1897, -204.1933,    4.1731, -277.3371,\n",
      "        -480.7329,    1.4568, -240.4294,   11.0720,    6.6930,  -37.4243,\n",
      "         -89.9961,  -67.2060], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47876.3828125\n",
      "Epoch training loss: 49262.826497395836\n",
      "Epoch validation loss: tensor([[50714.3047]])\n",
      "\n",
      "On epoch: 12\n",
      "Outputs tensor([  -9.0999, -678.0205, -132.0873, -423.6003, -224.2140,  -93.5828,\n",
      "        -219.0089, -150.3570, -175.0706,  -82.1776, -594.8408,  -12.3742,\n",
      "         -16.9826,  -64.3456, -216.6940,  -89.9066,  -98.4801, -207.5017,\n",
      "         -23.6573, -375.7582], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 52780.88671875\n",
      "Outputs tensor([   7.2625, -160.4923, -165.2971, -159.6828,   -4.2921, -230.1429,\n",
      "           5.1607,    1.4560, -442.2016,  -60.5089,  -20.9456, -389.9430,\n",
      "        -361.3826,    9.4086,    2.6087,  -11.6424,   -2.1822,    3.5391,\n",
      "        -168.3052,    2.6605], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 62318.51953125\n",
      "Outputs tensor([-401.1598, -349.2165, -406.2955,    5.3389,    2.9427, -169.2046,\n",
      "           5.9875,  -30.8480,    9.5289, -276.5211,    4.6256,    5.6074,\n",
      "           4.2791,   -7.1905, -237.4805, -500.4376,  -51.5705,    2.4455,\n",
      "          -6.1084, -321.9393], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35478.0234375\n",
      "Outputs tensor([-160.0603,  -21.6411,   -8.7286,  -61.2010,    8.4095, -442.9217,\n",
      "        -397.0272,    2.5699,  -20.1402, -218.5452,   -2.3615, -569.6062,\n",
      "           1.7443,  -19.8961,  -59.9869, -387.4532,    4.7526,   -3.4811,\n",
      "        -427.7570,  -64.7624], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 18667.595703125\n",
      "Outputs tensor([   8.1126,    8.4566,    3.8804,    5.3301,    7.9690, -511.2607,\n",
      "           1.6185, -209.2940,    4.5837, -308.8051, -177.3949,  -10.2996,\n",
      "         -31.1884,    4.0705,  -13.3137, -223.2724,  -13.5222, -375.3361,\n",
      "           1.4154, -429.9323], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29611.630859375\n",
      "Outputs tensor([ 6.3300e+00, -9.3560e+01, -4.7979e+01, -1.2765e+02, -2.1563e+02,\n",
      "         7.5210e+00, -2.6838e+02, -5.2683e+02, -1.2579e+01,  4.8953e+00,\n",
      "         6.3929e+00, -4.4670e+02, -9.6474e+00,  3.0703e+00, -3.7601e+02,\n",
      "        -5.8208e+02,  2.1880e+00, -7.9469e+01, -5.7974e+02,  4.1473e-03],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42231.60546875\n",
      "Epoch training loss: 40181.376953125\n",
      "Epoch validation loss: tensor([[37010.0703]])\n",
      "\n",
      "On epoch: 13\n",
      "Outputs tensor([-2.1536e+01,  4.4911e+00,  1.2552e+01,  1.4346e+01,  7.1581e+00,\n",
      "         1.1238e+01,  5.7027e+00, -2.4508e+02, -4.6429e+02,  4.0968e-01,\n",
      "        -1.4036e+02,  7.9914e+00, -4.5759e+02,  7.1563e+00,  9.3358e+00,\n",
      "         4.7601e+00, -9.9519e+01, -3.1130e+02, -3.3941e+02,  6.4785e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33163.84375\n",
      "Outputs tensor([-268.3940,    4.0210,    8.5598, -408.2995,    2.8886,    6.5844,\n",
      "           4.4026,    5.7764, -487.3939, -281.3240,    6.3887,  -92.9336,\n",
      "           8.6350, -522.0828, -228.1690,    8.8365,    3.7528,    9.4139,\n",
      "          -2.9748, -555.3339], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 15055.1064453125\n",
      "Outputs tensor([ -16.5787, -471.6829, -137.1713, -181.5905,    8.5452,    2.3952,\n",
      "           6.4716,    3.3091, -254.7534, -326.9506, -359.3734,    2.1655,\n",
      "           5.5492, -437.6359,    4.8902, -445.9442,    5.3393,  -40.1460,\n",
      "        -535.8318,    2.9937], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43423.984375\n",
      "Outputs tensor([ -80.5269, -169.4961, -428.3639, -454.1079,    6.1835,    6.2128,\n",
      "         -12.5391,    1.6113,    3.5882,    1.2258,   12.1549,    1.4177,\n",
      "           7.6766,   13.8192, -318.9516,    3.0204, -326.6958,  -89.4987,\n",
      "        -263.3033,    6.9276], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32062.52734375\n",
      "Outputs tensor([   5.0649,    1.9626, -680.1545,   -4.2462,    3.0745,    3.0384,\n",
      "        -272.1537,    8.6054,   -4.3029,  -80.1156,  -44.2201, -324.6070,\n",
      "        -493.9211,    3.3190,    6.8351,    9.7157,    3.5368,    4.0457,\n",
      "        -533.4445,    2.1444], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30303.06640625\n",
      "Outputs tensor([-122.2940, -129.2253, -230.6492,    8.4763,    0.8482,    8.4201,\n",
      "           3.7470,    5.7141, -185.5283, -224.0014,    1.2762,  -14.1522,\n",
      "        -572.5597,   -0.8935,   -8.4915,    7.0576,    8.9859, -571.3318,\n",
      "           3.9506, -270.4724], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 65881.515625\n",
      "Epoch training loss: 36648.340657552086\n",
      "Epoch validation loss: tensor([[36047.5469]])\n",
      "\n",
      "On epoch: 14\n",
      "Outputs tensor([   3.4600,   13.9916, -363.8909,   13.1187,    4.4372, -427.5162,\n",
      "        -296.5242,   14.2880,    8.5929, -197.7617,  -57.4302,   10.9352,\n",
      "           3.5252,    7.0024,    7.2612,  -93.1494,    5.8429,   14.0209,\n",
      "        -415.3727,    2.5935], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34668.8671875\n",
      "Outputs tensor([   9.7569,  -38.6641,   11.0609,   13.5030,  -73.3160, -252.4695,\n",
      "           9.1836,   14.6514,   16.9883,   14.8973, -164.9603,    6.6415,\n",
      "        -340.3350, -232.1955,  -47.9325,   14.3412,  -61.8970, -395.4245,\n",
      "        -335.1890,  -37.7221], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 60787.83203125\n",
      "Outputs tensor([-545.2163,  -33.4437,  -56.8143, -329.2914,  -14.9576, -166.2723,\n",
      "         -82.7586, -726.1670, -356.2814, -136.9964, -161.7355, -402.5995,\n",
      "         -69.4464,  -30.0363, -621.2888,   -2.1908, -166.7623, -145.7952,\n",
      "         -37.6764,  -67.6219], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 83545.59375\n",
      "Outputs tensor([   7.1680,    6.7686,    8.3707,    9.7435, -187.8379,    1.7016,\n",
      "           6.5113,    6.6232, -117.3575, -187.8937,  -99.2038, -359.0225,\n",
      "          14.3107,    3.9808,  -51.6218,  -26.0544,    5.6143, -360.6727,\n",
      "        -330.7349,   10.8590], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49978.40625\n",
      "Outputs tensor([   3.0514,   -3.1361, -284.3463, -303.1144,    4.6206,    2.1870,\n",
      "           8.6115,  -17.2588, -291.5351,   -2.7955, -155.1695,  -38.2758,\n",
      "        -423.2452,    1.4539,  -13.7330,    6.4709,  -85.8082,  -18.3665,\n",
      "        -185.3589,    1.4133], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43862.99609375\n",
      "Outputs tensor([ -52.0714,   11.9228, -226.0214, -155.5588,  -42.5486, -515.2987,\n",
      "        -449.4181,   10.0754,   11.3208,    1.4930,   -0.9814,    8.7482,\n",
      "           4.6381,    5.7277,    1.0031, -233.3652, -147.2262,    5.4846,\n",
      "           1.7345,    3.2789], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34043.3828125\n",
      "Epoch training loss: 51147.846354166664\n",
      "Epoch validation loss: tensor([[38349.6094]])\n",
      "\n",
      "On epoch: 15\n",
      "Outputs tensor([-412.9674,    7.6410,   15.6368, -270.7371, -186.4442,    4.0827,\n",
      "        -154.2434,    3.6864, -362.3851,    3.7086,    3.0728,    7.4489,\n",
      "        -416.2059,   11.2401,    5.8932,   -6.5174,    0.9912, -395.8037,\n",
      "          -4.4022, -360.1768], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 54377.2890625\n",
      "Outputs tensor([-5.2186e+02, -6.6234e+01, -5.7034e+02, -3.3749e+02,  8.1957e+00,\n",
      "         3.6409e+00,  3.8657e+00, -4.5972e+01, -1.8868e+02, -2.1331e+02,\n",
      "        -2.5284e+00,  5.3697e+00,  7.5131e+00,  6.3732e+00,  8.6277e+00,\n",
      "        -8.7373e+01, -9.6525e+01,  3.2920e-01,  7.3001e+00, -4.1316e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27447.875\n",
      "Outputs tensor([ -12.7215,    3.9060, -137.6839,    4.5053,  -42.7930,  -41.8966,\n",
      "           1.1850, -306.6979,    6.0027, -480.7291,  -75.5260,    2.0356,\n",
      "         -29.6926,  -13.6799, -470.2947, -550.5344, -356.2112,    2.9723,\n",
      "         -74.5480, -491.0125], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 19210.57421875\n",
      "Outputs tensor([-152.5095, -608.2980, -362.6502,    3.8094,    1.4417,  -74.6126,\n",
      "         -25.1991,   -9.6899,  -72.3019, -494.8048,  -78.2197, -160.0021,\n",
      "           0.8998,    1.7551,  -33.7240,    2.7733,    1.4909, -367.7499,\n",
      "        -492.8815,  -17.2916], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39652.77734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([-273.8638, -266.8656, -511.2007,    6.3639, -532.8373, -250.1325,\n",
      "          15.2540,   10.2961,    2.8712, -503.8745,    7.0567,   14.9581,\n",
      "          13.6699,    8.0140,  -34.6981,  -31.6003,   12.5686,    6.8913,\n",
      "           4.0264,   10.3959], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25626.322265625\n",
      "Outputs tensor([  14.9238, -555.4137,   15.2252,    7.9954,  -29.0709,   -1.2775,\n",
      "           2.0960,    7.7983, -164.2808,    6.3713,   14.9880, -244.8503,\n",
      "        -487.8528, -129.7473,  -88.7049, -319.3751,    3.9391,    2.7484,\n",
      "           6.3252,    9.1418], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32026.90234375\n",
      "Epoch training loss: 33056.956705729164\n",
      "Epoch validation loss: tensor([[46460.1914]])\n",
      "\n",
      "On epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aofeldman\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\aofeldman\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ 7.1151e+00,  7.2936e+00, -2.8583e+02,  5.4665e+00,  1.3425e+01,\n",
      "         8.2784e+00,  1.1268e+01, -3.7501e+02, -1.7562e+02,  9.6149e+00,\n",
      "         6.6995e+00, -1.3334e+02,  7.0150e+00,  8.6452e+00, -3.8054e-01,\n",
      "        -4.4699e+02, -3.5395e+02,  4.1188e+00,  5.2948e+00,  3.3427e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 59431.91796875\n",
      "Outputs tensor([-330.5573,  -18.5266, -249.1633,   15.4446,   14.2778, -419.0934,\n",
      "          12.3544,   11.5315,    6.9917,    9.6997,   16.6173,  -87.6270,\n",
      "        -340.0897,    9.5307,   12.7459, -139.8948,   16.8288,    5.4411,\n",
      "           7.3541, -416.1987], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39328.34375\n",
      "Outputs tensor([-467.3857, -541.7798, -348.0974,   10.1051,   11.4207,    5.0602,\n",
      "         -48.8865,    9.5493, -534.4104,  -39.3190,    9.9871, -193.2053,\n",
      "           6.9545,    4.7196,    1.5255, -152.9680,    5.7927,    1.3027,\n",
      "          14.2622,    6.2855], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27867.287109375\n",
      "Outputs tensor([-167.6195, -450.9585,    3.5063,    3.1514,   12.1017,  -11.0143,\n",
      "           3.2770,    2.7235,   -2.2627, -330.7230, -423.3218, -468.7758,\n",
      "          10.4341, -115.0773, -444.6639,  -14.7126,   -2.3533,    4.7601,\n",
      "           9.7740,   15.2008], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37209.40234375\n",
      "Outputs tensor([   5.0441,  -61.1307,    3.0157,   16.2582, -559.6556,   12.3047,\n",
      "           4.5799,    5.0741, -185.4842,   16.6295,    2.5541,    0.9001,\n",
      "        -574.4677,  -70.8807,    2.3666, -464.4490, -337.4254,    1.2174,\n",
      "        -253.3320,    2.6115], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31336.58984375\n",
      "Outputs tensor([ -60.0064,    1.9322,    5.6270,   -1.9683, -412.0820,    5.2086,\n",
      "        -636.6783,    5.3879,    4.8310,  -59.4346, -470.3412,    7.6486,\n",
      "           5.5487, -169.2243, -114.1651,    2.1507,   -8.8616,  -19.5566,\n",
      "        -669.7059, -332.6333], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 54353.15625\n",
      "Epoch training loss: 41587.782877604164\n",
      "Epoch validation loss: tensor([[36407.9141]])\n",
      "\n",
      "On epoch: 17\n",
      "Outputs tensor([   4.6800, -381.4664,   13.6127,    5.5000,   11.7680,    1.7634,\n",
      "         -94.5500, -421.1537,   14.2523, -534.1036,   -2.5555,   15.3092,\n",
      "          -3.1739,   16.8587,   10.2243,    3.8241, -325.8225, -269.0088,\n",
      "        -411.3181,   13.2769], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33411.6796875\n",
      "Outputs tensor([ -56.0612, -477.0928,   -7.0861, -151.4578, -450.2473, -190.2790,\n",
      "         -66.5347,   -0.6904,  -54.2551,   -5.3848,    2.7012,    3.5539,\n",
      "        -405.7326,  -15.3394,    5.5835, -336.6488, -392.8270,    3.8153,\n",
      "         -10.5040,   12.7965], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23380.4140625\n",
      "Outputs tensor([ -16.3517,    3.8656,   10.5745, -570.8850,   -2.6176, -513.3937,\n",
      "        -119.2305, -576.8363,   -5.1767,  -18.6501,   -3.5961, -531.7449,\n",
      "           5.2597,    3.0706,  -68.1357, -138.8033,  -19.3030,    7.5026,\n",
      "          -3.0269,  -78.8203], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51585.0546875\n",
      "Outputs tensor([   9.3770, -289.7379,    7.7526,    4.4803,    9.0717, -496.6520,\n",
      "          11.2158,  -87.7963,   -6.3753,    4.9453,    6.8142,    9.2497,\n",
      "        -161.9421,    2.9416, -496.8580, -240.5230, -201.8461,   -2.7035,\n",
      "           9.6929,    7.3326], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47838.09375\n",
      "Outputs tensor([ -53.4424,    9.8077, -460.4472,    7.5542,   11.9866,   12.6423,\n",
      "        -319.8543,   13.7680,    7.2299,    7.6143,   -0.9305, -401.0933,\n",
      "          -2.7513,   12.3653,  -37.1488, -491.3783,   10.0441,  -92.8305,\n",
      "          -2.4797,   12.0116], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45827.60546875\n",
      "Outputs tensor([-399.0497,  -11.8886,    3.7612,  -10.9363,    6.7725, -193.7825,\n",
      "           9.8006, -463.4452, -178.7234,   10.0696,    8.9282,    7.3861,\n",
      "           6.9986,    9.5613,   -5.6216,   10.6289,   12.6902, -157.4343,\n",
      "        -534.6654,    4.7502], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32278.6875\n",
      "Epoch training loss: 39053.589192708336\n",
      "Epoch validation loss: tensor([[38606.2070]])\n",
      "\n",
      "On epoch: 18\n",
      "Outputs tensor([   4.3828, -213.9402,   17.6353,   13.3274, -397.7916, -262.7637,\n",
      "        -235.6981,  -23.5194,    4.7109,   14.8534,    3.9500, -325.4139,\n",
      "           8.2186,   12.5274,    5.6477, -194.4916, -477.9694,    9.5550,\n",
      "         -56.6210,    6.6233], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46918.109375\n",
      "Outputs tensor([  -5.3452,    7.7587,  -53.4324, -430.4932, -144.1216, -354.1687,\n",
      "           3.2773,    7.7242,    3.2705,  -12.8739,    6.9898, -451.7935,\n",
      "         -17.3033,    2.2579,    9.7981, -375.5019,    2.7732,  -67.5394,\n",
      "           8.7979, -199.8975], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37200.16015625\n",
      "Outputs tensor([-422.7963, -559.5114,  -15.4689,  -56.6816, -248.8411,   13.5276,\n",
      "          13.9636,    4.1071,    8.7752,   -3.4377,  -19.4067, -567.9245,\n",
      "          13.5514, -323.5805,    0.6225,    7.3407, -550.3765,   -4.3483,\n",
      "           5.0615,   -0.8376], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 20612.009765625\n",
      "Outputs tensor([   9.4087,    5.3251,    7.8891,    4.0872, -532.8677,   -1.2899,\n",
      "          -5.9014,  -12.5718,   -3.6104,    0.9147, -736.6223,    8.3331,\n",
      "        -559.5145,    2.3001, -247.7789, -657.9724, -147.9455,  -96.8644,\n",
      "        -117.3857,  -30.7088], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35628.53125\n",
      "Outputs tensor([  11.2173,   12.0420,    1.8863, -135.9008,    5.0355,    2.8243,\n",
      "           4.6059,    8.2867, -445.0050,   13.3590,   -6.9032,    7.2866,\n",
      "           8.8706,  -18.3352, -474.7557,    1.8136, -453.7162, -338.8181,\n",
      "         -92.9550, -544.0608], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37093.9140625\n",
      "Outputs tensor([-128.3023,  -40.8330,   11.9505,    9.4360,   18.0210, -467.8854,\n",
      "           9.9529,   15.9401,   10.5501,   13.9073, -594.3418,    2.6181,\n",
      "           5.4196,  -12.9364,    9.7828,  -44.9041, -138.9335, -236.6793,\n",
      "          -2.7944,  -10.0163], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37846.52734375\n",
      "Epoch training loss: 35883.208658854164\n",
      "Epoch validation loss: tensor([[34962.3203]])\n",
      "\n",
      "On epoch: 19\n",
      "Outputs tensor([ -74.5782,    5.7978, -130.9795, -377.2038,   16.7585,   10.4446,\n",
      "        -287.1837,  -22.4623,   10.1121, -259.9501, -369.0426, -425.1231,\n",
      "           8.0572, -330.0553,   15.2066,   13.9126,    7.3973,   -6.9265,\n",
      "         -13.7429,   18.6551], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43677.265625\n",
      "Outputs tensor([-256.0349,  -17.4927,  -19.5225,  -17.8650,    4.2977,    6.0067,\n",
      "           1.8459,    1.8776, -733.2099,  -27.4779, -313.3848,   -2.0534,\n",
      "          -8.3362, -181.4715,  -59.2902, -221.4663,  -54.5400, -635.1451,\n",
      "        -652.5174,  -13.7540], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 57250.63671875\n",
      "Outputs tensor([-181.9432,   -2.8994,    1.8961, -131.3987,   -6.8643,    7.8157,\n",
      "        -218.7404,   11.3132,  -21.8599,    7.7966,    4.2316, -465.8781,\n",
      "           7.0516, -334.2932,    6.1767, -573.3733,   -2.8237,    9.6843,\n",
      "         -40.0905,    4.0601], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24852.90234375\n",
      "Outputs tensor([ -14.8826,    8.0201,   -1.6946,    2.2096,    1.9944, -131.5462,\n",
      "           9.7236,   13.0092,    3.6869,    5.2613, -537.6444, -372.6957,\n",
      "           4.8895,   -7.3329, -584.3640,  -71.8534,   -2.9978,   -4.7094,\n",
      "           2.5066, -127.7151], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38751.765625\n",
      "Outputs tensor([-179.9775,  -38.3444,   13.3953,   14.1630, -167.9302,    5.1519,\n",
      "        -427.7944,  -46.7539,    1.7734, -496.8524,    7.2299, -509.7832,\n",
      "          12.9693,    2.4505, -293.2356,  -10.3001,    1.8733,    3.0564,\n",
      "           2.2965,    4.3177], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51106.30078125\n",
      "Outputs tensor([ 6.7374e+00, -2.1080e+01,  1.0526e+00, -3.7172e+02, -4.5576e+00,\n",
      "        -4.5383e+02,  2.5126e-01,  5.6250e+00, -1.1231e+01,  1.3536e+00,\n",
      "         1.6601e+00,  3.6736e+00, -4.7151e+02, -4.1841e+02,  3.2831e+00,\n",
      "        -5.7508e+02, -2.4481e+00, -3.7358e+01,  7.1170e+00, -3.6965e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28322.29296875\n",
      "Epoch training loss: 40660.194010416664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch validation loss: tensor([[33531.6133]])\n",
      "\n",
      "On epoch: 20\n",
      "Outputs tensor([-132.4758,  -15.3509,  -19.9732, -141.0812, -113.7254,   -8.6587,\n",
      "           6.9114,   -6.7809, -249.1775,    0.7506,  -10.1112,    3.0755,\n",
      "          -2.7273,    8.2200, -297.0318,    1.8486, -471.6083,    9.8026,\n",
      "        -332.3778,   -4.5563], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39237.1328125\n",
      "Outputs tensor([   9.3462,  -18.2644,    4.2809,   -1.7962,    9.0954, -282.5922,\n",
      "         -43.2260,  -41.3810, -624.2283,   -6.1958, -579.5392, -308.1072,\n",
      "           7.2888,    9.1180, -138.9757,    1.7687,   -7.1262,   -2.2056,\n",
      "        -601.1363,    1.6655], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23497.462890625\n",
      "Outputs tensor([-8.9108e+01,  6.2961e+00, -5.2985e+02, -6.5224e+02, -6.9807e+02,\n",
      "        -2.4887e+01, -9.8132e+00, -3.4755e+02,  2.2935e-01,  1.1814e+01,\n",
      "        -5.2180e+02, -1.0331e+02,  2.4593e+00,  3.3624e+00, -2.1365e+02,\n",
      "         2.0643e+00,  8.6939e-01,  7.7096e+00,  5.8175e+00,  5.9009e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23386.654296875\n",
      "Outputs tensor([-7.5685e+00, -3.9063e-01,  8.5873e+00, -7.0649e+01, -3.7076e+02,\n",
      "        -4.4042e+02,  2.3463e-01,  2.3819e+00, -2.9683e+02, -6.4617e+01,\n",
      "        -4.9123e+02,  4.4759e+00, -9.1916e+00,  2.5068e-01, -5.3226e+02,\n",
      "         1.1946e+00,  5.7055e+00,  5.0179e+00, -2.0727e+02,  4.0476e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 73729.2578125\n",
      "Outputs tensor([   8.8672,    6.3998,   -4.9377, -392.0232, -263.1783,   12.5420,\n",
      "           8.5250,    4.7188,    8.1547,    5.8011, -406.3095,   11.6862,\n",
      "           4.6071,   -6.6188,   11.5451,   13.6800,    6.7040,    6.4611,\n",
      "         -50.9321, -384.5634], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37246.89453125\n",
      "Outputs tensor([  12.4088,    7.3636, -253.8702,    2.6711, -391.8612,  -33.2818,\n",
      "        -381.1976,    4.3756,   16.2866,   -4.3436,  -93.6280,  -15.8765,\n",
      "        -358.9476,   14.1270, -442.9227, -112.9569,    7.1975,   11.0238,\n",
      "          -2.1594, -200.3721], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35583.1875\n",
      "Epoch training loss: 38780.098307291664\n",
      "Epoch validation loss: tensor([[38583.5781]])\n",
      "\n",
      "On epoch: 21\n",
      "Outputs tensor([-6.2194e+02,  5.8517e+00,  3.0673e+00, -6.5709e+02, -4.6508e+02,\n",
      "         2.1731e+00,  4.4130e+00, -2.3818e+02, -5.3861e+02,  1.7712e+00,\n",
      "        -2.2265e+02,  3.2474e-01, -2.0156e+01,  4.5760e+00, -1.8375e+00,\n",
      "         7.9312e+00, -4.5255e+02,  9.5770e+00, -1.0614e+00, -3.0693e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 20888.515625\n",
      "Outputs tensor([-368.7838,    7.3096,  -62.2056,    2.4095,  -22.9265,    4.6202,\n",
      "           1.7531, -437.0054, -356.9520, -256.9753,  -16.5676,    4.6818,\n",
      "         -23.1262, -498.2403, -128.3958,   -6.5012, -638.1566,  -47.1489,\n",
      "        -425.7024,  -33.1811], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27055.0625\n",
      "Outputs tensor([-403.7534,    3.7898,  -31.5269, -479.0388, -187.7281,    5.9719,\n",
      "        -747.1918, -279.4768, -488.1696,    3.2122,   -7.1841,  -28.9843,\n",
      "         -13.8456,  -40.2108,  -20.3925,  -91.4249,   11.4040,  -51.2237,\n",
      "           4.4007,    4.2540], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25269.193359375\n",
      "Outputs tensor([ -37.2324, -497.3671, -418.8926,   -6.8122,    4.0296, -438.4480,\n",
      "         -13.6976,   -5.9165, -226.1900,   -5.6358, -563.6232,   -7.4634,\n",
      "          10.6785,    6.6070,  -12.0240, -484.9955,   -4.4394,    7.0784,\n",
      "          -9.2212,    5.6799], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32386.40625\n",
      "Outputs tensor([ -10.0625,  -94.9884,    1.9352, -138.8829,    4.7959, -119.2306,\n",
      "           0.7936, -503.7767,   -8.1170, -155.8279, -205.2280,    7.8956,\n",
      "        -509.5728,    5.0288,  -21.0975, -289.3379,   -6.6326,    3.7305,\n",
      "        -115.2091,   -9.1629], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45296.078125\n",
      "Outputs tensor([  -6.2807,    3.3045,  -48.0853,    0.7730,    1.1385, -525.9648,\n",
      "           7.8736,  -20.0195,  -83.7505,   -5.1183,    7.0300, -248.4161,\n",
      "          -5.0794, -581.8931,  -97.1494,  -23.5243, -156.0510, -243.6941,\n",
      "           7.4020,    5.6039], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 54809.0234375\n",
      "Epoch training loss: 34284.046549479164\n",
      "Epoch validation loss: tensor([[34240.5391]])\n",
      "\n",
      "On epoch: 22\n",
      "Outputs tensor([  16.9809,   14.5901, -422.9784,   12.2334,   12.5549,  -11.0421,\n",
      "        -377.9200,    9.5900, -294.0647, -389.8868,    4.6907, -470.9631,\n",
      "        -212.8129,    6.8214,    4.8381,    6.2994,    4.9959,   13.2488,\n",
      "           0.5219,    5.1086], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27507.921875\n",
      "Outputs tensor([   9.0626,    3.3988, -438.4398, -129.5498,    7.9839,    9.4981,\n",
      "         -49.7171, -452.5573,    0.8979, -223.4918,    1.8953,  -61.8369,\n",
      "           4.9649, -109.3001, -487.8296, -472.0880,    4.6642,    1.9402,\n",
      "          11.3924,  -12.6660], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44128.3828125\n",
      "Outputs tensor([-659.8773,  -58.2647,  -38.7894,  -53.2633, -112.8837,  -25.7183,\n",
      "        -126.3860, -129.7665,   -4.7100, -588.9606,  -44.5905, -262.5097,\n",
      "         -30.6048,  -88.2027, -654.4462,   -2.0587,  -32.8719, -430.4067,\n",
      "        -658.7521,   -6.5659], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34326.5703125\n",
      "Outputs tensor([ -34.1430,  -23.3110,   -4.5113,    2.1271, -211.3855, -569.9199,\n",
      "        -673.8682,  -75.3538,  -27.7386, -102.3780, -293.7134, -575.9515,\n",
      "           5.8672,   -0.8978,    7.2561,    1.0746,    5.7130, -160.6622,\n",
      "         -15.1638,   -8.5198], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47722.3828125\n",
      "Outputs tensor([  11.7119,  -35.2479, -233.9339, -515.1381,    1.0390,   -3.4205,\n",
      "           4.3217,   -3.9971,  -50.2975, -180.1504, -332.0274,  -52.8043,\n",
      "         -83.5568,    9.6622,    2.2755, -324.5482,   -3.7398,    6.6741,\n",
      "           4.2795,   -4.1911], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41474.71484375\n",
      "Outputs tensor([ 2.7227e-01, -1.4037e+01,  2.2192e+00, -1.6231e+00, -5.5986e+01,\n",
      "        -1.1933e+01, -1.9627e+01,  7.2480e+00,  2.0441e+00, -3.7854e+02,\n",
      "        -1.7390e+01,  5.4688e+00, -2.2761e+02, -3.3379e+01, -1.9352e+01,\n",
      "         3.3432e+00, -5.9407e+02, -4.6680e+02, -1.9015e+01, -1.3788e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51530.74609375\n",
      "Epoch training loss: 41115.119791666664\n",
      "Epoch validation loss: tensor([[40058.1094]])\n",
      "\n",
      "On epoch: 23\n",
      "Outputs tensor([  -0.5119,  -14.5099,    4.8752, -149.7303,    2.9187,    9.2436,\n",
      "           7.7417,    5.0511, -491.8307,    9.2799,    4.5327,    8.2067,\n",
      "          -5.4946,  -84.1902,   -3.1507,    9.2027, -289.9278,    0.5068,\n",
      "         -66.6282, -216.6871], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33840.3984375\n",
      "Outputs tensor([ -16.0967, -496.4196,    1.2973,    5.5231,    7.7310,   -8.0291,\n",
      "          11.7409,  -12.9043, -130.6751, -284.3922,    8.3318,    6.4051,\n",
      "           2.1038,   -4.1346, -373.1237, -369.4139, -471.3300,   -0.5389,\n",
      "          -9.4403,    2.2873], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41370.8671875\n",
      "Outputs tensor([ 1.2896e-01, -5.0590e+02, -5.9407e+01, -6.2379e+02, -1.7977e+02,\n",
      "        -1.7934e+01, -2.2130e+01, -1.4067e+01,  2.3127e+00,  1.5743e+00,\n",
      "         1.7684e+00, -5.1553e+00, -3.4382e+01, -3.9459e+02,  7.0080e-01,\n",
      "        -4.2797e+02, -7.7413e+00, -1.3942e+00, -2.2192e+01, -2.6656e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46825.00390625\n",
      "Outputs tensor([   2.7276,    2.3539,    4.0354,    4.3633,  -53.4949,    3.6602,\n",
      "         -76.2363,    9.5450, -255.1675,    7.8975,   11.0336, -397.1385,\n",
      "        -469.3601,   10.7647,    9.8400,   15.2065, -410.6702, -376.4654,\n",
      "           4.5204,    4.9211], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39736.6171875\n",
      "Outputs tensor([-3.8208e+02, -5.2934e+02, -1.8722e+00, -5.5360e+01, -2.2451e+01,\n",
      "        -3.3041e+00, -2.1566e-01, -4.7610e+01, -9.6829e+01,  4.0526e+00,\n",
      "        -7.7170e+00,  8.7241e+00, -3.2015e+02, -4.4616e+02, -1.2168e-02,\n",
      "         3.7753e+00,  5.1699e+00, -4.6637e+02,  1.3271e+00, -2.2755e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39245.6796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([-3.3423e+01, -1.2163e+01, -6.3472e+02, -5.3451e+01, -8.2528e+01,\n",
      "        -6.3969e+02, -3.3509e+00,  3.3471e+00, -1.6239e+00, -2.3019e+01,\n",
      "        -5.9231e+02, -1.8578e+01, -4.5578e+01, -2.6975e+01,  6.2218e-01,\n",
      "        -1.0243e+02, -1.3007e+01, -3.3100e+02,  5.7499e+00, -3.3392e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32846.41796875\n",
      "Epoch training loss: 38977.497395833336\n",
      "Epoch validation loss: tensor([[37489.2305]])\n",
      "\n",
      "On epoch: 24\n",
      "Outputs tensor([-5.0825e+02, -3.4058e+02,  4.1652e+00,  1.5300e+00,  5.8894e+00,\n",
      "        -1.1951e+02,  5.3864e+00, -3.8205e+02,  4.4522e+00,  8.6269e+00,\n",
      "         1.0876e+01,  1.7246e-01,  3.2654e+00, -8.9611e+01, -3.2035e+02,\n",
      "         1.2216e+01,  4.9980e+00, -3.3784e+02, -3.6868e+02,  1.8578e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33244.03125\n",
      "Outputs tensor([-4.7862e+02,  6.5385e+00,  9.4457e-01, -9.7617e+00, -5.8819e+00,\n",
      "         6.1492e+00,  2.5903e-02, -3.8819e+02, -2.2101e+01,  1.2316e+01,\n",
      "        -5.3850e+02, -3.8699e+02,  8.7218e+00, -5.5774e+01, -2.1953e+02,\n",
      "        -1.8971e+02,  7.0408e-01, -4.0160e+02,  9.2199e+00, -2.4806e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24954.25390625\n",
      "Outputs tensor([-4.9685e+02, -5.2672e+02, -1.4859e+02, -1.4880e+01, -6.3126e-01,\n",
      "         1.0302e+00, -6.7710e+01, -9.9505e+01, -1.0111e+02, -7.5791e+01,\n",
      "         1.5587e+00, -1.6345e+02,  5.8530e-01, -1.6248e+01, -5.3685e+02,\n",
      "        -3.2159e+01, -2.0338e+02, -5.5551e+02, -2.7191e+01, -5.5078e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42837.6953125\n",
      "Outputs tensor([ 4.4464e+00,  1.1484e+01,  2.9677e-01, -2.6939e-01, -5.3331e+02,\n",
      "         8.9084e+00,  6.6554e+00, -4.4372e+02,  7.2800e+00, -1.0238e+02,\n",
      "        -4.0862e+02, -3.9206e-01,  4.6552e+00,  7.0852e+00,  9.2185e+00,\n",
      "        -4.0570e+02,  3.2782e+00,  9.7614e+00, -6.0353e+02, -2.8662e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45876.55078125\n",
      "Outputs tensor([  -5.3075, -211.3471,    7.0443,  -10.9366,   -8.2465,  -13.9915,\n",
      "           8.6724, -505.9764, -480.9851,  -24.8356,    9.7395,    0.9825,\n",
      "        -577.9978,    5.8273,  -65.7937,  -27.3836,  -22.6812, -251.2675,\n",
      "        -100.2733, -464.4688], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44032.6953125\n",
      "Outputs tensor([-9.7842e-01,  6.8411e-01,  8.0456e-02, -3.2233e+01, -5.4255e+02,\n",
      "         1.2729e+01, -1.4425e+02, -1.8297e+01,  1.6957e+01,  3.1158e+00,\n",
      "        -4.7845e+02,  1.0311e+01, -1.5951e+02, -1.7548e+02, -5.0409e+01,\n",
      "         9.1242e-01,  1.1890e+01,  7.1553e+00,  5.7001e-01, -2.3552e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43484.5859375\n",
      "Epoch training loss: 39071.635416666664\n",
      "Epoch validation loss: tensor([[42114.7930]])\n",
      "\n",
      "On epoch: 25\n",
      "Outputs tensor([   5.5059, -284.4395, -437.3253,   11.4167,    8.2024,   20.9558,\n",
      "        -444.4259, -371.9254, -126.0478,   11.9766,    8.0365,   13.6620,\n",
      "          12.6940,   14.1688,  -15.3913,    5.4966,    2.9052,  -27.2762,\n",
      "        -160.0662,    4.6609], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41825.8984375\n",
      "Outputs tensor([-1.2742e+01,  2.7320e+00,  1.0909e+00, -2.2717e+00, -7.2753e+00,\n",
      "         7.1056e+00,  1.0013e+01, -2.4474e-01, -6.3136e+02, -4.6745e-01,\n",
      "        -2.9880e+02, -1.6847e+01,  3.7811e+00,  1.4717e+01,  5.5152e+00,\n",
      "         7.5620e-01,  2.9259e+00, -6.8933e+02,  5.6226e-01, -1.5004e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44636.19921875\n",
      "Outputs tensor([   4.8119,    8.5333,    9.3582,   -0.7574,  -10.8950,    8.0323,\n",
      "        -374.4671,    7.1017,    5.3483,   -8.7033, -508.6107,    6.9855,\n",
      "         -70.3299,    3.6916,    3.5188,   -6.1172,  -43.0293,    9.6849,\n",
      "          13.1047, -187.6118], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 19177.814453125\n",
      "Outputs tensor([  -1.8674,    7.3088,   13.2051,   17.6943,   11.2506, -104.1373,\n",
      "        -478.9502, -337.5049,    5.3314,   18.1196,    8.0691, -301.9800,\n",
      "           9.1096,   12.0183, -476.6857, -470.5428, -203.9516,    8.2085,\n",
      "        -298.5899,   18.3690], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29493.47265625\n",
      "Outputs tensor([-1.9466e+01,  8.6284e+00, -5.4529e+02, -5.9076e+02,  6.0665e+00,\n",
      "        -6.2952e+01,  7.4301e+00,  2.5351e-01,  1.1304e+00, -8.5864e+00,\n",
      "        -3.8806e+02,  1.0715e+01,  7.9704e-01, -5.7895e+02, -3.0167e+02,\n",
      "        -1.2358e+02,  3.8489e+00, -1.8994e+00, -1.2414e+02, -4.9722e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38712.66796875\n",
      "Outputs tensor([-270.7128, -193.6972,    3.2187, -180.1774,    8.5041, -277.1492,\n",
      "          -0.8015, -561.4748,  -13.2616,   -3.9336, -226.2303, -500.3284,\n",
      "          10.9898,    4.9098,    1.3508,    4.2752,   11.3804,  -82.6030,\n",
      "          -1.6112,    6.5112], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 54126.6796875\n",
      "Epoch training loss: 37995.455403645836\n",
      "Epoch validation loss: tensor([[33867.6445]])\n",
      "\n",
      "On epoch: 26\n",
      "Outputs tensor([-2.7394e+02, -1.2942e+02, -1.6343e+02, -8.9484e+00, -1.1585e+02,\n",
      "        -4.2956e+02,  9.1918e+00, -3.3342e-01, -5.6172e+00, -6.5773e+01,\n",
      "        -1.0083e+02,  2.9690e+00,  6.8205e+00, -4.1854e+02, -5.0447e+02,\n",
      "        -8.0293e-01, -1.4879e+01, -4.4941e+02,  8.4747e+00, -3.7636e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50435.328125\n",
      "Outputs tensor([ 2.1435e-01, -1.2060e+02, -6.1842e-01, -2.3755e+01,  7.0748e-01,\n",
      "         8.4927e+00,  7.2057e+00, -4.0304e+00,  2.5413e+00, -5.1035e+02,\n",
      "        -5.5002e+02, -2.0790e+02,  1.1046e+01,  6.4845e+00,  1.1754e+01,\n",
      "        -9.9974e+01,  8.0881e+00, -6.0150e+00, -2.7603e+01, -1.7011e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34244.7734375\n",
      "Outputs tensor([  20.9619, -493.5978,   20.4141, -371.2029, -229.4528,    6.9833,\n",
      "          16.3062,    3.9949, -404.7208,    5.5994,   12.2094,   13.4989,\n",
      "        -402.3927,    5.5454, -329.9792,    1.5299,   10.3089, -262.7805,\n",
      "        -383.1663,    2.0994], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50944.7109375\n",
      "Outputs tensor([-1.7230e+00, -7.5803e+02,  3.4819e+00, -4.9470e+02, -3.5677e+02,\n",
      "        -6.1432e+02, -1.3674e+00, -1.4495e+01, -1.6846e+01,  4.7146e-01,\n",
      "        -8.7537e+01, -3.1106e+00,  5.4154e+00, -5.9416e+00, -5.6283e+02,\n",
      "        -3.1253e+02,  4.7998e-01,  9.8962e-01, -5.6957e+01, -1.6093e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23846.31640625\n",
      "Outputs tensor([ 3.9052e+00, -2.4239e+01, -6.3708e+02, -1.3341e+01,  1.6628e-01,\n",
      "        -4.7962e+01, -4.9391e+02, -1.7063e+01,  6.4092e+00, -3.5450e+02,\n",
      "        -2.9680e+01, -7.3191e+01,  4.5190e+00, -1.7451e+00, -1.7074e+01,\n",
      "        -2.8409e+01, -5.1684e+01, -3.7929e+01, -1.2312e+00, -6.2978e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 17370.19921875\n",
      "Outputs tensor([-3.4330e+00, -2.8905e+00, -1.2024e+01, -4.8914e+02, -3.0011e+02,\n",
      "         2.5768e+00, -1.5731e+01,  1.4306e+00, -7.9056e-01,  1.2166e+00,\n",
      "        -4.9535e+02,  6.4641e+00,  7.9768e+00, -4.3491e+02, -1.2126e+02,\n",
      "         1.0724e+01,  2.1432e-01, -2.1562e+02,  6.7523e+00, -5.1246e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44641.41796875\n",
      "Epoch training loss: 36913.791015625\n",
      "Epoch validation loss: tensor([[32157.0586]])\n",
      "\n",
      "On epoch: 27\n",
      "Outputs tensor([-6.3590e+02, -7.1112e+00, -5.4038e+02,  7.7491e+00, -3.9181e+01,\n",
      "        -2.7119e+00, -3.7337e+02, -4.5545e+01, -7.7605e+00,  6.5548e+00,\n",
      "        -4.9442e+00,  8.9234e+00, -1.3081e+02,  4.7718e-01,  1.4776e+00,\n",
      "        -6.1319e+02,  4.1616e+00, -3.0568e+02, -1.5215e+00,  1.1851e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 22609.373046875\n",
      "Outputs tensor([-6.0775e+00, -2.4333e+02, -1.4913e-01, -3.0425e+00,  4.0735e+00,\n",
      "        -3.4271e+00, -9.3090e+00, -5.6495e+00, -6.5033e+02, -3.1621e+01,\n",
      "        -5.7535e+01, -5.3467e+02, -7.1792e+02, -3.2329e+00, -4.8185e+02,\n",
      "         3.6265e+00, -4.3440e+01, -2.1782e+01,  1.7284e+00, -1.7055e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34969.25390625\n",
      "Outputs tensor([ 1.2210e+01, -3.4961e+02, -2.7173e+01, -1.2265e+02, -3.8801e+02,\n",
      "         3.2521e+00, -1.2549e+01, -5.9833e+00, -3.1836e-01,  1.0810e+01,\n",
      "        -4.4332e+01, -6.4537e+01, -3.3145e+02, -5.3187e+02,  1.3188e+00,\n",
      "        -2.9036e+01, -3.1387e-02,  5.6126e+00,  6.9686e-01,  6.8332e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36005.4921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([-2.0889e+01,  4.9077e+00, -4.0267e+02,  1.5803e+01, -3.9316e+02,\n",
      "        -3.4811e+02,  5.7560e+00, -2.3230e+02, -9.4382e+00, -1.0282e+00,\n",
      "        -2.8403e+02, -2.2688e+02, -4.5815e+01,  3.9780e-01,  1.5677e+00,\n",
      "        -4.6182e+02,  1.9966e+00,  6.0328e-01,  1.2643e+01,  2.7936e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39225.859375\n",
      "Outputs tensor([  -3.5334, -111.3977,    5.3665,  -11.7178, -246.1847,  -26.7740,\n",
      "        -463.9859, -547.9005,  -45.8357, -482.6193, -205.4957,  -77.0948,\n",
      "          -3.8010, -108.7791,  -23.2630,    1.1567,   -3.5603,  -35.0328,\n",
      "        -638.1511,   -5.1183], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42382.0390625\n",
      "Outputs tensor([  -3.7489,   -1.6580,   -6.2024, -244.5015,  -19.8997,   -1.9238,\n",
      "          -0.6945,   -1.6569,    4.2180,   -5.6656,   -2.7180,   -2.6084,\n",
      "        -410.8540,    1.4676, -469.0757, -112.7637, -124.7966, -423.6358,\n",
      "           4.9851, -163.5321], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49164.7734375\n",
      "Epoch training loss: 37392.798502604164\n",
      "Epoch validation loss: tensor([[32934.1836]])\n",
      "\n",
      "On epoch: 28\n",
      "Outputs tensor([-5.3650e+02, -9.5939e+01, -2.7895e+01,  8.3157e+00, -4.7036e-01,\n",
      "        -5.0669e+02, -2.1176e+00,  3.8300e+00, -1.4100e+00,  4.5141e+00,\n",
      "        -3.0218e+01, -5.4888e+02,  1.6705e+00, -3.1707e+00, -1.0175e+02,\n",
      "        -3.1684e+00, -8.9064e+01,  1.9452e+00,  1.1055e+01, -5.6470e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31780.900390625\n",
      "Outputs tensor([-356.7040,    0.6236,  -73.2103, -197.8542,    4.0908,   -0.4803,\n",
      "          -8.5300,  -54.5620, -398.5072,  -23.9135, -362.5417,   -2.3876,\n",
      "          -5.8989, -384.5191,  -63.0885,   -5.3858, -468.6154, -281.5645,\n",
      "           3.0536,  -27.9212], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36322.3515625\n",
      "Outputs tensor([   1.4141, -380.8026, -407.5406, -471.3494,    1.3137,  -34.6858,\n",
      "         -97.6922,    1.1785,    3.7337,    4.9406,    1.5102, -341.6263,\n",
      "           1.7780,  -63.9737, -457.4838, -448.3336, -446.6546,    4.1151,\n",
      "           6.7848,    6.4264], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 22613.037109375\n",
      "Outputs tensor([-504.8244,    3.4577,   -3.6596,   -3.4999,  -24.5097,   -1.5328,\n",
      "        -143.7971,   -2.7561, -543.8296,   -1.9494,  -30.3849,   10.8796,\n",
      "         -25.6406,  -29.8981,   -1.2826, -633.1579, -104.6768, -612.4211,\n",
      "          -6.0701,    2.9685], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 55032.1796875\n",
      "Outputs tensor([-279.0908,    4.9263,   -3.2143, -224.9362,    2.6590,    0.9965,\n",
      "           1.1227,  -10.0009,    4.3699, -201.9712, -325.5593,   -2.9670,\n",
      "          -2.0429,   11.1330, -507.1940,    5.3641,   15.4766,   -2.1517,\n",
      "           5.6398,  -10.4977], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31059.427734375\n",
      "Outputs tensor([ 2.3719e-01, -5.0059e+02,  1.7015e+00, -9.9777e-01, -1.9987e+00,\n",
      "        -5.3643e+00,  6.4263e+00, -1.0152e+01, -5.0228e+02,  7.5509e+00,\n",
      "        -2.2175e+00,  8.0907e+00, -5.3418e+02, -3.9154e+02,  1.0086e+00,\n",
      "         5.7675e+00,  4.3313e+00,  1.3817e+01, -5.3424e+02, -1.3990e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25660.74609375\n",
      "Epoch training loss: 33744.773763020836\n",
      "Epoch validation loss: tensor([[34196.8633]])\n",
      "\n",
      "On epoch: 29\n",
      "Outputs tensor([-2.0909e+00, -2.5521e+02, -4.5038e+02,  1.5488e+01, -1.0846e+01,\n",
      "        -6.2703e+00,  1.0876e+01, -2.9898e+02,  7.9867e+00, -5.2869e+02,\n",
      "         6.9849e+00,  2.9537e+00,  4.8412e-01, -3.7491e+02,  8.7110e+00,\n",
      "        -5.8646e-02, -9.1226e-01, -2.1647e+00,  2.7795e+00, -2.3180e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30566.525390625\n",
      "Outputs tensor([ 1.5282e+00, -6.4221e+02,  8.2829e+00,  8.5301e+00, -5.6880e+02,\n",
      "         7.5107e+00,  2.2088e-01,  2.9282e-01,  7.7861e+00, -7.4391e+00,\n",
      "        -7.7875e+01, -1.3491e+02, -7.3276e-02, -1.6101e+00,  2.6777e+00,\n",
      "        -6.3047e+02, -6.7428e-01, -1.7119e+02,  1.0832e+01,  7.4226e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43775.28515625\n",
      "Outputs tensor([  -4.4455,   16.9404, -152.7174,    6.1417,   16.6359,   14.3531,\n",
      "          19.8213,   15.6832, -306.4863,   18.3880, -316.5330,   10.3288,\n",
      "           5.0734, -146.0156,  -12.1176,   10.0736,  -23.1194,    4.3622,\n",
      "          17.2183, -398.6212], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 57381.5625\n",
      "Outputs tensor([-299.3404,   -0.6825,    5.0896,    1.4562,    8.1439, -409.0708,\n",
      "        -361.9552,  -12.2131, -105.5027,    0.4645,   10.5594,   17.5628,\n",
      "           7.0776,    9.9741,    1.7368, -210.0096, -443.5807,   -1.1952,\n",
      "           1.6942, -296.0074], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37459.9765625\n",
      "Outputs tensor([-458.9924,    1.7326,  -15.2405,  -35.1226,  -65.3611,    1.0646,\n",
      "        -540.4258, -118.4608, -540.6115,  -59.7044,   -7.7211,   -4.0339,\n",
      "         -55.5267, -326.3841,  -81.7347,    4.3984,  -15.9782,  -69.8851,\n",
      "           2.3785, -310.6483], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34595.7421875\n",
      "Outputs tensor([-6.1682e+01, -1.1333e+01, -9.3956e+00, -2.7544e+02, -3.7350e+02,\n",
      "        -3.6224e+01,  7.1184e-01, -5.2334e+01, -2.6841e+01,  1.9372e+00,\n",
      "        -1.1637e+01, -2.9224e+01,  8.6781e+00, -2.1701e+02,  3.0019e-01,\n",
      "        -1.6683e+01, -5.4347e+02, -6.4687e+02, -1.3117e+02, -6.8409e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35803.26953125\n",
      "Epoch training loss: 39930.3935546875\n",
      "Epoch validation loss: tensor([[31284.9414]])\n",
      "\n",
      "On epoch: 30\n",
      "Outputs tensor([   2.6423,    6.9502,   -9.8603,    2.4461,  -10.9019,   -9.9624,\n",
      "           8.2163, -437.3224,  -14.9739, -166.6506, -397.1134,  -64.6267,\n",
      "        -436.5443, -450.6723,    0.8259, -265.5803,    7.1035,  -33.5166,\n",
      "        -548.2822,    7.3214], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39784.62890625\n",
      "Outputs tensor([   0.6717, -156.9488,    3.8539, -199.0111,    1.9326,   -0.9078,\n",
      "        -436.3500, -257.7198,    2.0687,    4.8190, -321.3448, -478.9243,\n",
      "          -6.1090,  -19.5781, -534.9110,  -11.2828,    2.4592, -142.5776,\n",
      "          -1.2370,  -44.6118], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33671.140625\n",
      "Outputs tensor([-4.3185e+02, -3.3139e+00, -2.8288e+00,  3.4904e-01,  6.5366e+00,\n",
      "         1.1356e+01,  3.7890e+00,  1.1889e+01,  1.1515e+01,  2.4014e+00,\n",
      "        -4.6403e+02, -5.0233e+02,  7.7070e+00, -5.6086e+02,  4.3080e+00,\n",
      "         2.0427e+00, -7.0994e-01, -4.4701e+02, -2.6352e+02,  4.2651e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35506.91015625\n",
      "Outputs tensor([-3.4853e-01, -9.6759e+00, -5.0422e+02, -4.9577e+02, -1.8588e+01,\n",
      "        -4.9695e+02,  3.4663e+00, -3.4897e-01, -3.2018e+02, -4.6620e+01,\n",
      "        -4.4066e+02, -5.9719e+02, -3.1315e-01,  3.2214e+00,  5.7136e+00,\n",
      "         4.4957e+00,  1.2174e+01, -2.2758e+00,  6.6420e+00, -4.6496e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26597.130859375\n",
      "Outputs tensor([ 3.2684e+00, -3.3430e+01, -9.4623e+01, -4.3550e+02,  7.8416e+00,\n",
      "        -4.2557e+01, -4.2618e+00,  6.3134e+00, -3.7213e+02, -2.0396e+02,\n",
      "         1.0982e+00, -9.5506e+00, -5.6603e+02,  4.5912e+00, -5.9749e+02,\n",
      "        -1.6461e+00, -1.7105e+02, -5.6256e-01, -4.0799e-01, -2.1066e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27469.208984375\n",
      "Outputs tensor([-2.2340e+02, -5.8605e-01,  2.5341e+00, -3.0177e+01, -2.7838e+00,\n",
      "        -2.1078e+00, -5.2451e+02, -1.5332e+02,  3.2808e-01, -3.8365e+00,\n",
      "        -4.7845e+02, -6.8452e+01,  1.1330e+01, -5.5450e+02,  5.6199e+00,\n",
      "         6.0660e+00,  7.7453e-01, -6.8470e+01, -1.5134e+02, -9.9691e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 20733.431640625\n",
      "Epoch training loss: 30627.0751953125\n",
      "Epoch validation loss: tensor([[36154.1055]])\n",
      "\n",
      "On epoch: 31\n",
      "Outputs tensor([-110.2712, -549.9214,    8.9634,    1.5668, -374.8204, -478.4444,\n",
      "        -490.9391,    9.2965,    7.9811,   -0.7572,   -3.2661,   -8.8209,\n",
      "          -1.6484,    3.9198,  -28.0099,    6.1530,  -10.0771,    3.3497,\n",
      "           9.4308,   17.0020], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31501.89453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ 1.2692e+01, -3.9060e+02, -1.4398e+00, -4.3443e+02,  1.1310e+01,\n",
      "         9.4102e+00,  7.2506e-01, -2.5706e+02, -4.9749e+02, -1.4279e+02,\n",
      "        -1.5112e+02,  1.2798e+01,  2.8354e+00,  8.3876e+00, -2.2046e+00,\n",
      "        -4.9629e+02,  5.4524e+00, -1.8283e-01, -9.3854e-01, -6.3979e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30803.740234375\n",
      "Outputs tensor([   8.0819, -485.2535,   -3.4225, -514.5889, -490.9704,    4.6528,\n",
      "           8.9331,  -38.0111,  -58.3991,   -0.5925,    5.9407,   -1.0522,\n",
      "        -289.7975,   -4.1933,  -28.0839, -328.6289,   -3.6529,  -51.7254,\n",
      "           4.4568,  -18.8717], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23905.91796875\n",
      "Outputs tensor([  -8.7738,    6.4705,  -27.3878,    1.5461,   -8.2528,    0.6608,\n",
      "         -44.7111, -246.7768, -414.5042,    6.7356, -467.5333,    9.8233,\n",
      "           2.5042,   -8.5716, -322.4506,  -87.6786, -573.5363,   -9.0333,\n",
      "          -7.3207,   -3.3003], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 52320.015625\n",
      "Outputs tensor([   9.4925, -161.9016,   10.9493,   -4.7341,    8.3116,    3.3233,\n",
      "          -3.3582,    9.0551, -233.5988, -256.8713,    1.0016,   11.7210,\n",
      "           6.9561, -586.5213, -338.3978, -152.0515,   -4.1153, -510.7980,\n",
      "           4.2700,   -3.9888], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37091.8515625\n",
      "Outputs tensor([ -10.0295,  -17.0242,    3.0445,   -2.5271,    2.3626,    5.7759,\n",
      "           9.6360,   -2.5494, -573.3853,    2.9227, -499.4063, -237.1304,\n",
      "        -783.5426,  -18.9469,   -1.2466, -107.6364, -455.8924,   -8.0616,\n",
      "        -669.1208,    2.7860], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43985.1328125\n",
      "Epoch training loss: 36601.425455729164\n",
      "Epoch validation loss: tensor([[39449.1797]])\n",
      "\n",
      "On epoch: 32\n",
      "Outputs tensor([-7.7581e-02, -3.4350e+02,  4.0718e+00,  1.0979e+01, -1.7586e+00,\n",
      "         3.4596e+00,  4.7405e+00, -3.8703e+02,  5.6803e+00,  1.3056e+01,\n",
      "         1.5932e+01, -3.7834e+02,  9.1511e+00,  1.7464e+01,  1.2991e+01,\n",
      "        -4.0337e+02,  3.7403e-01, -4.7878e+02, -1.6681e+02, -6.8511e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27534.17578125\n",
      "Outputs tensor([-134.0151, -573.6929, -546.0101,    8.4351,  -47.1546,  -20.4267,\n",
      "           3.2877,   -3.8576, -419.7735,   10.0846,   10.0400, -380.3574,\n",
      "           2.2248, -378.5534, -631.0596,    2.5865,    1.8139,   -3.2265,\n",
      "           5.0177,    7.3906], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41415.05078125\n",
      "Outputs tensor([-6.3883e+01, -4.9934e+00, -7.1895e+02, -2.0156e+00,  7.4849e+00,\n",
      "        -2.2607e+02, -2.3039e+01,  6.5626e-01, -2.0416e+00, -2.0912e+01,\n",
      "        -1.2369e+02, -2.6505e+00, -3.2312e+00, -6.0448e+00, -4.5342e+01,\n",
      "        -6.7180e+02, -1.6540e+02, -1.4906e+01, -1.4431e+00, -5.5477e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31494.53515625\n",
      "Outputs tensor([  -2.3068,   -9.5139,   -0.8692,    1.8077,   -0.8049,   14.6500,\n",
      "           9.2596, -180.5721, -249.6200,  -91.1695, -467.4738,   11.8180,\n",
      "          13.0403,   -7.7612, -113.3011, -120.3824, -571.4091,   10.8039,\n",
      "           8.3516,   -0.6122], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 22026.037109375\n",
      "Outputs tensor([ 1.1471e+01, -1.0540e+01, -2.8493e+02, -4.2942e+02,  3.4204e+00,\n",
      "        -3.0317e+02, -1.4489e+01, -3.2633e+01, -3.6805e+02, -5.3160e+02,\n",
      "         5.6133e+00, -2.3333e+00,  3.7641e+00, -1.2165e-01, -3.2789e+00,\n",
      "        -8.6705e+00, -2.9419e+02, -3.7772e+00, -1.7473e-01,  1.9875e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42991.63671875\n",
      "Outputs tensor([-1.2301e+02,  9.9779e+00, -1.7065e+01, -3.7074e+02, -2.9973e+00,\n",
      "        -2.3144e+02, -4.4120e+02, -7.3310e-01, -5.4435e+02,  6.3260e-01,\n",
      "        -1.8234e+01, -3.6997e+00, -5.3096e+02, -2.5985e+00, -1.3006e+01,\n",
      "        -1.4758e+02, -3.2911e-01, -2.5856e+01,  1.2847e+01,  6.3764e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39432.50390625\n",
      "Epoch training loss: 34148.989908854164\n",
      "Epoch validation loss: tensor([[36240.0859]])\n",
      "\n",
      "On epoch: 33\n",
      "Outputs tensor([-349.9557,   11.3652, -512.4492, -340.6331,    9.0652,   -2.9596,\n",
      "        -280.3517,   -0.7816, -473.4897,   15.3119,   13.6605, -117.2447,\n",
      "          -0.8459, -206.6299, -239.2449,   19.7534,  -22.8384, -467.3091,\n",
      "           5.9782,   -6.1031], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29038.64453125\n",
      "Outputs tensor([   3.1735,  -39.3981, -265.2130,  -10.7173,   -3.9730, -178.1469,\n",
      "         -10.7386,  -74.9637,   -1.6937, -517.6962,  -43.1660, -111.8531,\n",
      "        -282.0594, -245.3105, -397.5027, -559.8472,  -60.2164,  -90.4359,\n",
      "        -835.0447,  -15.6546], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 101258.0078125\n",
      "Outputs tensor([  18.1608, -374.7338,    6.5067,    5.0651, -376.1440,    8.0519,\n",
      "          12.0172,    4.5315,   13.1447, -116.5676, -388.1770,   15.4068,\n",
      "          15.2129,   17.7457,    8.0015,   16.1023,    3.3728,    9.5879,\n",
      "           8.8032,    2.3510], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40186.55859375\n",
      "Outputs tensor([   4.8880,   -1.0262,   17.0191,   21.3641, -360.6392,   17.8079,\n",
      "           4.0641,   15.9735,   20.3670,    6.8666, -301.2272, -235.6436,\n",
      "           8.4646, -397.9899,   15.2940,   10.2317,    7.0404,    4.4464,\n",
      "          10.5841,   12.8137], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49267.90234375\n",
      "Outputs tensor([   8.1190,   10.2487,    6.4003,   19.4883,  -55.4142,  -98.7858,\n",
      "           8.7951, -190.3657, -365.0917,    8.7930,   21.3075,    8.9022,\n",
      "          17.2217,    2.4931, -191.4516,    7.0677,   14.8287,    1.0573,\n",
      "        -312.6056, -306.2496], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 54702.6875\n",
      "Outputs tensor([   7.8429,   -1.3938,    3.9064,   -0.7439,    4.8246, -154.5822,\n",
      "           1.4504, -490.1950,   -5.8092,    1.6363, -204.2955,  -53.9126,\n",
      "        -148.9632, -376.4609, -532.5023,    1.4100,   -6.8941,  -19.9267,\n",
      "        -411.1152,    3.5415], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 60790.01953125\n",
      "Epoch training loss: 55873.970052083336\n",
      "Epoch validation loss: tensor([[41028.2930]])\n",
      "\n",
      "On epoch: 34\n",
      "Outputs tensor([-1.5636e-01,  2.0094e+00,  7.3408e+00, -3.0145e+02,  1.2360e+01,\n",
      "        -4.5823e+02, -2.6806e+01,  1.4911e-01,  4.9213e+00,  3.0875e+00,\n",
      "        -2.3569e+00, -8.8098e-02,  8.7427e+00,  2.3302e+01, -1.6307e+02,\n",
      "         1.1668e+01, -4.5627e+02,  8.9915e+00,  2.6840e+00, -3.6984e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50436.8046875\n",
      "Outputs tensor([-7.9365e+00, -2.7534e-01,  2.8467e+00, -2.5268e+02, -3.6132e+00,\n",
      "        -1.8583e+02, -5.0134e+02, -5.1724e+02, -3.0104e+01, -5.7606e+02,\n",
      "         4.7536e+00,  4.5704e+00,  7.3987e+00,  1.1379e+01, -6.4401e+00,\n",
      "        -3.6041e+00, -4.0832e+00, -2.9436e+02,  3.6374e+00, -5.7033e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41937.25390625\n",
      "Outputs tensor([ 6.9779e+00, -4.7931e+00,  3.3566e+00, -4.9901e+02, -2.9186e+00,\n",
      "        -2.4047e+00, -7.8884e+00,  6.8991e+00, -3.1637e+00,  1.5223e+00,\n",
      "         9.0800e+00, -3.9580e+00, -5.4176e+02,  4.8566e+00, -9.6564e+01,\n",
      "         7.5534e+00,  1.6133e+00, -4.6602e+02, -3.5207e+02,  1.4197e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27049.14453125\n",
      "Outputs tensor([-5.1416e+02, -4.5611e+02, -1.7499e+02,  1.4277e+00, -1.5663e+02,\n",
      "        -4.0189e+02,  5.3295e+00, -2.1107e+00, -2.0637e+02,  6.7217e+00,\n",
      "        -2.7841e+00, -6.5785e+00, -4.8791e+02,  1.1264e-01,  1.2746e+00,\n",
      "         6.9686e+00,  5.3747e+00,  6.4546e+00,  9.8443e+00,  1.0515e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28022.61328125\n",
      "Outputs tensor([-2.0368e+01, -1.5035e+01,  4.5498e+00, -5.2951e+02, -7.5199e+00,\n",
      "         2.2901e+00, -6.4468e+00,  7.3863e+00,  3.5341e-01, -2.2136e+01,\n",
      "        -6.2946e+02,  5.0972e-01, -2.1404e+02, -6.2957e+02, -2.8804e+00,\n",
      "         2.8509e-01, -2.9610e+02, -3.0180e+00, -4.9226e+02,  5.0704e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36963.48828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([-117.6804,   14.0122,    6.1449, -136.4662,  -17.1244, -610.0212,\n",
      "           2.3561,    2.4020,   -2.6262,   -2.6168,  -20.6655,  -32.3792,\n",
      "        -157.4821,    6.6144,    4.1681,    0.6881, -584.9415,   -7.0930,\n",
      "        -387.2354,   -3.2672], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35244.171875\n",
      "Epoch training loss: 36608.912760416664\n",
      "Epoch validation loss: tensor([[37706.4531]])\n",
      "\n",
      "On epoch: 35\n",
      "Outputs tensor([-126.9392,   10.4685,    4.1187,   -4.6894, -554.6456,   -1.8147,\n",
      "        -347.5087,    4.1603, -178.2786, -396.4240,   -4.0318,   -2.0704,\n",
      "        -167.1661,    1.1235,   -7.0339, -405.9759,    3.4419, -355.3957,\n",
      "           9.9031,    4.5415], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38832.203125\n",
      "Outputs tensor([ -11.5549,   -5.3156, -119.2669, -476.1318, -743.6243,  -61.1915,\n",
      "           3.3609,   -1.5378,  -17.2696,   -1.2580,  -43.0103,  -80.9782,\n",
      "          -3.3989, -279.6881,  -99.3625,  -63.1940, -101.4684, -347.1817,\n",
      "        -713.9135,   -2.2456], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 59549.59375\n",
      "Outputs tensor([   9.4104,   14.3026,  -10.8878,    3.2257, -243.0819, -249.9678,\n",
      "         -63.5570,    8.2954, -164.6377, -341.6623,  -83.0229,    8.9034,\n",
      "           4.7970, -152.5400,   19.9359,    3.5591, -133.9485,   19.4198,\n",
      "         -14.3254,    9.9878], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 70391.1484375\n",
      "Outputs tensor([ -26.4212,   -3.0728,   -6.8387, -376.5758, -211.9162, -399.5008,\n",
      "          -4.5953,  -30.3918,  -61.9865,   -6.7913,   -7.3061, -181.8649,\n",
      "         -15.7216, -622.6932,  -29.7115, -202.4157,    2.2836,  -54.3764,\n",
      "        -552.7953, -187.7935], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 53212.2578125\n",
      "Outputs tensor([ -70.8117,  -59.0599, -118.7996,   -4.5637,  -22.9671,  -20.0661,\n",
      "         -17.5390,    0.7111,   -2.8249,  -21.8082,   -6.2154, -639.6527,\n",
      "        -231.0724,  -75.2150,  -16.8231,   -1.9783, -157.8218,  -27.4893,\n",
      "        -187.9044,  -61.3967], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 56421.71875\n",
      "Outputs tensor([ -64.0133,    2.1792,   -2.5849,   -4.6217,    7.0993,   -3.0367,\n",
      "           7.2019, -350.4104, -472.1470,   -4.9881, -339.9343,    4.1945,\n",
      "          -0.9086,   -8.2842, -458.9863, -387.1397,   -4.8583,    7.0462,\n",
      "           4.6626,    8.3662], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28379.068359375\n",
      "Epoch training loss: 51130.998372395836\n",
      "Epoch validation loss: tensor([[35742.5234]])\n",
      "\n",
      "On epoch: 36\n",
      "Outputs tensor([-223.9322, -446.9454,    7.7263, -274.0453,    6.7729, -174.7313,\n",
      "           5.5870, -427.7948,  -19.6369, -441.0679,    4.8324, -482.4476,\n",
      "          -4.7808, -310.9359,    6.6491,    3.2404,   -2.0345,    0.9528,\n",
      "           2.1236,   -3.7302], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33505.875\n",
      "Outputs tensor([  -3.0925, -343.3081, -521.4310,    3.9037,  -90.3767,  -51.3778,\n",
      "          -2.7957,   -8.0292, -573.5178,  -66.9936,   -4.8126,    2.2532,\n",
      "           1.9042,   -5.0169, -561.6973,    2.9437,   -4.7020, -100.2895,\n",
      "          -6.8348,   -3.0354], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24119.81640625\n",
      "Outputs tensor([-5.1566e+02, -1.0838e+02, -2.4193e+01,  1.4071e+00,  2.4386e+00,\n",
      "         3.1760e-01, -7.3979e+00, -2.1599e+01, -4.0230e+02, -1.9608e+02,\n",
      "        -2.2822e+01, -5.7423e+00, -5.4328e+00, -4.1488e+02, -5.3337e+00,\n",
      "        -2.2343e+01, -6.8471e-01, -1.7595e+01, -1.5855e+02, -2.4297e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45359.265625\n",
      "Outputs tensor([  15.3819,   -4.9120,   -9.8768,  -94.0695,    5.4892,  -39.3500,\n",
      "           6.2338, -354.2638, -374.6634,   11.9600,   -3.3204,    0.7162,\n",
      "        -156.5404,   -5.9441, -300.4257,    6.5661,    1.6204,   10.6222,\n",
      "          12.3589,  -23.5924], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 62959.0234375\n",
      "Outputs tensor([   0.9739,  -17.2289, -230.4795,    1.2882, -228.6442, -213.8226,\n",
      "        -133.5833,  -31.6214, -437.8463,    8.3155,    5.3068,   -6.4192,\n",
      "         -85.6287,    0.7308,   -5.5341,    5.9091, -113.5877,    1.3830,\n",
      "        -488.4160,    1.7630], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38651.2734375\n",
      "Outputs tensor([-1.8854e+00, -7.3528e+01,  2.3610e+00, -6.0735e+02, -1.5862e+02,\n",
      "        -1.6837e+02, -2.0204e+01, -2.2913e+00, -4.4952e+01, -1.7388e+01,\n",
      "        -1.9409e+00, -3.6459e+01,  5.5060e+00, -5.1131e+02, -1.7238e-01,\n",
      "         3.2537e+00, -5.2793e+02, -5.1827e+02, -4.5308e+00, -6.8496e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31317.771484375\n",
      "Epoch training loss: 39318.837565104164\n",
      "Epoch validation loss: tensor([[33903.2109]])\n",
      "\n",
      "On epoch: 37\n",
      "Outputs tensor([-2.6718e+01, -4.3059e+02,  1.7291e-02, -2.3189e+00, -5.8461e+02,\n",
      "        -1.6456e+02, -3.8557e+01,  9.2584e-01, -6.1375e+01,  6.2628e+00,\n",
      "        -1.5253e+02,  6.1339e+00, -6.0080e+02, -2.7397e+01,  2.6473e+00,\n",
      "         2.1935e+00, -6.0796e+02,  5.9250e+00, -6.6746e+02, -1.8315e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43012.8515625\n",
      "Outputs tensor([   9.2551,   -3.2250, -549.4409, -437.4257,   -1.5470,    1.7280,\n",
      "           4.5899,    8.4383, -300.2763,    9.3235,    3.1498,    9.0030,\n",
      "          -5.1120,   -4.7758,    9.0638,   -5.0921,   -2.2757,   -5.5894,\n",
      "         -33.1926, -311.4072], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41345.5859375\n",
      "Outputs tensor([-165.3887,    3.1801, -166.5778, -456.5060,   -5.1047,    0.6962,\n",
      "           8.3925, -438.3992,    6.6489,    7.1205, -294.6697, -163.4625,\n",
      "        -125.7226,    8.8545,    9.1454,   10.4666,  -11.4167,   -5.6107,\n",
      "        -465.9277,   -5.3110], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24800.31640625\n",
      "Outputs tensor([-5.1907e+01, -5.1759e+01,  2.1793e+00, -2.5913e+01, -6.0576e+02,\n",
      "        -5.2280e+01, -4.1420e+01, -6.5276e+02, -2.4436e-01,  8.4495e+00,\n",
      "        -3.2118e+00, -3.2159e+00, -5.8043e+02, -9.7427e+00, -3.8515e+00,\n",
      "        -6.0394e+00, -1.6657e+02, -6.2200e+00, -8.9069e+00, -4.7146e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39852.19921875\n",
      "Outputs tensor([ 7.1319e+00, -1.2919e+00, -4.6849e+02, -4.2902e+02, -1.0105e+01,\n",
      "        -1.5454e+02,  3.9473e+00,  4.5461e-01,  3.8861e+00, -3.5631e+01,\n",
      "        -3.6171e+01, -7.4596e-01,  3.4356e+00, -2.3943e+00,  2.6604e-01,\n",
      "        -1.4272e+01, -4.7307e+02, -4.7744e+02,  7.8578e+00,  7.6512e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32210.08984375\n",
      "Outputs tensor([ 1.8657e+00, -2.2100e+02,  2.2912e+00, -6.1803e+00, -5.3293e+00,\n",
      "        -3.6804e+02,  1.0701e+00, -5.2251e+02, -1.2539e+02, -2.5100e+02,\n",
      "        -7.5429e+00, -1.2393e+01,  6.5563e+00, -3.0467e+02,  5.1072e+00,\n",
      "        -3.6440e+00,  1.2683e+00,  9.3717e+00, -1.3632e-02,  2.1449e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36277.0390625\n",
      "Epoch training loss: 36249.680338541664\n",
      "Epoch validation loss: tensor([[39870.8516]])\n",
      "\n",
      "On epoch: 38\n",
      "Outputs tensor([  -6.9082,  -93.9255,  -11.4860,   -5.1638, -487.4744,   -5.0784,\n",
      "          -1.1325,   -1.2438,   -2.1311,  -97.0034,  -66.2536,   -6.0036,\n",
      "          -4.4990, -684.6026, -127.7737, -552.3289,  -12.1477, -170.0896,\n",
      "         -86.8463,    1.0721], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39959.3125\n",
      "Outputs tensor([-3.7002e+02,  3.3011e+00, -1.3880e+00,  8.3962e+00, -5.1577e-01,\n",
      "        -3.7051e+02,  1.0472e+01, -1.4186e+02, -3.8578e+02, -4.0512e+02,\n",
      "        -5.4763e+00,  6.4797e+00, -4.5928e+02,  6.2901e+00, -1.1986e+02,\n",
      "         4.5770e-01, -2.0768e+02,  1.1031e+01,  1.1996e+01,  2.4281e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37051.7734375\n",
      "Outputs tensor([-3.4243e+02, -6.7359e+00, -6.1465e+02, -1.7583e+01, -6.9856e+02,\n",
      "         1.1443e+00, -1.5626e+01, -1.7349e+01, -5.6449e+01, -8.0807e-01,\n",
      "        -6.6789e+00, -5.6611e+00, -2.3841e+02, -9.1922e+00,  3.5136e-01,\n",
      "        -1.3481e+01, -1.0777e+01, -3.0981e+02,  6.5544e-01, -5.8468e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49131.9375\n",
      "Outputs tensor([-7.0621e+00, -9.0814e-01,  6.1179e+00, -8.4871e+01, -1.9066e+00,\n",
      "        -4.8114e+02,  1.1103e+01,  9.8906e+00, -4.7343e+02, -4.5738e+02,\n",
      "        -6.1234e+01,  4.0423e+00,  3.7037e+00, -1.6001e+02, -4.1018e+01,\n",
      "        -8.4438e+00,  1.5617e-01,  3.7127e+00,  7.6495e+00, -1.9704e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38971.2578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([-4.2008e+02,  4.2722e-01, -1.4971e+01,  1.6161e+01,  1.1415e+01,\n",
      "        -2.9948e+02,  7.9874e+00, -3.7144e+02,  1.4571e+01,  1.3942e+01,\n",
      "         1.1726e+01, -9.3258e-01, -2.7963e+00, -1.5672e+02,  1.0612e+01,\n",
      "        -3.9046e+02, -1.1099e+02, -3.8125e+02,  3.5761e-01, -1.4145e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 60798.5234375\n",
      "Outputs tensor([ -88.2195,    2.7342,  -25.8681,   -8.0488,    1.0020,  -26.7875,\n",
      "        -404.6465,  -28.0112,   -2.0274, -607.5143,   -6.5997, -683.1084,\n",
      "         -24.8362, -341.3794,  -31.6041,    1.3155,  -26.2996,   -7.1611,\n",
      "        -259.5411, -344.1314], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 12957.392578125\n",
      "Epoch training loss: 39811.699544270836\n",
      "Epoch validation loss: tensor([[35929.6836]])\n",
      "\n",
      "On epoch: 39\n",
      "Outputs tensor([  -6.6231,   -4.4927,   -1.3963, -698.3976,   -5.4911,   -3.8266,\n",
      "         -67.1398, -238.9017, -652.6821,  -10.7252,   -8.0392,   -8.7614,\n",
      "         -73.1904, -122.4115,   -7.7918,   -4.3813,  -72.8750,  -50.3755,\n",
      "        -468.5038, -532.5601], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36502.52734375\n",
      "Outputs tensor([-4.6438e+00, -3.7052e+00,  9.1991e+00,  5.2956e+00, -2.6275e-01,\n",
      "         3.0320e+00, -2.7019e+00, -2.2314e-01, -3.0850e+02, -3.4079e+02,\n",
      "        -4.0070e+02, -3.7458e+00, -1.5373e+02, -4.9949e+00,  1.5097e+01,\n",
      "        -4.9685e+02, -4.4635e+02, -4.7076e+02,  1.1836e+00,  1.1340e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37646.4375\n",
      "Outputs tensor([ -83.9523,    1.2296,   -1.3011, -199.3405, -511.0892,   -4.0856,\n",
      "           2.0112,   -4.0661, -442.4446, -223.7641,   -4.9572,   -3.8618,\n",
      "           1.6737,  -37.1081, -503.7849,   -5.1647, -523.3739,  -16.9257,\n",
      "           2.6149, -614.0692], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42137.90234375\n",
      "Outputs tensor([-2.4401e+01, -2.6435e+02, -5.2518e+01,  8.5324e+00, -2.0501e+00,\n",
      "        -6.2695e+02, -2.4685e+01, -8.8902e+01, -4.3468e+00, -6.0967e+00,\n",
      "        -5.6476e+02,  4.1389e+00, -3.0678e+00, -8.6925e+00, -6.9415e+00,\n",
      "         1.1736e+00, -2.6623e+00,  8.8311e-02, -4.3743e+02, -4.6900e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28946.056640625\n",
      "Outputs tensor([-6.4187e+00, -4.6792e+00, -1.1583e+01, -4.1537e+00, -4.9760e+02,\n",
      "        -4.9371e+02,  9.4270e+00, -2.8629e+02, -4.8389e+00, -1.4189e+01,\n",
      "         3.0491e+00, -2.1998e+00,  4.2902e+00, -5.2359e+02,  2.2856e+00,\n",
      "        -4.5310e+00, -2.4502e+00, -4.3638e+02, -4.2315e-01,  1.1345e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45305.8671875\n",
      "Outputs tensor([-6.8183e+00, -5.1613e+00, -4.5392e+00, -1.1674e+02, -4.1002e+01,\n",
      "        -4.5203e+00,  1.8529e+00, -8.9577e+00, -8.4723e+01, -2.4455e+02,\n",
      "        -5.4059e-01, -2.1672e+02, -2.5495e+02, -3.2101e+00, -4.8058e+00,\n",
      "        -2.0331e+00, -5.5990e+02, -1.0105e+01,  3.5954e+00, -5.9115e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26215.75\n",
      "Epoch training loss: 36125.7568359375\n",
      "Epoch validation loss: tensor([[33409.6289]])\n",
      "\n",
      "On epoch: 40\n",
      "Outputs tensor([ 3.4511e+00, -3.8386e-01, -6.9559e+01, -2.7595e+01, -3.2570e+00,\n",
      "        -9.2837e+00,  4.6602e+00, -1.6652e+01, -5.1832e+02,  4.4645e+00,\n",
      "        -1.6974e+01, -2.7752e+02, -3.8931e+02, -5.9030e+01,  3.4302e+00,\n",
      "        -5.3153e+01, -6.1177e+02, -2.5997e+02, -6.5747e+00, -3.0859e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50773.92578125\n",
      "Outputs tensor([   4.0043,   -4.7841,  -23.6103,    6.0432, -506.2744,    5.7809,\n",
      "          -4.8427,    9.1270,   -6.3050,   -1.3540,   -6.8262, -495.6743,\n",
      "        -140.8303, -529.9516, -380.5580,    4.5356, -241.9601,    2.6901,\n",
      "          -4.4027,  -11.8260], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38114.20703125\n",
      "Outputs tensor([ 6.3284e+00,  5.8165e+00,  4.2432e+00, -1.4156e+01, -2.7329e+00,\n",
      "        -4.1700e+00, -2.3136e+01,  1.6379e+00, -1.5835e+01,  2.5487e-01,\n",
      "        -4.0840e+02,  7.5778e+00, -2.7341e+02, -5.0368e+02, -4.1939e+02,\n",
      "         8.2970e+00, -3.1208e+01, -8.1311e+00, -1.8824e+00, -1.1024e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26688.79296875\n",
      "Outputs tensor([-651.2047,  -25.0514, -207.0852,    2.6858,   -2.6151, -483.6478,\n",
      "         -74.6075, -309.1172,    7.2012, -243.3195,   -5.1171,    5.9423,\n",
      "           4.4058, -334.2086,    7.2346,    1.4607,   -6.2632,   -7.2408,\n",
      "          -5.4207,    8.2524], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 14672.6611328125\n",
      "Outputs tensor([-5.2841e+02, -3.5838e+02,  1.0460e+00,  2.5391e-01,  7.6560e-01,\n",
      "         9.0446e+00, -3.9496e+02, -9.4791e+00, -3.2583e+02, -2.4449e+00,\n",
      "        -2.7383e+02,  3.8005e+00,  2.0024e+00, -3.4942e+00, -6.7744e+00,\n",
      "         7.3999e+00, -4.2616e+02, -4.2301e+02,  3.4977e+00, -1.7408e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41515.5546875\n",
      "Outputs tensor([-1.1888e+02, -4.2089e+01, -9.2417e+00,  2.6593e-01, -6.3545e+02,\n",
      "        -6.3531e+02, -3.2432e+02, -7.3872e+02, -3.4794e+01,  1.6588e+00,\n",
      "         3.8196e+00, -4.9602e+02, -2.6342e+01, -6.5664e+00, -6.9369e+00,\n",
      "        -9.2890e+00, -1.4356e+00, -6.8312e+00, -7.1240e+00, -6.5572e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 18736.119140625\n",
      "Epoch training loss: 31750.210123697918\n",
      "Epoch validation loss: tensor([[35556.1172]])\n",
      "\n",
      "On epoch: 41\n",
      "Outputs tensor([ -86.4491,   -8.1833,    1.1341, -133.2726,   -3.3176,   -9.2302,\n",
      "        -186.5154, -609.5188,   -4.6088,   -8.3185, -649.6854,  -13.6011,\n",
      "          -3.9222,   -5.8035,  -16.7500, -439.2303,   -4.7474, -125.1552,\n",
      "          -3.5269,   -2.2079], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42607.12109375\n",
      "Outputs tensor([-400.4808, -144.9062, -504.1668,    6.7792,   -7.7161, -304.5950,\n",
      "           8.1004,    4.1193, -447.2406, -403.0341,    3.2092,   10.0260,\n",
      "        -191.6746,    8.6885,   -6.8103,    1.7789,   -3.4124,   -1.6794,\n",
      "           5.1207,   -3.8625], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31626.71875\n",
      "Outputs tensor([  -0.5145,    5.5488,  -24.8132,   -2.1941,   10.4211,   11.1667,\n",
      "          -3.4942,   -5.7420, -264.4440, -465.2567, -485.8018, -408.8697,\n",
      "           3.9273,    7.8625, -507.5769, -110.2277,    8.3429,   -0.7136,\n",
      "        -438.3669,   10.6554], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33917.66015625\n",
      "Outputs tensor([-3.7806e+02, -3.1984e+02, -2.6446e+02, -1.2031e+01,  6.1206e+00,\n",
      "         2.0577e-01, -8.3828e+01, -1.8173e+01, -4.6753e+01, -1.5292e+00,\n",
      "        -1.0674e+01,  2.4828e+00, -8.5058e+00, -6.3598e+02, -5.3843e+02,\n",
      "         4.7814e-01, -5.9157e+02, -4.1389e+01, -4.7703e+00, -8.2521e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34318.671875\n",
      "Outputs tensor([-109.7730,  -10.6711, -218.2901,  -19.5995, -129.1888, -103.1207,\n",
      "         -20.1479, -510.4735,   -1.7956, -177.2443,  -55.3630, -537.5044,\n",
      "        -146.7788,   -6.8825,    3.4416, -629.6483, -339.3972, -106.5590,\n",
      "         -40.2592,   -9.5348], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38822.7890625\n",
      "Outputs tensor([-8.1821e+00,  2.2661e+00, -2.9142e+02, -2.7919e-01,  2.5700e+00,\n",
      "         1.9417e+00, -2.9290e+02, -5.1717e+02, -7.2751e+00, -1.6310e+02,\n",
      "         5.2193e+00, -6.1611e+02, -1.9611e+01,  2.7151e+00,  1.1145e+01,\n",
      "        -4.2929e+00, -1.4397e+00,  6.9912e+00, -1.4359e+02, -1.9974e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 60333.4921875\n",
      "Epoch training loss: 40271.075520833336\n",
      "Epoch validation loss: tensor([[37713.0781]])\n",
      "\n",
      "On epoch: 42\n",
      "Outputs tensor([ 5.8292e+00,  7.1098e+00, -4.5575e+02, -2.8913e+02,  8.4876e+00,\n",
      "        -1.7922e+00,  2.2412e+00,  1.3135e+00,  7.4856e+00, -3.5546e+00,\n",
      "        -4.6314e+02, -6.1246e+01, -4.9973e+02,  1.1385e+01,  5.0214e+00,\n",
      "         1.8100e+00, -1.6214e-01,  7.1105e+00,  3.1747e+00, -8.2298e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31018.828125\n",
      "Outputs tensor([-3.6681e+02, -2.7357e+02, -3.4133e+00,  5.4430e+00, -4.8428e-01,\n",
      "         3.3214e+00, -5.6175e+00, -3.5427e+00,  1.1494e+01,  4.0909e-01,\n",
      "         9.0087e+00,  3.1501e+00, -4.3470e+02, -5.2015e+01,  1.1122e+01,\n",
      "         1.8650e+00, -4.1458e+02, -2.1122e+00,  7.6624e+00, -1.7898e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43375.59765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ 2.3762e+00,  2.5488e-02,  1.0717e+01,  6.4444e+00, -2.7219e+02,\n",
      "         1.2863e+00, -5.0366e+00,  9.2956e+00, -7.8341e+00, -4.4258e+02,\n",
      "         4.5076e+00, -4.4757e+00, -2.8053e+01,  1.1123e+01,  7.0340e+00,\n",
      "        -1.8344e+00, -2.8849e+02, -4.9711e+02,  6.3080e+00, -5.1554e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43616.26953125\n",
      "Outputs tensor([-9.9685e+00,  1.6406e+00, -1.8889e+02, -3.2864e+02,  1.8945e+00,\n",
      "        -6.8800e+00,  5.1095e+00, -5.4725e+02, -4.5174e+02, -4.3270e+01,\n",
      "        -4.5680e+00, -7.8832e+00, -4.3055e+00, -4.7420e-01, -3.3975e+02,\n",
      "         3.4369e+00,  3.8024e+00, -2.8690e+02, -3.6410e+02, -1.9049e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29310.427734375\n",
      "Outputs tensor([-180.3779, -646.0809,   -2.1372, -443.0504,   -2.1328,   -7.4469,\n",
      "          -8.3149,  -20.4519,    1.4874,   -8.9671,  -22.1143, -637.1796,\n",
      "        -634.1688,   -7.5789,   -5.4844, -121.9936, -625.6395,   -8.6779,\n",
      "          -0.7349,  -57.3617], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29064.583984375\n",
      "Outputs tensor([-1.0360e+01, -1.3717e+01, -7.6256e+00, -3.4747e+02, -7.4902e+00,\n",
      "        -4.6345e+02,  1.6130e+00, -4.4804e+02,  6.2165e+00, -2.4812e+02,\n",
      "        -6.3131e+02,  6.4911e+00, -7.3807e+00, -4.8288e+00, -9.9847e+00,\n",
      "        -8.2547e+00, -8.0392e+00,  2.2666e-01, -4.8882e+00, -1.1294e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38354.15625\n",
      "Epoch training loss: 35789.977213541664\n",
      "Epoch validation loss: tensor([[36185.2891]])\n",
      "\n",
      "On epoch: 43\n",
      "Outputs tensor([-5.7897e+01, -4.9465e+01,  1.3290e+01, -4.0171e+02, -2.0840e+02,\n",
      "        -5.2255e+00,  1.2428e+01,  2.3445e+00, -8.9882e+01,  4.0388e+00,\n",
      "         5.9560e+00, -5.6957e-01, -2.2256e+00, -4.4457e+02,  3.3883e+00,\n",
      "         1.0215e+01, -2.7821e+02, -3.9015e+02, -2.4074e-01, -3.6266e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 57039.8515625\n",
      "Outputs tensor([  -8.0908, -596.1975,   -3.6003,  -11.3534, -267.3168, -122.0351,\n",
      "         -55.1056,  -63.1964,   -9.4800, -618.3109,  -42.3495, -700.4360,\n",
      "         -10.8728, -364.8106,   -1.9090, -751.3508,   -7.2733,  -10.7884,\n",
      "         -12.6088,   -5.0905], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50140.703125\n",
      "Outputs tensor([-5.2627e+00, -4.1882e+02, -2.7645e+00, -1.2753e+02, -7.9076e+01,\n",
      "        -1.3295e+02, -1.7693e+02, -1.6723e+01, -2.8647e+01, -4.9003e+00,\n",
      "         1.7953e+00, -4.0985e-01, -9.2066e+00, -8.8923e-01, -4.7393e+02,\n",
      "        -3.5350e+01, -6.6166e+00,  1.6169e+00, -7.3129e-01, -3.0906e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51293.015625\n",
      "Outputs tensor([-2.7308e+02,  5.8463e-01, -3.3354e+00, -5.7415e+00, -4.3293e+02,\n",
      "        -8.7918e+00, -4.1650e+01, -6.0298e+00, -6.2734e-01, -1.2147e+00,\n",
      "         2.4641e+00, -7.2017e+00, -2.2875e+00, -1.5566e+00,  4.1198e+00,\n",
      "        -4.1747e+01, -2.1213e+02, -4.2978e+02, -5.8554e+02,  5.6988e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40432.6015625\n",
      "Outputs tensor([  -2.3644, -241.3197,    2.7748, -473.9979,    9.7633,    0.5769,\n",
      "          -3.6653,   -5.5555,   -3.9936,   -5.1603,   -0.7898, -221.0608,\n",
      "           7.5486,    6.0501, -124.2785,   -3.8020,    2.6183,    5.6087,\n",
      "        -246.7399,  -68.7116], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30249.806640625\n",
      "Outputs tensor([-366.3098,    2.3316, -469.1661, -548.3933,   -4.8866,   -4.2588,\n",
      "          -6.5532, -535.9559,   -6.9788, -423.9516,   -8.6051,    4.9539,\n",
      "          -6.8869,   -2.8082,   -3.5023,    1.8386,   -3.0795,   -3.1609,\n",
      "           6.5022,   -7.5671], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 17959.296875\n",
      "Epoch training loss: 41185.879231770836\n",
      "Epoch validation loss: tensor([[39124.8945]])\n",
      "\n",
      "On epoch: 44\n",
      "Outputs tensor([-5.0123e+00, -6.2234e+02, -2.9871e+02, -1.5031e+02, -2.1957e+02,\n",
      "         9.3610e-01, -5.1002e+01, -6.2156e+00, -6.3201e+00, -8.2919e+00,\n",
      "         2.4212e+00, -8.5786e+00,  3.3791e+00, -5.6960e+02, -2.7502e+00,\n",
      "        -4.8499e+00,  1.8996e-01, -1.8720e+00,  5.4410e+00, -2.3254e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51401.0234375\n",
      "Outputs tensor([ 4.1895e-01, -3.7521e+00,  2.2264e+00,  1.0237e+01, -4.5209e+02,\n",
      "        -2.2670e+02, -7.4250e+00,  9.4604e+00,  3.8842e+00,  1.0072e+01,\n",
      "        -4.4665e+00, -5.0096e+02, -1.9405e+00,  5.3655e+00, -6.2390e+02,\n",
      "        -4.4376e+00, -3.6854e+00, -1.4106e+01, -3.2153e+02, -4.9253e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32324.67578125\n",
      "Outputs tensor([-3.4285e+02, -1.3409e+00,  6.7121e+00, -7.6958e+01, -5.0828e+02,\n",
      "        -2.6796e+00, -4.3239e+02, -5.7685e+02, -2.5106e+01, -8.0571e+00,\n",
      "        -1.1246e+02, -1.8851e-01, -2.3700e+00, -2.0368e+02,  1.1933e+00,\n",
      "        -1.0907e+01,  1.6472e+00,  3.8335e+00, -2.1608e+01, -4.6924e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34945.16796875\n",
      "Outputs tensor([-1.0924e+01, -1.3357e+02, -4.8735e+01, -5.4437e+02, -2.5229e+02,\n",
      "        -3.5591e+00, -1.1109e-01, -7.0321e+00, -4.2780e-01, -1.6488e+00,\n",
      "        -5.1343e+00,  2.2322e-01, -2.3849e+02,  8.8091e+00, -5.2538e+02,\n",
      "        -7.1302e+02,  4.0794e+00, -4.4520e+00, -2.2764e+00, -3.0844e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 48609.0859375\n",
      "Outputs tensor([ 9.6222e+00,  1.2416e+01, -1.2804e+00, -2.6699e-01,  8.7349e+00,\n",
      "         9.7330e+00, -1.2508e+02, -3.9929e+00,  2.9858e+00, -4.7158e+02,\n",
      "        -2.4790e+00,  1.6210e+00, -1.9077e+00,  1.0204e+00,  5.8782e+00,\n",
      "        -8.2937e+00,  1.3045e+01, -6.5926e+00, -1.5182e+01, -3.0747e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32999.94140625\n",
      "Outputs tensor([ 6.3893e+00,  7.8952e+00, -3.7108e+02,  9.7873e+00,  5.9613e+00,\n",
      "        -1.3834e+02,  1.2563e+01,  5.6273e+00,  6.1738e+00,  1.0193e+01,\n",
      "        -3.8859e+02, -3.2556e+02,  4.0234e+00, -2.1782e+02, -3.0773e+02,\n",
      "        -4.4408e+02,  3.0191e+00,  7.4058e+00, -3.9690e-01, -4.0341e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24543.93359375\n",
      "Epoch training loss: 37470.638020833336\n",
      "Epoch validation loss: tensor([[40695.2109]])\n",
      "\n",
      "On epoch: 45\n",
      "Outputs tensor([-2.2905e+01, -2.7949e+02, -3.0791e+02, -8.7341e+01, -1.1020e+01,\n",
      "         1.1575e+00, -5.3448e+00, -4.7522e+01, -8.7061e+00, -3.6506e+02,\n",
      "        -8.8384e+00, -6.9867e-02, -6.9482e+01, -1.7915e+00, -1.3747e+02,\n",
      "        -7.4657e+00, -9.7371e+00,  7.3627e-01, -2.0635e+01, -4.2725e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 82742.859375\n",
      "Outputs tensor([-1.6511e+01,  8.7446e-01,  1.6746e+01, -2.5311e+00,  3.8336e-01,\n",
      "         1.0889e+01,  8.6074e-01, -4.4692e+02,  6.0697e+00, -1.1087e+02,\n",
      "         1.4398e+01,  4.6704e+00,  3.3884e+00, -3.6810e+02, -3.3887e+01,\n",
      "        -2.1297e+01,  1.3791e+01,  1.4678e+01, -3.5981e+02, -4.7197e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51540.546875\n",
      "Outputs tensor([   6.7690,    8.6966,   -6.6006, -136.6414,   -5.7667,   -6.3358,\n",
      "        -410.4579, -338.1562,    4.5237, -526.9630,    2.7321,   -6.4974,\n",
      "          -2.7665,  -23.8747,    1.3007, -191.6024,    7.7952, -403.3503,\n",
      "          -7.5351, -388.0000], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36632.65234375\n",
      "Outputs tensor([-590.8727,  -21.1489,  -36.0368,   -8.9590,   -3.3918,   -7.8907,\n",
      "         -47.2772,   -7.7986,   -2.3656,   -2.0486,  -11.1150, -116.1432,\n",
      "        -260.2136,  -46.0950,  -28.3807,   -5.6954, -181.3501,  -24.8642,\n",
      "         -13.8978,  -12.1710], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39613.39453125\n",
      "Outputs tensor([ 9.1288e+00,  3.5360e+00, -4.4476e+02,  1.6208e+00, -3.5438e+01,\n",
      "        -5.5242e+02,  2.0039e-01,  3.0861e+00, -2.0532e+01, -1.9223e+02,\n",
      "        -2.7048e+02,  7.5410e+00,  1.3868e+00, -3.5967e+02, -4.6165e+02,\n",
      "         6.6481e+00,  9.3504e+00,  1.2193e+01, -1.9574e+01, -7.3289e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40606.25\n",
      "Outputs tensor([  -5.1271, -451.1539,  -11.0688, -228.0301,   -2.2286, -561.2756,\n",
      "        -471.4151,   -2.0626,   -4.3444,   -7.9912, -605.5436,   -7.2401,\n",
      "         -10.5741,   -4.5217,  -10.3116,   -4.9938, -584.6425,   -9.1022,\n",
      "        -473.4531, -121.4493], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24649.439453125\n",
      "Epoch training loss: 45964.1904296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch validation loss: tensor([[35350.2617]])\n",
      "\n",
      "On epoch: 46\n",
      "Outputs tensor([ -27.8958,   -6.8421,   -4.9699, -528.9380, -620.6840, -141.6367,\n",
      "        -224.0780,  -28.1174,  -32.1364,  -46.7744, -567.7373,   -6.5064,\n",
      "         -13.9827, -465.9713,  -28.8000,   -9.6061,  -24.6210,  -49.3510,\n",
      "        -311.1937,   -2.4269], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34975.3046875\n",
      "Outputs tensor([ -67.2499,  -49.8030, -370.0947,  -28.0674,  -10.7494,  -11.3698,\n",
      "        -114.0467,  -87.4699,  -82.6748,   -8.3064,  -85.2029, -547.9223,\n",
      "         -31.7115, -820.0279,  -22.9729, -184.2640, -577.7061, -164.8919,\n",
      "        -141.3154,  -32.6405], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29655.3125\n",
      "Outputs tensor([ -22.7253, -208.8341,  -11.0630,  -12.7338,   -6.0842,  -20.7631,\n",
      "          -5.6455,   -3.0974, -525.5159,   -8.6863,   -6.6542, -474.8240,\n",
      "         -13.8626,  -20.5902, -317.8638, -226.2191,  -30.5581, -105.6985,\n",
      "           1.8728, -562.8932], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29197.859375\n",
      "Outputs tensor([-1.3665e+01, -3.5864e+02, -4.9638e+00, -1.3674e-01, -1.8078e+00,\n",
      "        -3.1260e+02, -1.9124e-01, -4.0721e+02, -1.4783e+01, -9.0335e+01,\n",
      "        -2.5321e+00, -4.3861e+02, -9.6018e+00, -8.0896e+00, -4.1142e+02,\n",
      "        -5.6352e+02, -1.6951e+00, -1.0461e+01, -1.2288e+01, -2.1281e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47017.3203125\n",
      "Outputs tensor([ -13.1187, -693.9614, -725.5120,  -30.5475, -336.2621, -229.8388,\n",
      "         -68.8821,  -11.2872,  -14.1991,  -89.4916,  -26.6450, -148.5911,\n",
      "         -12.8684,  -10.8309,   -8.8499, -198.5808,  -31.2579,  -12.6690,\n",
      "         -18.3189,  -10.6183], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46589.546875\n",
      "Outputs tensor([-3.9787e+01, -2.3313e+02, -3.2886e+02, -1.7491e+01, -1.7744e+01,\n",
      "        -1.5957e-01, -3.8404e+00, -1.7688e-01, -1.0249e+01, -8.7587e+00,\n",
      "        -4.1727e+00, -4.7362e+02, -2.9172e+01, -8.3988e+00, -2.9123e+00,\n",
      "        -4.8705e+02, -3.7357e+02, -1.1747e-01, -3.5148e+02,  2.2003e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46371.859375\n",
      "Epoch training loss: 38967.8671875\n",
      "Epoch validation loss: tensor([[35187.0039]])\n",
      "\n",
      "On epoch: 47\n",
      "Outputs tensor([   2.6955,  -32.1724,   -8.3155, -286.4579,  -19.4117,   -1.6063,\n",
      "        -449.0828,  -14.1828,   -2.0610,   -7.4026,   -5.2783,    2.5069,\n",
      "           2.3845,   -4.8725,    0.6779,  -13.2798, -174.6113, -233.0169,\n",
      "        -235.7498,  -14.1129], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31797.91015625\n",
      "Outputs tensor([-325.5699, -230.6284,    4.8429,  -33.9966,   -8.8573,    2.2559,\n",
      "        -263.0335, -434.2624,   -8.1813,  -21.0456, -486.6940,    0.7761,\n",
      "           1.9480, -402.0299,    2.0794,   -0.5001,  -10.2539,   -9.2177,\n",
      "           5.4033,   -3.7091], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33896.1640625\n",
      "Outputs tensor([  -3.7275,  -62.0632,   -9.2132, -605.9224,   -7.0790,   -6.4487,\n",
      "         -19.2622,   -1.5119, -427.6720, -337.0217, -320.6273,    1.6025,\n",
      "         -76.8567,  -11.3052,  -18.8529, -467.7191,   -1.0356, -354.8548,\n",
      "          -1.3971,   -9.5405], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31573.32421875\n",
      "Outputs tensor([-1.9457e+01, -5.7583e+02, -5.4018e+02, -3.1244e+00, -4.7611e+02,\n",
      "        -1.0675e+02, -2.0290e+01, -2.5719e-01,  6.4993e-01, -1.4421e+01,\n",
      "        -2.9960e+01, -6.9577e+00, -5.6245e+02,  3.6749e+00, -7.2777e+01,\n",
      "        -1.8231e+00,  1.8639e+00, -1.1532e+01,  3.9426e+00, -1.6002e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41325.73828125\n",
      "Outputs tensor([  -9.4634,   -1.0606, -383.9464,  -10.0666,   -5.8322, -486.1426,\n",
      "         -14.8225,   -4.3304,  -94.4082,  -11.9465,  -13.8335,  -11.7532,\n",
      "           0.5641, -472.7082,  -84.4124,   -8.8272, -277.4614, -246.9335,\n",
      "           7.6976,   -7.2195], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33364.75390625\n",
      "Outputs tensor([-557.0463, -388.1788,    1.5905, -514.0637,  -10.4963,   10.7708,\n",
      "          -0.7721,    2.9632,  -11.5579,   -1.5089,  -20.0957,   -5.6041,\n",
      "        -468.2741,   -4.5415,    5.3034,   -1.6993, -330.8364, -377.0289,\n",
      "        -525.1020,   -7.6443], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 22669.31640625\n",
      "Epoch training loss: 32437.867838541668\n",
      "Epoch validation loss: tensor([[34084.6797]])\n",
      "\n",
      "On epoch: 48\n",
      "Outputs tensor([-126.8290,  -31.0948,   -5.9251,  -13.3364,  -13.3634,   -2.3769,\n",
      "        -489.7010, -293.1087,    4.9184,   -7.6138,    3.2470, -261.1136,\n",
      "         -92.7904,    2.1404,   -3.5952, -559.8607,   -7.1509, -470.3044,\n",
      "        -213.9804, -441.9797], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30602.177734375\n",
      "Outputs tensor([ -18.3621,  -12.5351, -681.8385,   -1.3767, -340.0423, -117.3243,\n",
      "        -344.3974,  -48.5454,  -65.1566,  -12.6264,  -10.7277,  -12.3432,\n",
      "        -705.4292, -378.2763,   -9.9166, -727.3744,  -23.5270,   -4.8696,\n",
      "         -12.4824, -703.0773], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 21960.76953125\n",
      "Outputs tensor([-593.7319,   -2.4010, -345.0328,  -19.2697, -116.6273,  -36.0319,\n",
      "         -10.7562,  -12.8489,  -46.1896,  -46.4797, -562.8657,   -5.6384,\n",
      "          -6.7621,   -3.3027, -106.3048, -390.4153,  -11.4188, -609.2466,\n",
      "         -18.1913,  -11.5184], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 52255.7109375\n",
      "Outputs tensor([  -2.8980,  -19.6720,   -7.8966,  -35.9004,    1.3126,   -7.3599,\n",
      "         -79.8014,  -39.9275, -576.2921,  -14.7580, -325.1005, -421.9315,\n",
      "         -12.0453, -566.1564,   -9.1945,   -1.1467, -158.3348,   -1.6353,\n",
      "          -9.6894,   -6.6459], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32045.318359375\n",
      "Outputs tensor([ -12.8238,  -99.7731,  -41.2402,  -33.0683,    0.5629,   -3.6991,\n",
      "          -1.6869,   -8.2225,   -5.4995,   -7.4071,   -2.2646,  -74.1023,\n",
      "          -7.4267, -485.4281,  -51.1754, -266.7291,   -8.1698,   -7.5939,\n",
      "         -32.9930,  -33.9784], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39141.75390625\n",
      "Outputs tensor([ 7.0825e+00,  9.8368e+00, -8.9001e+00, -3.5346e+02,  7.4978e+00,\n",
      "        -2.4884e+02, -5.1373e+00,  3.6585e+00, -1.6141e+02, -4.1305e+02,\n",
      "        -2.7724e+00, -1.0110e+02, -8.8438e+00, -9.2184e+01, -4.3815e+00,\n",
      "         9.9052e+00,  2.0532e+00, -4.1067e+02, -1.4801e+00, -7.7298e-02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41949.55859375\n",
      "Epoch training loss: 36325.881510416664\n",
      "Epoch validation loss: tensor([[42526.7891]])\n",
      "\n",
      "On epoch: 49\n",
      "Outputs tensor([  -1.8385,  -44.8043, -415.3235,  -16.0009,   -9.9089,   -8.3117,\n",
      "        -174.0408, -195.5125,   -6.4646, -522.8825,  -11.5771,   -8.1277,\n",
      "        -450.7132,   -6.1433,   -6.5804,  -25.2463,  -25.8738, -314.9759,\n",
      "        -392.2427,   -6.5794], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40400.66796875\n",
      "Outputs tensor([-3.8018e+02, -5.2725e+02, -2.4900e+02,  6.0119e-01,  1.7267e+00,\n",
      "         1.0544e+00, -1.9934e+02, -1.5334e+01, -2.4149e+00, -4.6016e+00,\n",
      "        -9.4658e+00, -1.0657e+00, -7.8611e+00, -9.4420e-01, -4.5639e+02,\n",
      "        -4.4323e+00,  2.8156e-02, -4.4921e+02, -6.0142e+00, -3.2574e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25220.095703125\n",
      "Outputs tensor([-380.4140, -408.5094, -110.5884,  -28.5583,   -7.5681,   -9.0880,\n",
      "         -85.9736,   -2.1781,   -1.5158,    4.2251,   -2.4243, -576.1040,\n",
      "         -23.1849, -576.2405,  -56.2379,  -22.5542,   -5.9217, -640.7405,\n",
      "         -48.2003,   -3.4154], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35800.7265625\n",
      "Outputs tensor([  -2.5619,   -6.6807,   -4.4016,  -27.5666,   -2.9815,   -1.7666,\n",
      "         -11.2360,  -20.2934,  -87.3366,   -5.9829, -594.1253,    3.1489,\n",
      "         -87.3802,    1.7237,  -65.3731, -100.2248, -170.9505,   -7.7429,\n",
      "        -220.7581,   -1.3605], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47835.6484375\n",
      "Outputs tensor([ 1.0980e-01, -2.1095e+02, -1.1078e+00, -4.6373e+00,  7.7374e+00,\n",
      "        -9.4032e+00,  2.6486e+00, -5.5433e+00,  3.0352e+00,  1.0388e+01,\n",
      "         3.3723e+00,  1.2748e+01, -2.0059e+02,  4.5623e+00, -3.9929e+02,\n",
      "        -2.3518e+00, -3.0033e+02, -3.6973e-01,  8.0967e+00,  2.9062e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51821.4609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([   5.9907, -442.0102,    3.3542,    1.0857, -412.5522,    3.4039,\n",
      "        -436.0062, -420.9803,   -5.3000,    2.0973,    5.1990,    3.5131,\n",
      "           5.5825, -144.4770, -432.4084, -458.9548,    5.2516,   -5.2939,\n",
      "          -8.4347,    7.4333], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35034.73046875\n",
      "Epoch training loss: 39352.2216796875\n",
      "Epoch validation loss: tensor([[32093.5801]])\n",
      "\n",
      "On epoch: 50\n",
      "Outputs tensor([-218.2319,   -4.1970,  -23.0665,   -6.4613, -195.2319,   -8.0036,\n",
      "          -9.5943,  -80.2506,  -12.2637,   -5.7671, -470.5432,   -7.8074,\n",
      "         -11.2274,  -10.6826,    5.5424,  -11.7903, -649.3408,   -1.0331,\n",
      "        -369.6891, -129.9978], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26841.369140625\n",
      "Outputs tensor([-1.3299e+01, -2.7642e+01, -7.4724e+00, -4.0938e+00,  6.6456e-01,\n",
      "        -6.8528e+02, -1.0725e+00, -1.7633e+00, -3.2240e+02,  1.6048e+00,\n",
      "        -3.9497e+02,  3.7494e+00, -3.7116e+02,  1.3467e+00, -6.5736e-01,\n",
      "        -3.7370e+02, -4.4388e+00, -8.9179e+00, -5.9868e-01,  6.3323e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29633.177734375\n",
      "Outputs tensor([-9.6157e-02, -5.9121e+00, -2.9464e+02, -8.0421e+00, -4.5727e+00,\n",
      "        -2.7477e+02, -4.4408e+02, -2.4297e+02,  1.2306e+01, -5.1533e+00,\n",
      "        -5.0726e+00,  4.3189e-02,  4.5732e-01, -4.5963e+02, -6.8899e+00,\n",
      "        -7.6837e+00, -5.8501e+00, -9.9941e-01, -1.7993e+01, -3.9293e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46653.3203125\n",
      "Outputs tensor([-613.3620,  -10.0820, -164.1990,  -14.6764, -159.2104,  -20.2643,\n",
      "        -361.5883,  -11.2374,   -5.9404,   -3.9130,   -6.2249,   -1.7836,\n",
      "          -5.5357,   -2.7767, -316.7747, -563.1099, -490.7543, -484.3322,\n",
      "           1.5817,   -5.4008], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29439.546875\n",
      "Outputs tensor([-123.0097,   -3.0559, -118.5573, -385.4158,   -9.2736,   -4.7980,\n",
      "        -768.3630,   -6.7206,  -11.7851,  -21.9848, -750.7874,   -4.2730,\n",
      "         -19.9232,    0.7980, -623.2554,   -5.4356, -150.8315, -754.8747,\n",
      "         -17.9133,  -60.9338], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37003.984375\n",
      "Outputs tensor([   3.9315,   -8.3374,  -14.6272,   -3.4185, -433.7787,  -25.7973,\n",
      "        -189.0738,   -6.6030,    6.3192,   -5.1500, -493.7970,    1.6279,\n",
      "          -2.9528, -530.8217,  -18.7525,    1.4634,  -73.5656,    5.1734,\n",
      "          -3.1481,   -2.6673], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42019.109375\n",
      "Epoch training loss: 35265.084635416664\n",
      "Epoch validation loss: tensor([[30516.2520]])\n",
      "\n",
      "On epoch: 51\n",
      "Outputs tensor([  -1.6378, -336.0329, -598.0963, -499.4319, -468.8094,   -6.4576,\n",
      "           2.2247, -359.7244,   -4.8384,  -11.3783,  -23.9113,   -6.3817,\n",
      "         -11.2816,   -2.3219,   -8.3065,   -8.8681,   -2.4293,   -0.6451,\n",
      "        -494.9708,  -33.8836], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34509.35546875\n",
      "Outputs tensor([ -57.8095,   -9.3121,  -11.8849, -176.5570,   -4.6029, -275.3518,\n",
      "        -547.4404,  -26.8027,   -7.0832, -382.8188,  -40.7332,   -4.6070,\n",
      "         -11.1888,  -11.2702, -383.5067,   -6.1765,  -41.5449,  -20.6750,\n",
      "        -557.1716, -555.0201], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25822.33203125\n",
      "Outputs tensor([  -6.8958,   -4.9129,   -8.2121, -370.0038,  -52.2830,  -58.9660,\n",
      "         -11.7030,   -7.2336,    3.4387, -627.0800,  -11.6788,  -11.3560,\n",
      "          -5.2697, -502.3214, -185.1909,  -11.2821,   -5.4891, -161.6509,\n",
      "         -47.1133,  -30.4628], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 64893.2734375\n",
      "Outputs tensor([-173.3230,   -7.4115, -151.0659,   -4.6462, -236.3405,    7.7974,\n",
      "          -6.4249,    6.2061, -428.1920,    3.6876, -362.3737, -335.9263,\n",
      "         -43.4419, -419.4446,    5.6151,    8.0137,    6.5568,   -4.4441,\n",
      "           5.3204,    5.1236], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40301.5546875\n",
      "Outputs tensor([-4.6046e+00, -2.2446e+01, -2.5539e+02, -2.1865e+01, -1.4256e+00,\n",
      "        -7.6681e+00,  3.1635e-01, -6.1824e+02, -6.3727e+02, -5.1500e+01,\n",
      "        -6.1784e+02,  3.1473e+00, -2.1548e+01,  6.0379e-01, -2.9515e+01,\n",
      "        -9.6318e+00, -2.4204e+01, -4.0124e+01, -2.2106e+01, -3.1897e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28389.724609375\n",
      "Outputs tensor([-4.3709e+00, -5.2549e+02, -6.7121e+00, -3.9490e+02, -5.0313e-01,\n",
      "        -6.6177e+00, -1.8520e+02,  3.8879e+00, -2.2088e+02, -7.6441e+00,\n",
      "        -3.7056e+02, -7.5678e+00, -1.1706e+01, -5.1187e+00, -9.5153e+00,\n",
      "        -5.6103e+00, -5.5295e+02, -6.6860e+00,  4.4442e+00, -1.1634e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40496.58984375\n",
      "Epoch training loss: 39068.805013020836\n",
      "Epoch validation loss: tensor([[35468.9414]])\n",
      "\n",
      "On epoch: 52\n",
      "Outputs tensor([-449.4260, -383.2786,   -0.6867,    2.9501,   -6.0455,   -9.8150,\n",
      "         -11.2337, -410.2195,  -10.0456,    1.2890,   -7.3426, -310.0235,\n",
      "          -4.4431,   -1.8890, -232.4798,    3.0842, -332.4890, -303.3186,\n",
      "           6.0909,   -5.6834], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26679.900390625\n",
      "Outputs tensor([-305.8454, -120.7820,   -4.8434,   -2.8604,  -24.9442,   -6.1617,\n",
      "        -450.2463,   -9.9126, -362.5021,  -11.0694,  -20.7940, -625.0925,\n",
      "          -7.4934,   -2.8852,   -1.7274,   -0.9673, -634.3990,  -15.2785,\n",
      "        -226.6496,   -5.2975], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37834.7109375\n",
      "Outputs tensor([-1.0907e+01, -3.5200e+02, -2.6687e+01, -1.8069e+01, -3.5272e+02,\n",
      "        -1.6464e+00, -8.3034e+00, -9.7462e+00, -8.8838e+00,  3.8107e-01,\n",
      "        -3.3710e+00, -3.1852e-01, -4.8635e+01, -1.4758e+01, -4.1638e+02,\n",
      "        -1.1297e+01, -2.5121e+01, -1.5927e+01, -8.2974e+00, -2.7439e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39752.70703125\n",
      "Outputs tensor([-7.0737e-01, -4.9522e+02, -5.6651e-01,  3.7187e-01, -5.0847e+00,\n",
      "        -1.4202e+02, -1.1565e+01, -4.3160e+00, -3.9813e+00, -1.7629e+01,\n",
      "        -1.2076e+01,  6.2349e-01, -1.1077e+01, -7.7140e+00, -6.2232e+02,\n",
      "        -4.4812e+02, -1.1477e+00, -3.6527e+00, -6.1736e+00, -1.9788e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35713.96484375\n",
      "Outputs tensor([  -4.6735, -201.4068, -157.6247, -334.8823,   -4.5312, -351.8619,\n",
      "        -151.4928,    7.5802,   -8.2459,  -19.9963,   -4.0841,   -5.1876,\n",
      "           2.9108,    9.0199,   -4.6333, -122.1430,    6.3424,    6.6122,\n",
      "          -7.2481, -411.2774], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 54004.25\n",
      "Outputs tensor([ -23.3353,  -10.3396, -433.9003, -467.9871, -586.9266, -238.8743,\n",
      "         -16.5665, -248.4339,   -6.4076,  -18.9904,   -6.4544,  -23.9447,\n",
      "        -430.4345,  -11.8407,  -11.0793, -569.0425,  -15.5218,   -6.4224,\n",
      "        -499.3826,   -2.2954], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26492.443359375\n",
      "Epoch training loss: 36746.329427083336\n",
      "Epoch validation loss: tensor([[30195.6387]])\n",
      "\n",
      "On epoch: 53\n",
      "Outputs tensor([-2.2065e+01, -1.3095e+02, -2.4845e+02, -6.6209e+02, -1.0341e+02,\n",
      "        -3.4350e+00, -1.4826e+00, -3.0948e+02, -1.7053e+01, -1.5012e+01,\n",
      "        -7.1526e+00, -9.8730e+00, -1.9046e+01, -3.0792e+01, -5.8307e+02,\n",
      "        -2.1082e-01, -1.7819e+02, -6.7700e+00, -9.3597e+00, -6.0589e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 56620.2890625\n",
      "Outputs tensor([-2.9858e+00, -2.7257e+02, -1.0367e+01,  1.5754e+00, -2.0214e+00,\n",
      "        -6.6561e+00,  1.7063e+00, -2.7004e+01, -4.5503e+02, -4.5483e+02,\n",
      "        -7.8422e+00, -2.2872e+02, -1.2684e+01, -7.5101e+00,  3.6862e-01,\n",
      "        -1.2648e+01, -6.5451e+00, -4.9890e+00, -2.3618e+02, -1.2004e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27658.865234375\n",
      "Outputs tensor([  -7.0397, -512.9884,  -12.4035,  -11.8143,  -19.0724, -293.9596,\n",
      "         -19.6984,   -3.9278, -402.2271,   -3.8182, -432.2398,   -2.0306,\n",
      "         -15.3632,  -43.2326, -656.3302,   -2.7946,   -1.9085,  -15.9447,\n",
      "          -6.0839,   -1.4750], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41004.33203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([  -4.5068,   -5.5854, -496.4761,   -6.0980,  -50.2105,  -11.8832,\n",
      "         -13.3659,  -15.3531, -531.8837,   -8.5879,  -50.9073,   -9.0393,\n",
      "        -537.9312,  -24.4604,   -1.5558,  -59.9677,  -58.1518,  -11.5190,\n",
      "         -15.3587, -300.5883], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26706.71484375\n",
      "Outputs tensor([-1.2261e+01, -2.4062e+00, -4.7480e-01, -6.5167e+00, -6.6575e+01,\n",
      "        -5.9670e+00, -2.2602e+02, -1.0830e+01, -6.3663e+00, -2.3617e+02,\n",
      "        -2.2823e+02, -5.8477e+02, -1.0265e+00, -1.2401e+01, -7.7277e+00,\n",
      "        -5.0797e+00, -7.4261e+00, -1.9044e+01, -3.0343e+02, -4.1726e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41207.25\n",
      "Outputs tensor([ 6.9172e+00, -3.6305e+02,  5.6733e+00, -3.1150e+00, -4.9298e+02,\n",
      "        -5.7286e+00,  4.0441e+00, -4.2350e+02, -4.6152e+02,  7.7502e+00,\n",
      "        -7.8532e+00, -2.6002e+02, -3.1252e+02, -3.3625e+02,  1.9834e+00,\n",
      "        -4.4956e+02, -5.1218e+00, -1.1778e-01,  3.0031e+00, -3.1916e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35397.0546875\n",
      "Epoch training loss: 38099.084309895836\n",
      "Epoch validation loss: tensor([[33438.4180]])\n",
      "\n",
      "On epoch: 54\n",
      "Outputs tensor([ -13.9215,  -20.8725,  -12.1908,  -35.4997, -593.0745,  -27.2077,\n",
      "        -222.9443,   -3.6666,   -3.4575,   -7.4028,   -8.1076, -441.0997,\n",
      "         -21.8995, -475.9868,  -13.1713,   -0.7387, -361.2249, -644.0565,\n",
      "         -11.7803, -371.0615], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39843.73828125\n",
      "Outputs tensor([ -10.8647,   -2.1874,   -6.1712,   -2.9832,   -6.1507,  -51.3288,\n",
      "         -16.3196,   -5.0733,   -6.5441, -245.7504, -334.1934,  -10.4163,\n",
      "          -8.5673,  -11.0685,   -4.0485,   -1.9858, -473.7677,   -8.4648,\n",
      "          -5.2161,   -2.3038], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 51117.55859375\n",
      "Outputs tensor([-8.5872e+00, -4.6900e-01,  5.9112e+00, -5.5459e+00, -6.0079e+02,\n",
      "        -6.2072e+01, -5.1572e+02, -1.0895e+02,  3.4917e+00, -6.8115e+00,\n",
      "        -5.4799e+00, -1.2137e+01, -6.9514e+01, -4.2611e+00, -1.8167e+00,\n",
      "        -1.8004e+00,  7.8354e+00,  4.3522e+00, -8.9266e+00, -2.9252e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 21210.24609375\n",
      "Outputs tensor([ 1.4986e+00, -4.2591e+02,  1.1058e+00, -4.4877e+02, -5.7706e+00,\n",
      "        -1.0532e+01,  3.8514e+00, -4.1888e+02,  8.9871e-01, -7.9722e+01,\n",
      "        -1.9360e+02,  2.5311e-01, -5.4401e-01, -1.9075e+00,  2.1369e+00,\n",
      "        -5.2222e+00, -7.2396e+00, -1.1914e-01, -4.0558e+02,  5.9769e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42168.41015625\n",
      "Outputs tensor([-1.2346e+00, -1.2656e+01, -3.8768e+02, -1.6917e+01, -1.2485e+00,\n",
      "        -1.0819e+01, -1.2781e+00, -2.7005e+02, -3.8657e+02, -8.5994e+00,\n",
      "        -5.4540e+02, -2.4538e+00, -4.2439e+00, -1.2430e+01, -7.1640e-02,\n",
      "         1.7628e+00, -5.8740e+02, -1.2228e+01, -5.1708e+02, -9.1900e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50362.98828125\n",
      "Outputs tensor([-3.4593e+02, -2.1541e+02,  7.4989e+00, -3.1591e-01,  1.2607e+00,\n",
      "        -3.3525e+02, -9.4103e+00, -6.9558e+00, -1.0118e+02,  3.9296e+00,\n",
      "        -3.3844e+00, -3.5750e+02, -5.0858e+02, -1.3242e+00, -3.6337e+02,\n",
      "        -4.1845e+02, -4.1047e+02,  3.6263e+00,  4.4482e-01, -3.3276e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29466.921875\n",
      "Epoch training loss: 39028.310546875\n",
      "Epoch validation loss: tensor([[35387.3750]])\n",
      "\n",
      "On epoch: 55\n",
      "Outputs tensor([ -83.9311, -666.8486,  -18.7108,  -13.9759,    0.6917, -596.4822,\n",
      "        -188.3736,  -64.7121,   -5.0827,  -39.9977, -502.7202, -114.7324,\n",
      "        -667.2892,   -9.6683, -303.6009,  -46.9258,  -19.9344,   -4.2028,\n",
      "         -35.2447,  -11.0567], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37989.7421875\n",
      "Outputs tensor([-506.8868, -467.6598,   -8.2865,    9.5360,   -8.9745,   -1.8956,\n",
      "        -344.5825, -259.8581,    4.9465,   -5.1598, -466.0469,    4.0863,\n",
      "           2.7012,  -45.3273,   -8.5505,  -93.0964,    6.8543, -351.3395,\n",
      "          -1.5056,    6.2117], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36262.6796875\n",
      "Outputs tensor([-242.1749, -324.4376,  -11.8908,  -14.8442,   -4.3677,   -5.6954,\n",
      "         -65.2004, -173.6412,   -2.0253, -541.2114,  -12.5681, -605.1447,\n",
      "         -48.4762,  -11.8817,   -6.6717,  -12.1593, -420.4478,  -12.3041,\n",
      "         -15.1994, -130.5590], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45390.41796875\n",
      "Outputs tensor([  -7.6608,  -15.1677, -432.1898,  -12.8989,   -6.9953,  -15.7990,\n",
      "         -11.8783, -667.5251, -415.0894,  -44.9379,   -6.6489,   -8.8635,\n",
      "          -4.9711,   -2.0326, -385.2557,   -7.8534,  -10.0144,  -15.4805,\n",
      "        -576.1537,  -11.7325], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31486.84375\n",
      "Outputs tensor([ -10.9348,   -1.5875, -283.3956,  -14.1100, -311.1270, -539.3297,\n",
      "         -12.8529,  -16.3663, -130.4319, -518.6734,   -1.5632,   -7.1603,\n",
      "          -8.6112, -469.3668,  -13.7832,   -5.3027,   -2.3869, -418.3941,\n",
      "         -13.2853,   -5.1931], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25033.79296875\n",
      "Outputs tensor([ -56.2401,  -69.6331,  -57.8363, -151.3482,   -9.4773,  -13.7672,\n",
      "         -16.9660,  -12.2782,  -13.7778, -121.4489,   -8.0833, -670.2998,\n",
      "         -14.4799,  -15.1960,  -12.0045,   -9.3898, -425.9448, -674.0126,\n",
      "          -6.6267,   -8.0526], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32017.837890625\n",
      "Epoch training loss: 34696.8857421875\n",
      "Epoch validation loss: tensor([[34824.7578]])\n",
      "\n",
      "On epoch: 56\n",
      "Outputs tensor([ -22.4716, -637.1631,  -27.0406, -548.0999,  -14.4326,    1.2869,\n",
      "           0.8874, -256.4652,   -9.3626,   -4.9819,  -15.9906,   -1.7524,\n",
      "          -6.1065,  -44.3612,  -17.5416, -132.4581,  -11.2510,  -11.4191,\n",
      "        -696.6053,  -94.0949], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46070.6953125\n",
      "Outputs tensor([-2.7477e+00, -8.8913e+01, -3.7826e+00, -3.4173e+00, -4.2968e+02,\n",
      "         1.5595e+00,  6.6629e+00,  2.7134e+00,  1.3198e+00, -4.6167e+00,\n",
      "        -2.2250e+02, -1.2030e+00,  4.1787e+00, -4.6236e+02, -4.3418e+02,\n",
      "        -3.1117e+00, -3.0856e+02, -9.8871e+00,  1.9604e-01, -3.7184e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35173.5\n",
      "Outputs tensor([ -14.4537, -522.4744,  -71.2856, -319.9628,  -82.2382, -237.8624,\n",
      "         -11.8599,   -7.7679, -630.3318,  -11.6227,  -14.2493, -296.0414,\n",
      "         -73.9977,  -10.6386,  -48.0294, -270.3228,   -5.6208,  -11.0427,\n",
      "          -5.1978,   -8.8240], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 52998.8046875\n",
      "Outputs tensor([-591.4935,   -1.2298,   -0.8617, -452.0343,   -9.5473,   -1.3175,\n",
      "        -259.0164, -456.4296,    1.5070,  -10.8483, -360.3312,   -1.2589,\n",
      "           3.6231,   -2.7472,  -52.2029,    0.9039, -336.3070,  -14.5942,\n",
      "          -8.5699,    1.9179], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49824.54296875\n",
      "Outputs tensor([ -18.0298,  -12.3503,  -10.0388, -344.7482,   -8.5395, -156.4230,\n",
      "         -14.4836,  -12.0518, -643.7822,  -16.5345,  -32.6429, -395.7389,\n",
      "         -62.1194,   -8.4356,  -15.9626, -137.2514,  -22.4243,  -21.3141,\n",
      "         -10.7592,  -10.5831], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31389.578125\n",
      "Outputs tensor([   4.6586,    1.6953, -400.6013,   -5.0936,   -5.8688, -641.8474,\n",
      "          -1.4624,   -2.6054,   -9.0120,   -4.5745, -379.2426,   -0.8722,\n",
      "          -9.8325,  -16.0110, -241.6000, -538.1867,   -1.0664,    1.6444,\n",
      "        -225.6610,   -2.6494], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25498.505859375\n",
      "Epoch training loss: 40159.271158854164\n",
      "Epoch validation loss: tensor([[35396.0703]])\n",
      "\n",
      "On epoch: 57\n",
      "Outputs tensor([-1.8325e+00, -1.2345e+01, -1.2398e+01, -1.0519e+01,  2.0929e-01,\n",
      "        -4.0083e+00, -5.5674e+02, -5.5732e+02, -1.3731e+01,  3.2542e+00,\n",
      "        -1.5709e+00,  1.0900e+00, -4.9010e+00,  4.5158e-01, -5.8124e+02,\n",
      "        -9.3651e+00, -3.4166e+00, -2.2098e+02, -1.1489e+01, -1.0317e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30170.796875\n",
      "Outputs tensor([  -2.0243,  -14.9275,  -12.0459,   -9.8418,   -1.6868,   -7.8993,\n",
      "         -17.1915,   -1.2793,  -72.8282,   -4.3573, -583.6783,   -1.9865,\n",
      "           1.6101, -304.4470, -439.0371,    5.5174,  -12.0096, -360.9388,\n",
      "        -487.2411,  -16.7397], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 20798.998046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([  -3.5452, -367.3004,   -3.0464,  -10.6052,  -16.7272, -323.8388,\n",
      "          -4.9891, -535.2901,   -5.9745,   -4.4095, -564.6915,  -18.2104,\n",
      "         -15.5792,   -3.6166, -356.8993,  -10.1988, -484.2930,  -56.6087,\n",
      "        -479.6425,   -2.2527], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33612.69921875\n",
      "Outputs tensor([ -70.5019, -182.0850, -755.6462,  -12.5467,  -82.7348,  -80.0297,\n",
      "        -230.7355,  -16.2710,  -77.6404, -213.7679, -531.6067,  -16.1323,\n",
      "         -16.0172,  -14.5468, -539.0814, -543.4891,   -1.7145,   -1.4561,\n",
      "         -14.5389,   -7.4904], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23835.69921875\n",
      "Outputs tensor([ -11.6320,  -11.6610, -173.0705, -516.4378,   -5.0731,   -9.1032,\n",
      "        -172.7028,  -10.4171,  -24.9272,    2.1925, -545.9303,  -14.4566,\n",
      "          -7.5534,  -40.1575,   -6.1002,  -20.8393,   -7.1177,   -4.8885,\n",
      "          -3.0242, -665.0463], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 68273.3984375\n",
      "Outputs tensor([  -0.7568,   -7.9137,   -6.8261,    2.2814, -335.0313,    7.4118,\n",
      "        -325.0240,   -0.6397,   -5.0230,   -6.2509,   -0.3409,    5.9520,\n",
      "           0.7943, -163.4385,    3.1352,  -15.7695,    6.2245, -243.9361,\n",
      "          -1.9265, -218.8083], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37205.0234375\n",
      "Epoch training loss: 35649.435872395836\n",
      "Epoch validation loss: tensor([[35435.8320]])\n",
      "\n",
      "On epoch: 58\n",
      "Outputs tensor([-2.8418e+00, -1.1185e+01, -4.2990e+02, -7.6145e-01, -1.4334e+00,\n",
      "        -4.3319e+02,  2.9095e+00, -7.5797e+00, -4.1332e-02, -7.9718e-01,\n",
      "         2.0717e+00, -4.7817e+02, -2.3027e+00, -4.5396e+02, -9.2400e-01,\n",
      "        -4.5047e+02, -5.1018e+01, -2.6578e+02, -6.5721e+00, -1.2264e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31617.87109375\n",
      "Outputs tensor([-5.6813e+02, -2.4009e+00, -9.1622e+00, -3.5300e+00, -1.0945e+01,\n",
      "        -2.5801e-01, -1.2029e+01, -1.6726e+01, -4.4235e+00, -1.0249e+02,\n",
      "        -7.8426e+00, -1.1145e+01, -1.1128e+01, -5.3264e+02, -9.3292e+00,\n",
      "        -4.7543e+02, -9.7778e+00, -4.7202e+02, -4.8395e+00, -3.1903e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26505.275390625\n",
      "Outputs tensor([-436.4088,  -33.9806,  -14.6493,  -11.2665,  -22.3152,   -5.1868,\n",
      "          -4.9600,  -12.0753,  -22.1239,   -6.3689,  -14.2518, -390.2558,\n",
      "          -4.3544,  -81.6634, -501.2434,   -9.6100,  -12.7546,  -42.0143,\n",
      "        -526.5691,  -18.3566], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36468.60546875\n",
      "Outputs tensor([   3.0637,  -78.4537,   -6.8234,   -3.6587,   -9.0633,   -2.9636,\n",
      "        -388.0271,    0.9197,  -15.1313,   -7.8171, -639.2054,    3.3307,\n",
      "           2.5646,  -10.0806,   -9.9306,  -70.6769, -187.4194, -280.5544,\n",
      "        -492.6759, -401.9749], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28925.384765625\n",
      "Outputs tensor([  -4.0837, -607.3704,   -9.4446, -552.3549, -607.8209,  -12.1927,\n",
      "         -16.4251,   -5.9294,   -4.1271,  -10.0634,   -7.2450,   -7.9428,\n",
      "        -456.3355,   -5.1499,   -5.1986,   -6.3736,  -12.0087, -525.9398,\n",
      "          -8.9923,  -29.9379], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32839.8515625\n",
      "Outputs tensor([-333.4514,   -8.0052,  -23.5970,  -16.1784, -217.4866,   -9.2864,\n",
      "        -561.9961,   -6.4768,  -16.3555, -549.6736,   -9.7316,   -3.6595,\n",
      "         -15.5361,  -27.0279,   -5.5636,   -8.6735, -387.7199,   -5.5007,\n",
      "        -222.2203,  -14.0460], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44688.7578125\n",
      "Epoch training loss: 33507.624348958336\n",
      "Epoch validation loss: tensor([[32565.1738]])\n",
      "\n",
      "On epoch: 59\n",
      "Outputs tensor([ -14.8701,    1.3381,    4.3461,  -26.0067, -413.1870,   -4.6700,\n",
      "        -340.0456,   -4.4042,   -1.4150,   -6.8138,  -11.1902, -204.9830,\n",
      "        -381.6563,   -3.3521,  -10.3977,   -9.0821, -111.8088,  -10.8106,\n",
      "         -12.8984, -539.9925], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37036.546875\n",
      "Outputs tensor([-1.4251e+01, -1.5556e+00, -1.3441e+01, -4.3102e+02, -1.1460e+01,\n",
      "        -8.0779e+00,  1.9390e-01, -1.4603e+01, -2.2974e+00, -6.7956e+00,\n",
      "        -3.3751e+00, -4.4643e+02, -7.3869e+00, -2.1417e+01, -1.4312e+01,\n",
      "        -2.3097e+00, -3.6236e+02, -1.0665e+01, -9.9384e+00, -1.0068e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31944.63671875\n",
      "Outputs tensor([-3.1292e+02, -4.2742e+02, -1.0416e+01, -1.0256e+01, -2.0544e+00,\n",
      "        -8.0340e+00, -1.8354e+02,  1.7057e+00,  2.1192e-01, -1.0353e+01,\n",
      "        -3.6545e+02, -1.1283e+01, -5.1607e+02, -1.3005e+01, -1.3377e+01,\n",
      "        -2.0269e+00, -4.8495e+02, -3.9201e+00, -1.3595e+01, -9.3801e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32174.818359375\n",
      "Outputs tensor([-6.0502e+00, -8.4712e+00, -1.5259e+01, -1.9777e+01, -4.8817e+02,\n",
      "         1.8548e-01, -1.1499e+01, -1.6347e+02, -1.7822e+01, -5.7130e+02,\n",
      "        -4.6676e+02, -1.8514e+01, -6.5758e+00, -9.2615e+00, -2.3476e+01,\n",
      "        -3.1423e+02, -3.9225e+02, -7.3593e+00, -1.3154e+01, -5.3417e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27937.33984375\n",
      "Outputs tensor([-217.9766, -552.7166,  -15.9693,  -15.4790,   -5.5585,  -11.3849,\n",
      "         -18.8277, -486.9848,  -20.4463, -605.1707,  -59.4949,  -21.9815,\n",
      "        -590.2906, -146.9051,   -6.6407,   -8.1089,  -13.7868,  -11.1380,\n",
      "          -6.9816,  -61.5248], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31154.85546875\n",
      "Outputs tensor([-551.2769, -143.2554, -101.1761,  -10.0634,  -15.9421,  -13.7940,\n",
      "         -26.1982, -630.8813, -588.9893, -377.5328,   -9.6852,   -2.1644,\n",
      "         -14.3990,   -8.7075,   -5.7499,   -2.1066,  -13.0515,  -13.3062,\n",
      "          -7.2459,   -1.3869], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38868.15625\n",
      "Epoch training loss: 33186.058919270836\n",
      "Epoch validation loss: tensor([[33142.1094]])\n",
      "\n",
      "On epoch: 60\n",
      "Outputs tensor([-600.5154,  -37.6550,   -9.8424, -337.2957, -188.3492,   -5.5591,\n",
      "         -66.1145,   -5.1708,  -53.1980,   -4.7424,  -67.3420,  -10.0450,\n",
      "          -4.4300,  -11.0885,  -10.4299, -196.4731, -520.8135, -513.8942,\n",
      "         -17.5277, -143.2027], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28372.109375\n",
      "Outputs tensor([-6.3113e+02, -1.0614e+01, -2.4184e+02, -5.5319e+01, -4.3325e+00,\n",
      "        -1.1354e+01, -1.5815e+01,  1.3999e+00, -5.6562e+00, -1.1605e+01,\n",
      "        -2.5316e+00, -1.2370e+01,  7.8896e-01, -5.2791e+00, -6.4220e+02,\n",
      "        -4.6199e-01, -1.7615e+00, -5.0705e+02, -5.7215e+02, -5.6032e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30535.787109375\n",
      "Outputs tensor([-6.6513e+00,  5.7922e-01, -4.5828e+02, -4.3459e+02, -5.2260e+02,\n",
      "        -2.2357e+00, -1.3602e+01, -2.8849e+02, -9.5282e+00, -1.0238e+01,\n",
      "        -1.5210e+01, -1.5150e+01, -1.1943e+01,  4.1227e-01, -2.8719e+01,\n",
      "        -5.1759e+00, -6.2321e+00, -9.1467e+00, -3.4372e+02, -5.4474e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35479.73046875\n",
      "Outputs tensor([-455.6434,   -3.6216,   -6.6844, -342.9463,  -12.7307,   -5.0860,\n",
      "        -493.4099,  -13.2874,  -12.9204,  -32.7987,    0.5899,  -33.3493,\n",
      "        -589.5695,   -4.6550, -279.5717,  -27.9031, -576.4817,  -23.5726,\n",
      "         -11.5597,    1.9660], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24310.083984375\n",
      "Outputs tensor([ -10.6991, -629.7007,   -7.2092,  -18.4452,  -13.5567,  -15.1572,\n",
      "         -18.1287,  -55.8719,  -14.0665, -608.0505, -155.3850,  -17.4483,\n",
      "         -17.9108, -443.6780,  -17.6050, -339.8777, -558.3573, -641.4816,\n",
      "         -10.0165,   -6.4136], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42932.9140625\n",
      "Outputs tensor([-3.6660e-01, -4.8522e+02,  2.1824e+00,  2.6329e-01, -2.4909e+01,\n",
      "        -3.7157e+02, -6.1705e+00, -1.2581e+01, -1.2604e+02, -2.9658e+02,\n",
      "        -4.6343e+00, -8.1081e+00, -9.0621e+00, -4.9496e+00, -4.8522e+00,\n",
      "        -2.1667e+00, -2.3433e+01, -5.3982e+02, -1.2386e+02, -7.1374e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35014.22265625\n",
      "Epoch training loss: 32774.141276041664\n",
      "Epoch validation loss: tensor([[33668.7656]])\n",
      "\n",
      "On epoch: 61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([-4.6756e+00,  3.1295e+00, -1.9327e+00, -4.5329e+01, -2.3032e+01,\n",
      "        -4.2463e+00, -1.0727e+02, -4.2673e+00, -2.3525e+00, -5.1801e+02,\n",
      "         7.0495e-03, -1.1459e+01,  1.2334e+00,  3.4945e+00, -2.6054e+01,\n",
      "        -8.6599e+00, -9.6343e-01, -3.8315e+02, -1.4902e+01, -1.2253e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37864.96875\n",
      "Outputs tensor([-287.7697, -237.2903,   11.5955, -325.9080,   10.3296,    6.0078,\n",
      "        -367.7769,    2.3968,   10.5195,    8.6640,    3.9874,    7.5279,\n",
      "          -2.3559,   -0.5636,   -1.9128, -264.8791, -435.0136,   -4.3686,\n",
      "           4.1273,    7.1643], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28672.76953125\n",
      "Outputs tensor([  -7.0488, -162.8821,  -48.2804,    6.6243,    6.9861, -251.3442,\n",
      "        -431.9784,    1.5011, -247.9868,  -10.1419,   -4.1219, -222.5540,\n",
      "         -11.8948, -296.7994, -211.0911,   -6.0883,   -6.7596,    7.9223,\n",
      "           4.9845,   -8.0881], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 54433.7890625\n",
      "Outputs tensor([-135.2348,  -17.4641, -209.4210,  -21.1875,  -54.1376,  -14.4846,\n",
      "        -245.1271, -618.9339,  -14.2060, -632.1141, -721.2031,  -21.1569,\n",
      "         -22.6512,  -54.2315,  -21.0181, -331.9396,  -18.5983,  -18.7256,\n",
      "         -85.2286,  -20.5647], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28181.69140625\n",
      "Outputs tensor([ -95.7416,  -10.2953,  -14.1060,  -73.0741, -104.7255,  -18.9460,\n",
      "        -426.0911,   -7.2736, -534.3686,  -10.0916,   -7.9685,   -5.1729,\n",
      "        -673.2015, -667.0309,   -9.7599,  -28.5938,  -23.3195,  -16.3300,\n",
      "          -6.1676, -197.1854], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27556.490234375\n",
      "Outputs tensor([  -5.1742,  -11.2197,   -9.5659, -243.0084,  -13.0863, -183.4526,\n",
      "          -4.1113, -550.8014,  -13.0208,   -7.9801,   -0.7526,  -20.6883,\n",
      "        -489.9735,   -6.7549, -301.1629, -220.9012,  -14.9774,    0.8234,\n",
      "          -7.5989, -246.0995], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 71262.8359375\n",
      "Epoch training loss: 41328.757486979164\n",
      "Epoch validation loss: tensor([[37747.8672]])\n",
      "\n",
      "On epoch: 62\n",
      "Outputs tensor([  -8.2002,    7.1291,   -5.1650,   -8.4174,    5.0358, -110.7764,\n",
      "        -390.6269,    1.7635,    4.3669,    5.5940, -379.1471,   -1.4909,\n",
      "        -518.4220,   -2.0700, -165.2883, -448.9371,   -9.3611, -287.3451,\n",
      "          -8.1702,    3.3768], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36671.03515625\n",
      "Outputs tensor([-204.8409,  -20.4423, -592.6877, -237.6368,  -20.6862,  -15.9727,\n",
      "          -9.6216,  -21.5293,  -12.3957,  -10.3469,   -8.3701,  -15.9706,\n",
      "         -14.7169, -276.4313,   -9.0543, -355.4398, -534.3904,   -5.9853,\n",
      "         -14.2966,   -6.7202], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33022.30078125\n",
      "Outputs tensor([ -16.1773, -101.7030,  -71.2338,  -13.0599, -683.0901,   -6.5688,\n",
      "        -117.0085,  -11.1390,  -14.7836,  -11.9354, -102.1221, -597.8482,\n",
      "        -179.2348,  -12.4391,   -7.3392, -377.4557,  -10.2870,  -15.6267,\n",
      "          -6.4349,  -14.9942], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26465.958984375\n",
      "Outputs tensor([  -4.4019,   -1.6094, -502.5392,   -9.0062, -464.7229,   -3.7286,\n",
      "          -2.8097, -251.4709,   -3.5803,   -5.0623,   -8.6586,   -3.8106,\n",
      "        -586.3243,  -12.0007,  -17.5841,   -6.2554,  -10.8797, -150.6698,\n",
      "        -409.9212,   -2.9640], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36349.7890625\n",
      "Outputs tensor([-524.6746, -350.5164,   -2.6797,  -16.1801,   -9.8764, -136.9230,\n",
      "        -571.5834,   -5.1225,  -15.9547,   -2.9494,   -1.1380, -404.0150,\n",
      "          -7.8798, -416.2493,  -12.6371,   -5.6290, -472.8661,   -4.6525,\n",
      "         -14.8519,   -5.8789], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29861.353515625\n",
      "Outputs tensor([-539.2428,  -10.1346,    5.1905,   -1.7043,  -17.7567,  -12.0947,\n",
      "          -5.7241,   -9.2012, -381.2855,    0.6903, -321.3243,    0.6800,\n",
      "          -3.9049,  -29.1178,  -15.9017, -493.3044,  -15.7396, -528.6840,\n",
      "          -5.4516,  -17.3638], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38980.75\n",
      "Epoch training loss: 33558.53125\n",
      "Epoch validation loss: tensor([[34936.4180]])\n",
      "\n",
      "On epoch: 63\n",
      "Outputs tensor([  -2.3998,  -10.9509,   -6.8568,    1.0641,   -5.3405,   -8.5275,\n",
      "          -3.5810, -407.1654,  -18.4647,   -9.1537,    1.8832,  -11.7149,\n",
      "         -12.1214, -610.1613,   -1.7152,  -71.6029, -540.1294,  -10.1665,\n",
      "           5.7666, -190.0590], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29636.771484375\n",
      "Outputs tensor([ 3.0792e+00, -5.0172e+00, -4.6973e+02,  5.3182e-01, -3.8391e+02,\n",
      "        -3.7911e+00, -1.9527e+02, -1.3459e+01, -6.5111e+00, -3.6312e+02,\n",
      "        -1.9263e+02, -1.6327e+01, -5.5700e+02, -7.1351e+00, -1.4307e+01,\n",
      "        -7.0120e+00, -4.8418e+02, -8.4059e+00, -1.2155e+01, -6.2081e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25530.427734375\n",
      "Outputs tensor([ -16.8490,  -20.9349, -219.1222,  -18.4825, -376.2191,   -6.9826,\n",
      "        -664.3284,  -11.9001,   -9.9044, -313.0676,  -14.9908, -447.6588,\n",
      "         -13.8819, -101.4714,  -12.9215,  -21.4449,   -6.6358,  -18.1089,\n",
      "        -374.1286, -471.3273], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27391.552734375\n",
      "Outputs tensor([-8.4018e+00, -2.6718e+00, -7.9457e+00, -1.7663e-01, -4.5051e+02,\n",
      "        -4.9121e+02, -2.5079e+00, -1.2753e+01,  2.2060e+00, -2.1611e+02,\n",
      "        -1.0179e+01, -2.9318e+02, -6.0556e+00, -4.3171e+00, -2.6906e+02,\n",
      "        -2.6702e+00, -8.0532e+00, -5.1619e+00, -1.1359e+02, -3.1982e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 67562.5546875\n",
      "Outputs tensor([  10.0730,    5.4092, -379.8637,   -5.7208, -352.1325,    2.9731,\n",
      "           9.6613, -304.6980,   -5.7124,    5.3859, -116.9238,    8.1023,\n",
      "           8.1570,   -5.9901,   -1.0963,   -4.5840, -208.6744, -372.0497,\n",
      "           4.3604,   -2.6428], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 57208.8515625\n",
      "Outputs tensor([ -12.6589, -530.4277,    1.5508,   -0.8463, -530.8984,   -2.1148,\n",
      "           2.4361,   -9.8154,   -1.4737, -322.2426,   -7.3917,  -13.0352,\n",
      "          -3.1256,  -12.7050,    3.0777, -478.2348, -520.2755,  -14.1264,\n",
      "          -2.8040,   -5.9316], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23253.12109375\n",
      "Epoch training loss: 38430.546549479164\n",
      "Epoch validation loss: tensor([[35533.1875]])\n",
      "\n",
      "On epoch: 64\n",
      "Outputs tensor([-1.6820e+01, -1.0147e+01, -2.4453e+00, -5.6369e+00, -4.6745e+00,\n",
      "        -1.5537e+01, -1.2097e+01, -9.4568e+01,  2.8857e+00, -2.2108e+02,\n",
      "         1.0101e+00,  5.5201e-01, -1.8365e+01, -4.2732e+02, -4.7898e+02,\n",
      "        -6.5495e+02, -1.0838e+01,  4.7928e+00, -5.4345e+00, -5.2910e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39635.00390625\n",
      "Outputs tensor([   1.9224,  -14.7218, -542.9739,   -6.1262,   -5.7692, -545.9924,\n",
      "         -10.8836,  -11.3898, -271.2798, -275.5725,   -4.7345,  -15.6519,\n",
      "         -23.9831,    4.0105,    3.6753,   -8.8807,  -15.7517,   -5.4952,\n",
      "           5.5071, -320.0906], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 56879.9375\n",
      "Outputs tensor([  -5.1373,   -1.8127, -234.5165,  -12.3185,   -9.2676,    1.1366,\n",
      "          -7.5041, -175.7722, -350.1004,  -12.6202,   -5.1386,  -23.7375,\n",
      "          -8.2556,  -11.9299,  -13.7519,    1.4828,  -13.0507,   -5.6861,\n",
      "        -368.2204, -727.3026], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 60734.53125\n",
      "Outputs tensor([ 4.2422e+00, -4.7617e+00, -3.9169e+02, -3.7405e+02, -4.3534e+02,\n",
      "         2.4941e+00,  2.6412e+00,  2.4174e+00, -9.4862e+00, -7.3366e+00,\n",
      "        -4.2576e+02, -4.9466e+00,  1.4015e+00, -8.0589e+00,  4.1187e+00,\n",
      "        -1.3678e+00, -6.6828e+00,  4.4122e+00,  1.2001e-01, -1.8723e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44789.48046875\n",
      "Outputs tensor([  -8.2854,   -3.0424,    0.7610, -265.6710,  -14.6726,    1.2137,\n",
      "         -11.1397,   -2.7191,   -6.9691, -381.0922,   -8.5866, -432.1163,\n",
      "         -10.5158,  -14.4757,  -26.4835,  -10.1479,   -8.8866,   -7.0201,\n",
      "         -15.6948,   -6.8397], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 21903.04296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([  -4.7072,    2.8745,    2.4778, -118.5042,   -7.7563, -457.5348,\n",
      "         -11.0746,    4.2561,  -10.4713,  -16.0774, -457.5016, -423.6358,\n",
      "        -377.6850,   -5.5867,   -6.6514, -369.0378,   -4.8319, -463.0456,\n",
      "           6.1814,   -1.5057], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44643.21875\n",
      "Epoch training loss: 44764.202473958336\n",
      "Epoch validation loss: tensor([[34018.2109]])\n",
      "\n",
      "On epoch: 65\n",
      "Outputs tensor([ -20.3722,  -36.9954, -103.1452,  -17.7879,  -47.7329, -789.5117,\n",
      "         -60.6680, -358.6959,  -23.0713,  -27.5360,  -19.9175, -239.8384,\n",
      "         -25.8453,  -25.4489, -808.0938,  -19.3923,  -27.8368,  -19.5820,\n",
      "         -22.9560,  -18.9787], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 79109.546875\n",
      "Outputs tensor([   6.2936,   -4.1430, -415.7975,   -5.8151,    6.8004,    7.3298,\n",
      "           9.4170,  -20.0479,  -39.4139,    8.7768,   -1.9343,    1.0459,\n",
      "        -380.9575,    6.6889,   -3.1736,   -6.8294,    2.4313,    5.2972,\n",
      "           9.3565,   -3.4442], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33873.078125\n",
      "Outputs tensor([   2.1376,    0.9268, -427.1448,    0.6336, -426.9875,   -5.8399,\n",
      "        -349.8460,  -10.5949,    0.8077,  -16.9894,   -5.6644,   -3.8983,\n",
      "          -4.3188,    2.6205,   -9.3836,    3.2174,   -1.7379,    1.5662,\n",
      "         -87.2812,   -9.1303], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42159.0078125\n",
      "Outputs tensor([-326.9649,    1.9351,   -6.4764,   -6.1964,    3.6697,  -17.3490,\n",
      "          -7.8218,  -35.8943,  -12.7512,   -4.1721, -109.9334,   -5.8670,\n",
      "        -163.2590,   -5.1760, -221.5864,   -1.5642,  -12.9172,  -18.9146,\n",
      "        -470.4937, -227.9446], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49906.04296875\n",
      "Outputs tensor([ -16.5629,  -16.0114, -285.3636, -336.8819, -106.6647,  -15.8837,\n",
      "        -648.3376,  -13.9505, -534.0269,   -8.9707,  -49.0128,  -24.0632,\n",
      "        -538.9590,  -20.0187, -146.1688,  -19.2111,  -24.1171, -206.3391,\n",
      "         -24.8251,  -18.1216], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28590.302734375\n",
      "Outputs tensor([ -92.2372, -778.4590,  -22.7805,  -36.3015, -572.8898,  -28.6493,\n",
      "         -81.0673, -797.8762,  -34.0467,  -24.5325,  -38.1210,  -47.4448,\n",
      "         -14.4858,  -26.2909, -705.3024, -648.4543,  -17.4028, -377.1214,\n",
      "         -54.1329,  -45.0331], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35988.10546875\n",
      "Epoch training loss: 44937.6806640625\n",
      "Epoch validation loss: tensor([[33489.7461]])\n",
      "\n",
      "On epoch: 66\n",
      "Outputs tensor([-105.7965,  -39.7180,  -70.7552,  -45.4538, -692.5600,  -17.7804,\n",
      "         -10.7157, -334.7012,  -13.4488, -114.1387,  -12.1777,  -15.8151,\n",
      "         -71.8603,  -13.4422,  -16.7710,  -13.5955,  -14.9645,  -17.4840,\n",
      "        -374.7885,  -12.6565], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 61183.51953125\n",
      "Outputs tensor([  -9.1111,   -2.2002, -349.3620,   -0.5343,  -21.6518,   -1.7569,\n",
      "           1.2395,   -6.4578, -262.4521, -121.5416, -284.3536,   -1.1661,\n",
      "           0.4810,   -8.5031,   -6.2379,   -2.6126, -236.1133,   -0.4691,\n",
      "        -108.9674,   -5.5515], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 53104.01171875\n",
      "Outputs tensor([  -8.6703,  -13.5910, -282.4227,  -10.0504, -560.2980, -578.1097,\n",
      "        -338.5104,  -22.6286, -324.2492, -318.2276, -463.3976,  -19.7470,\n",
      "         -19.0793,  -11.6373,  -14.0564,  -16.9456,  -16.9041,  -11.1050,\n",
      "         -19.4370, -195.0934], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39918.77734375\n",
      "Outputs tensor([ -11.7585,  -16.9970, -391.1556,  -15.7203, -751.4187,  -12.6997,\n",
      "         -21.2325,  -18.9343,  -13.5154, -133.7015, -513.7247,  -18.4172,\n",
      "         -23.1348, -387.8321,  -27.5746,  -22.2642, -231.3060, -638.1163,\n",
      "         -26.3128,  -15.9247], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40798.953125\n",
      "Outputs tensor([ -16.8224,  -10.0240,  -11.8884, -397.3934,  -15.4490,  -21.5391,\n",
      "         -53.8671, -418.0154,  -12.6567,  -13.8921, -544.5156,  -90.3388,\n",
      "        -249.4495,  -34.4094,   -8.5216, -399.6183,  -16.0063, -410.9684,\n",
      "         -14.9252,  -83.6726], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30030.4375\n",
      "Outputs tensor([-561.7261,  -19.6325,  -12.5743,   -7.2683,  -20.6409,  -12.0896,\n",
      "        -206.9213,  -17.0074,  -73.6524,  -15.7113,  -13.1938,  -12.1952,\n",
      "         -23.5649,  -10.3217, -478.3987,   -8.9449,  -22.4755, -652.4687,\n",
      "         -13.2516,  -12.1128], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26920.80078125\n",
      "Epoch training loss: 41992.75\n",
      "Epoch validation loss: tensor([[33253.7891]])\n",
      "\n",
      "On epoch: 67\n",
      "Outputs tensor([ -20.9095,   -8.2998,  -15.5982, -107.7993,  -20.1935,  -11.1812,\n",
      "        -563.4604,  -12.9347, -489.9799,  -61.8041,   -5.0716,  -16.5469,\n",
      "         -11.7928,  -16.4524,  -16.8507,  -14.0422, -444.4839,   -5.2780,\n",
      "         -10.3614, -542.8698], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32328.55078125\n",
      "Outputs tensor([ -20.6227,   -7.4204,  -13.7943, -131.5019,   -8.0259, -386.6687,\n",
      "         -19.3450, -133.4538,  -13.6726,   -8.2917,  -19.4202,   -9.1384,\n",
      "         -13.9138, -391.1120, -680.4780,   -4.7579,  -17.5052, -459.7054,\n",
      "          -9.1762,  -12.5911], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35338.6484375\n",
      "Outputs tensor([-510.2910,  -12.3827,  -16.4018,  -15.3286, -459.0395,  -18.6175,\n",
      "        -207.6203, -476.7965, -524.3964,  -13.1166,  -11.4197, -182.7706,\n",
      "         -13.1382,   -9.9657,  -11.7501,  -17.7654, -392.1734,   -8.9951,\n",
      "         -11.5368,   -9.4117], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29483.82421875\n",
      "Outputs tensor([ -37.0278,  -23.3868,  -17.6903, -332.6374,  -14.0262,  -12.5531,\n",
      "        -117.1607, -623.3397, -506.5961,  -19.9675,  -13.2349,   -9.8616,\n",
      "         -21.1738,  -15.4917, -381.8328,  -46.9221,  -13.5765, -207.3312,\n",
      "         -21.9666, -409.4133], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39250.1796875\n",
      "Outputs tensor([  -2.8493,  -10.1471,   -1.5746,   -7.2871, -424.6700,  -11.1546,\n",
      "        -395.1117,  -13.7847,  -10.7397,   -8.0791, -484.7197,  -15.9478,\n",
      "          -8.7461,   -4.5900, -420.2842,  -17.0127, -452.6363,   -5.5768,\n",
      "          -6.3461, -356.4398], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34461.23046875\n",
      "Outputs tensor([ -16.7359,  -12.5080,  -11.6224,  -14.0619,  -19.7184,  -14.0211,\n",
      "        -172.0002,  -14.3885, -329.5856, -607.7153, -460.1916,  -31.8233,\n",
      "         -12.3680, -349.3033,  -10.5722,  -63.2240,  -11.7944,  -10.3010,\n",
      "         -22.5784,  -12.4485], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32556.306640625\n",
      "Epoch training loss: 33903.123372395836\n",
      "Epoch validation loss: tensor([[35542.6680]])\n",
      "\n",
      "On epoch: 68\n",
      "Outputs tensor([ -13.0679,  -22.9500,   -8.9213,  -20.5464,  -15.0128,  -13.0780,\n",
      "          -8.2084,   -6.7895,  -11.4534, -718.3013,  -19.1849,  -17.1021,\n",
      "          -7.3773,   -8.6102, -455.2427, -457.0004,  -16.6400,   -9.1735,\n",
      "        -583.2905,   -8.8552], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40948.046875\n",
      "Outputs tensor([  -8.9476,  -28.0147,  -10.4703, -546.5436,  -16.0972,   -7.8670,\n",
      "         -16.8117, -551.9014,   -8.5997,  -11.2572,  -11.6741,   -6.4400,\n",
      "          -3.5076, -411.9720,   -8.3431,  -18.3673,   -7.6281,  -12.2407,\n",
      "        -265.7055,  -10.0606], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31301.197265625\n",
      "Outputs tensor([  -7.1782,  -18.2004, -604.3920,  -13.2812,  -12.9881,   -7.1947,\n",
      "          -8.9916,  -17.0897,  -13.4266,   -9.1236,  -12.4002,  -68.9147,\n",
      "         -18.2211, -252.1243,  -10.7719, -261.2198, -500.2937,  -11.4314,\n",
      "        -207.5200,   -3.1996], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 52383.3046875\n",
      "Outputs tensor([ -12.4834,  -12.4982,  -15.2482,  -16.7502,   -6.8203, -420.3992,\n",
      "         -20.3746,  -87.8082, -426.2053,  -11.3317,   -6.9920, -566.3901,\n",
      "        -482.5597, -246.5095,  -12.3956,  -16.2402,  -12.9207,  -68.2850,\n",
      "          -5.3001,   -9.2722], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50356.59765625\n",
      "Outputs tensor([  -5.9787, -449.2731, -352.1880,   -3.8177,  -10.8646, -188.2004,\n",
      "        -454.6687, -361.6579,  -16.6714,   -5.5935,  -15.7804,  -10.3656,\n",
      "        -360.7635,   -9.5082,   -8.1299, -357.0339, -222.8030,   -2.1910,\n",
      "          -6.9526, -324.6291], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28506.390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ -27.1727, -473.8391, -756.5338, -144.1904,  -32.5841,  -16.9914,\n",
      "         -22.8273,  -22.9708, -580.3179, -364.6509,  -17.3419,  -79.3287,\n",
      "         -17.2117,  -21.1663,  -16.4539,  -71.5574, -236.5203,  -20.6518,\n",
      "         -71.6413, -224.6246], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28096.947265625\n",
      "Epoch training loss: 38598.747395833336\n",
      "Epoch validation loss: tensor([[34513.2617]])\n",
      "\n",
      "On epoch: 69\n",
      "Outputs tensor([-545.5074, -540.5355,  -10.1724,  -19.5215,  -13.9750, -458.9507,\n",
      "         -11.5752,  -33.0957,  -11.1989,  -10.9913,  -19.4147, -255.8383,\n",
      "         -11.4266, -247.9139,  -18.7231,  -17.7748,  -14.7144,  -12.4002,\n",
      "        -475.0612,  -13.4295], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 22102.74609375\n",
      "Outputs tensor([ -13.6180, -212.5112,  -19.5207,  -20.3095, -415.0325, -106.1820,\n",
      "         -16.6218,  -79.6685,  -19.8000,  -97.0598, -642.6707,  -45.5070,\n",
      "         -12.1062,  -13.6997,  -19.0069,  -16.1204,  -92.8492,  -22.3623,\n",
      "         -13.4179, -331.5356], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37456.1640625\n",
      "Outputs tensor([  -3.8707,  -15.7449,  -11.9407, -445.7171,  -16.8410,   -6.3809,\n",
      "         -45.4431,   -8.0029,  -16.5029,  -12.6383,   -7.9040,   -7.4431,\n",
      "        -407.8279,   -8.4023, -459.8512,  -10.9497,  -10.6625,   -5.1579,\n",
      "         -14.6495,   -7.6864], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34171.6484375\n",
      "Outputs tensor([  -7.9413,   -6.1689, -485.1410,  -49.9162,  -13.8365,   -5.4882,\n",
      "         -11.4270,   -7.0011,  -15.9090,  -12.2093,  -11.5107, -104.7360,\n",
      "        -504.7659, -353.0134,  -12.0978,  -13.1621, -513.0295,  -12.5816,\n",
      "          -9.0639,   -8.6593], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31237.78125\n",
      "Outputs tensor([  -1.7452, -187.2795,  -12.5524,  -11.6916,   -8.2196,   -5.0002,\n",
      "          -3.6455,  -10.1627,   -5.7487,   -7.6179, -271.3900,   -2.1558,\n",
      "        -360.8993,   -5.9215, -421.0453, -475.0753,   -3.5706,  -13.4824,\n",
      "        -418.5357, -397.7422], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50286.26953125\n",
      "Outputs tensor([ -15.5120, -404.9655,  -19.1155, -414.4071,  -22.4658,  -27.8460,\n",
      "        -566.0990,  -35.9852,  -12.7394, -117.3146,  -11.6758, -243.7942,\n",
      "         -17.5897, -639.1917, -222.5336,  -35.9689,  -15.0054,  -12.0850,\n",
      "        -154.7075, -294.7700], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 52269.29296875\n",
      "Epoch training loss: 37920.650390625\n",
      "Epoch validation loss: tensor([[35662.3555]])\n",
      "\n",
      "On epoch: 70\n",
      "Outputs tensor([-623.5652, -295.3530,  -15.8924, -681.7681,  -12.8958,  -33.4930,\n",
      "         -13.7697,  -26.1793, -264.8958,  -20.1228, -677.3834,  -40.2078,\n",
      "         -10.9056, -120.9223,  -22.9493,  -23.7069,  -30.2382,   -9.3956,\n",
      "          -8.9061,  -16.2271], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30335.650390625\n",
      "Outputs tensor([-307.0168, -156.3243,  -12.6025,  -11.0691,   -9.6202,  -13.5964,\n",
      "         -16.8513,   -4.0343, -432.7011,   -5.0345,  -66.4219,  -11.7926,\n",
      "          -4.3942,  -11.6628,   -2.9836, -461.0958, -110.8402,  -17.0431,\n",
      "        -234.4495,   -7.6560], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50921.6796875\n",
      "Outputs tensor([ -19.4358,   -3.9288,  -19.9523,  -44.1818,   -7.2233, -603.8024,\n",
      "        -396.5797,  -10.8026,  -15.5350,  -13.1056,  -10.7211,  -20.8751,\n",
      "        -635.1024,  -17.7435,  -18.1502, -626.7649,  -14.5963,  -15.2044,\n",
      "        -527.8264, -334.8548], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24764.75\n",
      "Outputs tensor([-158.8126,  -43.3967,  -13.4989,  -43.2675,  -18.9318,   -9.1006,\n",
      "         -20.9119,  -22.5987,  -16.0148, -462.9482,  -17.6569,  -13.8292,\n",
      "         -14.0181,  -33.7641, -142.8734,  -13.6891, -633.8416,  -10.3572,\n",
      "         -19.2981, -437.7802], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36793.64453125\n",
      "Outputs tensor([ -34.1488,   -9.9087,   -4.5473, -467.5059,  -14.0874,  -27.2458,\n",
      "          -8.3838,   -7.5303,  -45.6232, -406.7028,   -9.6024,  -15.0700,\n",
      "        -454.6427,  -12.6957, -480.1283,   -7.5865, -251.7660, -412.0874,\n",
      "         -10.3315, -484.5114], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39048.7421875\n",
      "Outputs tensor([  -3.1873,  -16.5485, -495.7679,  -11.1213,   -7.6292,   -8.2320,\n",
      "        -415.3077,   -9.0499,  -15.4107,  -31.8845,  -12.3536,  -12.0863,\n",
      "         -11.7556, -655.4195,  -13.0623,   -9.3459,   -9.0714, -459.7026,\n",
      "         -16.6427,   -9.3635], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32181.25\n",
      "Epoch training loss: 35674.2861328125\n",
      "Epoch validation loss: tensor([[31681.0430]])\n",
      "\n",
      "On epoch: 71\n",
      "Outputs tensor([ -47.5000,   -9.6565, -591.8260,  -11.6346,  -15.2316,  -16.4051,\n",
      "         -10.4646,  -18.9300,  -10.1921,  -14.2407,  -13.1304,  -63.0330,\n",
      "         -12.3036,  -23.3332, -406.2948, -246.2610, -495.6515,   -8.6836,\n",
      "         -13.7141,  -91.8414], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45567.6328125\n",
      "Outputs tensor([ -14.9028,   -6.6393, -262.4091,  -17.2129,  -17.8859, -489.9940,\n",
      "         -96.2795,  -15.5842,   -4.2496,   -4.6935,  -36.4601,   -3.9464,\n",
      "        -413.0088,  -10.8396,  -12.7576,   -6.8273,   -5.2048,   -7.7479,\n",
      "         -10.3013, -346.5966], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50889.1953125\n",
      "Outputs tensor([ -25.1230,  -13.1037, -402.6702,  -17.6523, -501.4643, -327.7809,\n",
      "         -12.3775,  -16.9198,  -17.2801, -694.4565,   -8.8033, -502.5638,\n",
      "        -455.8742,  -24.6389,  -15.2946, -424.8511,   -9.0086,  -11.9515,\n",
      "          -5.2075,  -20.7104], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 19541.279296875\n",
      "Outputs tensor([ -18.4539, -234.9823,  -15.9782,  -18.3796,  -36.3014, -663.2036,\n",
      "         -17.6684,  -12.9676,  -14.1553,  -23.3484, -442.3742, -481.1871,\n",
      "        -177.9076, -579.8527,  -13.0433,  -15.9467,  -18.1242,  -18.7087,\n",
      "        -253.0405,  -27.4058], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33941.671875\n",
      "Outputs tensor([ -12.1798, -208.6490, -107.2097,  -10.2622, -209.3456,   -7.7883,\n",
      "         -13.8020,   -9.3296,  -13.4053,   -6.9575,   -8.8282, -339.4692,\n",
      "         -12.3490,  -90.0999,   -8.2823,   -5.8957, -325.6537, -505.2353,\n",
      "        -536.0068,   -3.3283], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29635.18359375\n",
      "Outputs tensor([ -12.5464,  -18.3883,   -6.7426,  -14.2713,  -17.3036,  -15.3308,\n",
      "         -17.8967, -269.2613, -644.3350,  -10.8010,  -30.9701, -526.6724,\n",
      "         -18.7171,  -15.4017, -564.2341,  -37.6101,  -18.3041, -241.8860,\n",
      "         -14.8017,  -22.2483], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34023.55078125\n",
      "Epoch training loss: 35599.752278645836\n",
      "Epoch validation loss: tensor([[38137.1211]])\n",
      "\n",
      "On epoch: 72\n",
      "Outputs tensor([ -46.0121, -588.6261,  -12.9048,  -10.8382, -440.4539,  -35.6316,\n",
      "         -15.1474,   -9.9443,  -19.7146, -252.6764,  -11.8331,  -16.6083,\n",
      "         -63.1781,   -9.8138, -621.0721,  -41.7654,  -76.0751,  -12.9565,\n",
      "         -14.0649,  -12.1758], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 22513.43359375\n",
      "Outputs tensor([ -16.6363, -607.6445,   -8.9765,   -7.1625,  -15.0623, -571.1024,\n",
      "         -13.2591,  -33.2119,  -11.3807,  -10.3058, -164.0401, -433.8522,\n",
      "          -9.1326,   -7.8488, -282.7463,  -82.5375,  -19.9274,  -12.8968,\n",
      "        -396.0948, -231.0164], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28518.78515625\n",
      "Outputs tensor([-200.1344,  -13.5878,  -10.4107,   -8.8442,  -15.7182,  -53.4315,\n",
      "        -330.6400, -637.6461,  -80.2015,   -7.6039,  -12.2577,  -11.9276,\n",
      "         -30.9220,  -17.9236, -589.4126,   -7.0210, -155.1917,  -11.3075,\n",
      "         -18.5900,  -54.0591], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 64692.94921875\n",
      "Outputs tensor([  -9.5157,    3.3984,    2.5295,    3.0328,   -0.7590, -271.3143,\n",
      "          -5.2188,   -3.2740,   -7.2781,   -1.5412,   -1.2551,   -7.6630,\n",
      "          -0.9305, -275.5961,   -0.8296,   -9.6962, -335.7446, -163.1708,\n",
      "           0.5950, -282.8513], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 55169.1953125\n",
      "Outputs tensor([ -14.4621, -249.4717,    0.6009,    2.4176, -127.0195, -280.2145,\n",
      "          -8.8075,  -10.1809,   -1.9299,  -11.8705, -411.7849,   -9.2632,\n",
      "          -3.5547, -375.6520,   -3.9342,  -12.0907,   -9.0102, -198.4489,\n",
      "          -2.2660,   -6.9255], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 56561.98828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ -18.9493,   -4.5851, -218.7394,  -13.1502,  -16.2233, -275.4488,\n",
      "         -20.1343,  -11.5211,  -11.6929,  -20.8542, -518.6634,  -15.1664,\n",
      "          -9.7409,  -26.5029, -524.8925, -549.6712, -619.9020,  -11.3378,\n",
      "          -8.5949,  -28.5833], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31989.26171875\n",
      "Epoch training loss: 43240.935546875\n",
      "Epoch validation loss: tensor([[34122.2305]])\n",
      "\n",
      "On epoch: 73\n",
      "Outputs tensor([-537.5284,  -11.2600,  -15.0024,  -12.8948,   -6.6705,  -83.8519,\n",
      "        -574.5597,  -11.2333, -627.3975,  -17.5629,  -19.4864,  -17.9559,\n",
      "         -10.0502, -309.8549, -449.9323,   -8.8090,  -17.0893,  -15.5583,\n",
      "         -20.0367,  -16.3077], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38291.5546875\n",
      "Outputs tensor([-400.2283,   -6.6287,  -61.7992,  -12.7746,   -3.4581,   -3.0684,\n",
      "        -160.8643,  -17.6079,   -7.8222,  -16.7184,  -14.6537, -566.7311,\n",
      "        -459.4838,   -3.6337,  -16.0107,   -2.7532,   -2.1656,    3.5192,\n",
      "         -10.3466, -438.4775], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28904.125\n",
      "Outputs tensor([-149.7331,  -12.7842,  -19.8055,   -9.9156, -510.4396,   -7.9304,\n",
      "         -11.6790,  -10.2988, -385.9548,  -32.2293,  -12.7685,  -15.7346,\n",
      "        -634.2712,  -11.4671, -363.5765,  -17.3796,   -5.1800,   -7.7381,\n",
      "        -509.2242,  -46.5857], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43931.390625\n",
      "Outputs tensor([ -17.1159,  -18.3012,  -16.4570, -171.6942,   -9.6050, -441.9523,\n",
      "        -375.9169,  -44.5494,  -20.6539,  -11.1808,  -15.7371,  -22.8544,\n",
      "         -14.6788, -429.0527,  -20.9279,  -15.0491,  -32.0282,  -16.8174,\n",
      "         -23.9352, -595.6150], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 21716.94921875\n",
      "Outputs tensor([ -11.5422, -559.2845, -125.8205, -275.0359,  -18.0499,  -27.2369,\n",
      "        -291.3143,  -21.4240, -509.1663,  -14.0463,  -15.2073,  -14.5041,\n",
      "         -15.1266,  -10.4029,   -9.8490, -476.5547,  -19.1011, -262.0686,\n",
      "         -19.9962,   -9.2372], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24305.802734375\n",
      "Outputs tensor([  -8.9666, -548.7761,   -9.4811,  -20.5189, -503.3110, -550.2263,\n",
      "         -44.2441,  -23.9676,  -18.6802,  -16.5787,  -22.8146, -451.1315,\n",
      "         -10.9089,  -18.0170,  -13.5556, -593.1766,  -17.2769,  -14.6990,\n",
      "         -18.0102,  -12.0127], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32700.5\n",
      "Epoch training loss: 31641.720377604168\n",
      "Epoch validation loss: tensor([[33717.2461]])\n",
      "\n",
      "On epoch: 74\n",
      "Outputs tensor([ -12.8035,  -14.8333, -662.9359,  -18.7051,  -19.3109,  -13.7206,\n",
      "         -16.6119,  -13.7428, -365.8219, -185.6418,   -9.5772,  -13.2634,\n",
      "         -11.4936, -367.6233,  -18.3258,  -20.0577, -641.8004,  -15.3267,\n",
      "         -11.5583,  -15.3039], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40847.29296875\n",
      "Outputs tensor([ -16.1671,   -7.6948,  -11.1449,  -12.6484,   -9.8440, -354.7130,\n",
      "         -17.0248,  -12.9978, -474.3022,  -19.5538, -118.3627,   -4.3278,\n",
      "         -16.4535,   -2.3974, -634.0026, -232.1664,   -9.9165,   -8.2008,\n",
      "         -19.5887,   -7.6736], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 22117.041015625\n",
      "Outputs tensor([  -5.7561,   -6.8537,  -11.5186, -393.1865,  -12.7219,  -25.3296,\n",
      "          -9.6572,   -7.6135, -274.6076, -539.8707, -540.4838,   -8.9770,\n",
      "        -560.1627, -259.8089,  -12.6880,  -12.5994,  -11.1031,  -16.4951,\n",
      "          -7.5724,   -6.9428], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32642.759765625\n",
      "Outputs tensor([ -12.6033,  -17.9996,  -17.9870,   -7.4980, -242.3901,  -20.9681,\n",
      "          -9.8945, -421.8900,  -37.9957,  -14.0336,  -26.4773, -486.9703,\n",
      "          -5.4897, -426.0121,   -9.3885, -459.9520,  -14.3346,  -12.6544,\n",
      "        -579.9035,   -9.9692], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27645.599609375\n",
      "Outputs tensor([-443.1784, -590.5520,  -13.8252,  -17.2207, -457.3171,   -7.0598,\n",
      "          -7.4604,  -23.8111,  -26.3523,  -20.4748,   -6.7266,  -11.6397,\n",
      "         -21.6647,  -26.5217, -191.3989,  -19.7888,  -74.4367, -460.9298,\n",
      "         -11.0576, -599.0718], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28660.63671875\n",
      "Outputs tensor([ -12.1491,  -15.4710, -560.1305,  -95.2961,  -14.2355, -197.6174,\n",
      "         -38.7453, -638.3297,  -10.3228,  -26.9732,  -20.6655,  -12.2574,\n",
      "        -543.4963, -109.2888, -217.7424,  -15.2407,  -18.6673,  -12.8020,\n",
      "         -76.2473,  -10.0071], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43169.5078125\n",
      "Epoch training loss: 32513.806315104168\n",
      "Epoch validation loss: tensor([[34179.9844]])\n",
      "\n",
      "On epoch: 75\n",
      "Outputs tensor([-158.8462, -553.8652,   -7.1884, -524.3090, -500.2001, -431.7592,\n",
      "          -7.0739,   -9.6474,   -5.4031, -485.9658,   -5.9170,  -15.7475,\n",
      "         -14.7065,  -16.5004,   -8.5782,   -3.6607, -387.7371,  -11.8711,\n",
      "         -12.3999, -311.1328], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34579.9140625\n",
      "Outputs tensor([-219.7076,  -10.7615,  -69.9903,  -19.8753,  -21.1439,   -6.0529,\n",
      "        -570.1305, -526.3942, -614.3550, -569.9887, -752.5449,  -15.1225,\n",
      "         -14.7668,  -26.0285,   -9.8972,  -11.4717, -666.8873,  -37.8658,\n",
      "         -20.9783,  -11.7817], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31673.20703125\n",
      "Outputs tensor([-405.5740,   -5.7776, -334.1589,   -9.0108, -172.9798,  -18.4408,\n",
      "         -11.0505,  -36.6191,  -43.4608,  -36.5231,   -9.0125, -362.2050,\n",
      "          -9.0301,  -16.6671,  -18.8534, -486.2744, -453.9398,  -10.2997,\n",
      "         -11.0118,  -23.7278], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23839.05078125\n",
      "Outputs tensor([ -10.7305,  -52.2543,  -13.0486,  -13.9271,  -16.1709,  -13.1566,\n",
      "         -24.0833,  -18.9301,  -14.0619,   -6.0716,  -15.2310,  -14.1788,\n",
      "         -18.0594,  -15.2881, -638.4499, -417.9896,  -12.1266,  -29.0739,\n",
      "         -23.8695, -508.4069], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33463.35546875\n",
      "Outputs tensor([  -9.9866,   -7.8940,  -13.4107,   -6.8707,   -7.7824,  -17.8848,\n",
      "          -8.3081,  -99.3116,   -5.6756,   -1.1734,   -6.8644, -355.0919,\n",
      "        -373.3232,  -14.3974,  -12.9508,  -12.2237,  -22.5761,  -21.9684,\n",
      "        -376.9757, -361.1649], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 59678.4453125\n",
      "Outputs tensor([-3.1008e+00, -6.6556e-02, -1.5041e+02, -3.0960e+02, -6.2351e+00,\n",
      "        -2.4166e+02, -2.8126e+02, -5.0180e+01, -1.0245e+01, -3.6011e+00,\n",
      "        -7.7910e-01, -1.3241e+01, -7.1641e+00, -1.4866e+01, -2.2024e+00,\n",
      "        -3.9604e+02, -4.7792e+01, -6.1276e+00, -2.0130e+01,  3.1439e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41158.51953125\n",
      "Epoch training loss: 37398.748697916664\n",
      "Epoch validation loss: tensor([[33415.6797]])\n",
      "\n",
      "On epoch: 76\n",
      "Outputs tensor([-517.0404,  -11.0087,  -15.7709,   -8.8385,   -8.7820,  -18.8851,\n",
      "        -545.6691, -356.0213,  -42.9123,  -17.3006,  -16.7553,   -5.0579,\n",
      "        -117.0086,   -9.0450,   -6.1351,  -13.1351,  -19.5426,  -76.8388,\n",
      "         -37.7009,   -9.4510], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32352.037109375\n",
      "Outputs tensor([-174.3448, -167.6020,  -12.5434, -458.2982,   -6.9507,  -40.2387,\n",
      "         -10.2165,   -4.2211,  -13.6690, -481.4484,  -10.1505, -313.7416,\n",
      "         -41.8025, -373.9905,  -18.3144,  -10.6402,  -14.8615,  -11.8981,\n",
      "        -287.8839, -479.5550], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 18586.216796875\n",
      "Outputs tensor([-315.3240,  -11.7353,  -43.5588, -648.9729, -679.0613,  -13.4031,\n",
      "        -611.3708,  -87.9063,  -13.8580,  -30.5530,  -22.2220, -604.3541,\n",
      "        -580.0543,  -13.7190,  -15.1346,  -34.3573,  -33.5545,  -26.5478,\n",
      "         -18.4673,  -48.4611], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27806.802734375\n",
      "Outputs tensor([ -16.5393,  -16.9947,  -17.5201, -519.5736,  -42.6964, -115.8796,\n",
      "         -12.3525, -479.9309,  -11.4836,  -57.3496, -642.9193, -523.2604,\n",
      "        -140.4208,  -14.8959,  -16.2707, -140.0302,  -14.4568,  -20.6237,\n",
      "        -160.6516,  -15.9528], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47687.15625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([  -9.9056,   -5.2973,   -3.1818,   -5.1781,   -6.5888,   -4.5522,\n",
      "        -288.9714, -340.5963,   -4.5315,  -15.0951,   -5.2156,   -6.6560,\n",
      "        -418.9081, -259.3943,  -18.0773, -432.8889, -167.7433, -132.3811,\n",
      "         -14.6819,   -4.4034], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42176.6328125\n",
      "Outputs tensor([ -12.6554, -560.6834,   -6.9755,  -10.5038,   -4.9793,  -21.8022,\n",
      "        -147.8737, -561.9207,  -33.1265,   -7.2985,  -10.0976,  -13.0614,\n",
      "        -375.5096,   -9.3657, -134.4839, -194.9912,  -12.8221,   -7.9536,\n",
      "         -16.9733,   -9.9069], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35310.1640625\n",
      "Epoch training loss: 33986.501627604164\n",
      "Epoch validation loss: tensor([[37458.3750]])\n",
      "\n",
      "On epoch: 77\n",
      "Outputs tensor([-472.8598, -376.0994,   -2.6438, -247.5482,   -1.5662,   -4.0590,\n",
      "        -602.2601,  -13.9255,  -19.8504,   -8.5929,   -6.9578,  -12.0694,\n",
      "          -9.6673,   -1.4528, -507.8964,   -2.1139, -489.8496,   -5.1739,\n",
      "          -4.8927,  -17.9780], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 22684.81640625\n",
      "Outputs tensor([  -5.0098,   -1.4855,  -13.9740,   -7.2536, -402.3551, -130.1849,\n",
      "        -354.6303,   -4.7184,  -20.3259,  -21.5173, -282.0582,   -8.7372,\n",
      "        -269.5019,   -7.1320, -527.7627, -541.7842,  -13.4889, -318.7863,\n",
      "        -501.2708,   -5.0310], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26257.978515625\n",
      "Outputs tensor([-669.7278,  -86.2125, -349.8623,  -94.7445, -362.8382,  -18.1961,\n",
      "         -24.7582,  -19.5561,  -19.9872, -119.6874, -168.8835,  -17.5322,\n",
      "         -19.5374,  -15.4773, -611.6483, -708.4214,  -24.2050,  -96.6062,\n",
      "         -21.0398, -559.6361], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42125.8046875\n",
      "Outputs tensor([-303.9897,  -90.3270,  -13.8621,   -3.6977, -266.6279,   -7.6379,\n",
      "         -18.0297,  -13.2384, -484.8289,   -6.9850,   -3.4185,   -9.5808,\n",
      "          -7.9455,  -10.0584, -541.6796,  -13.5390,  -44.1653,  -16.4508,\n",
      "          -6.1470, -118.9344], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30293.578125\n",
      "Outputs tensor([-449.4204,   -1.7437,  -11.9775,  -12.0261,  -68.7319,  -18.3637,\n",
      "          -2.9363,   -5.1551,   -2.0076,  -12.9611, -534.8202,   -4.1356,\n",
      "        -458.7150,   -4.1351, -113.3308,  -11.2390,   -8.6146, -122.6470,\n",
      "          -3.8804, -417.5670], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40699.88671875\n",
      "Outputs tensor([  -3.1553,   -4.1949,  -18.8536,  -12.8771,   -5.3695,  -11.1237,\n",
      "        -127.3649,   -2.1554, -464.6035,   -6.2063,   -8.3518,  -56.8825,\n",
      "        -328.2342,  -16.5575,   -4.9269, -325.2069,  -10.3371,  -16.5223,\n",
      "          -9.9321,   -5.5723], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 56100.21875\n",
      "Epoch training loss: 36360.380533854164\n",
      "Epoch validation loss: tensor([[36675.1055]])\n",
      "\n",
      "On epoch: 78\n",
      "Outputs tensor([  -6.8130,   -5.4226,   -5.0375,  -12.7190,  -11.1331, -383.4659,\n",
      "         -10.6349,   -5.3862,  -10.2706, -276.4952,    1.1885,   -3.0506,\n",
      "          -5.2510,   -0.7106,   -9.8953, -507.0697, -269.9116,  -11.5399,\n",
      "        -464.0454, -324.6957], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41784.7109375\n",
      "Outputs tensor([ -98.4327,  -89.9564,   -4.1394, -445.5201,   -5.2382,   -6.8893,\n",
      "         -33.2622,   -7.7346,  -16.8178, -186.6385,  -17.7374, -407.6021,\n",
      "        -310.7143,  -18.6634, -434.9999,   -8.4323, -222.2350,   -5.4729,\n",
      "          -8.0642, -491.8338], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49623.57421875\n",
      "Outputs tensor([  -2.6341,   -2.6259,  -10.4369,   -3.4733, -472.8900,    3.1246,\n",
      "          -8.4475,   -3.4406, -356.1013, -271.5388,   -5.3840,    0.5907,\n",
      "           1.7049, -309.4197,   -7.7394,   -1.6340,  -10.2151, -419.7299,\n",
      "          -1.7802,   -1.4967], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31945.48828125\n",
      "Outputs tensor([-383.5094, -424.6890,   -9.9299,   -2.5787, -396.7964, -403.9466,\n",
      "         -11.9848,    0.7006,    6.5249,    1.0315,  -11.9914,    4.0246,\n",
      "        -607.5495,   -5.7167,    1.0569,   -2.0209,   -8.0842,  -46.0847,\n",
      "         -10.6281, -509.5093], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30011.95703125\n",
      "Outputs tensor([-396.6688,  -18.3166,  -12.7603,  -10.5769, -416.8988, -405.9397,\n",
      "         -16.2871,  -13.8936, -322.3367, -228.3831,   -1.3954, -495.1171,\n",
      "          -1.7117,    3.0844,   -6.3340,   -0.9748,  -44.7786,   -5.6342,\n",
      "         -16.9702,   -0.9413], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32705.33203125\n",
      "Outputs tensor([ -15.9092,  -50.3139,  -17.9349, -771.5203,  -14.1392, -287.6519,\n",
      "        -149.9260,  -68.3925,  -13.0918,  -91.6440,  -81.1148,  -13.5518,\n",
      "         -88.6377,  -26.2875, -206.8791, -293.5383,   -7.4049,  -15.5980,\n",
      "         -11.8671, -561.6759], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 62244.3984375\n",
      "Epoch training loss: 41385.91015625\n",
      "Epoch validation loss: tensor([[42963.6523]])\n",
      "\n",
      "On epoch: 79\n",
      "Outputs tensor([-2.4777e+02,  3.4842e-01, -7.8956e+00,  2.9593e+00, -3.9618e+00,\n",
      "         5.1260e+00, -2.7138e+02, -1.0336e+01,  3.4429e-01, -2.3002e+00,\n",
      "         3.1033e+00, -4.1905e+00, -3.9840e+02, -2.7432e+00, -1.0239e+01,\n",
      "        -1.0236e+00,  4.1351e+00, -3.4835e+02, -4.6079e+02,  7.7011e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35998.9765625\n",
      "Outputs tensor([-403.0165,   -3.2768,   -4.3433,  -17.2319,   -6.4809, -503.1674,\n",
      "          -8.8048,   -3.0184,    3.0748,  -11.0881,  -11.8796,    3.2489,\n",
      "          -3.9420,    2.0813, -383.6369,  -12.7634,  -13.6570,   -7.4429,\n",
      "         -17.5343,  -36.2628], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32134.30078125\n",
      "Outputs tensor([-533.2735,   -2.1507,   -2.0914, -616.8616, -528.5885,   -9.5025,\n",
      "          -7.8448, -174.3298, -395.7934,   -3.2778,    4.0599,   -9.4050,\n",
      "          -2.5196,   -6.9182,   -4.3803,    2.6420,    3.8686,   -5.9332,\n",
      "          -5.6193,    2.1185], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32523.193359375\n",
      "Outputs tensor([-5.3003e+02,  2.2346e+00, -3.3339e+00, -6.3864e+01, -5.4019e+00,\n",
      "         1.3711e-01, -1.4649e+01, -1.7254e+00, -3.8746e+02, -1.7029e+01,\n",
      "        -1.6499e+01, -2.1132e+01,  2.4942e-01, -1.8274e+02, -9.8998e+01,\n",
      "        -1.6221e+01, -5.9333e+00, -9.5426e+00, -5.1573e+02, -1.3563e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49440.10546875\n",
      "Outputs tensor([-623.4344,   -3.0377,   -4.4249, -578.6638,  -15.4677,  -20.8614,\n",
      "        -633.0314,  -10.4930,   -5.8293,   -7.7176, -519.9299, -310.6348,\n",
      "          -7.2142,   -5.1075,  -13.9047, -633.2236,  -25.2932,   -7.3212,\n",
      "        -520.4641,  -11.5442], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28154.0\n",
      "Outputs tensor([-1.2319e+01, -2.3360e+00,  2.1438e-01, -5.5170e+02, -4.0370e+00,\n",
      "        -3.6692e+00, -5.0205e+00, -1.7510e+01, -1.8608e+02, -1.3886e+01,\n",
      "        -8.2507e+00, -1.9094e+01, -1.0660e+01, -5.5799e+02, -8.3949e-01,\n",
      "        -1.1112e+01, -4.6952e+02, -4.3130e+02, -7.5155e+00, -1.3257e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46202.390625\n",
      "Epoch training loss: 37408.827799479164\n",
      "Epoch validation loss: tensor([[38015.6953]])\n",
      "\n",
      "On epoch: 80\n",
      "Outputs tensor([-4.0780e+02, -4.7086e-01, -1.1312e+01, -1.2781e+01,  1.2928e-01,\n",
      "        -1.4207e+02,  2.3167e+00, -4.3546e+00, -1.0833e+01, -1.2542e+01,\n",
      "        -4.8780e-01,  1.2118e+00, -3.1078e+02, -2.4581e+00, -4.0814e+02,\n",
      "        -2.3443e+00,  1.1270e+00, -3.8328e+02, -8.8087e+00, -2.9055e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50863.8828125\n",
      "Outputs tensor([-423.7897,   -4.4511, -237.4965, -386.6703, -519.5453,   -6.5236,\n",
      "          -9.2841,  -11.9207,   -8.0090, -443.7013, -507.0386,   -5.5494,\n",
      "          -3.6245,   -6.5554,  -19.4946,   -3.8360,   -9.2925,   -4.7407,\n",
      "         -56.4008,   -9.9068], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 46757.8359375\n",
      "Outputs tensor([  -6.0159,   -1.1688,  -11.1524,  -10.4251, -540.6959,  -21.0626,\n",
      "          -7.4822,  -21.8818,  -11.8424, -123.8250, -161.6829, -185.1926,\n",
      "        -319.6362,  -11.0298,  -12.3148, -538.5069, -614.0347,  -11.7948,\n",
      "        -299.2636, -478.7568], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27239.43359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ -13.2036,  -17.8109, -736.9349,  -39.9679,  -21.4915, -710.6018,\n",
      "        -297.3411, -628.5822,  -13.0401,  -20.1946,  -16.4051,  -22.0830,\n",
      "          -9.7596,  -75.5459,  -12.6804, -133.1475, -666.5682,  -21.2781,\n",
      "         -22.7939,  -88.6400], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 20268.20703125\n",
      "Outputs tensor([-279.0548,  -13.2109, -102.4813, -177.9334,  -11.2999,  -10.8175,\n",
      "        -452.5107, -659.1320,  -11.7880,  -19.6567,  -16.6270,  -57.2473,\n",
      "          -9.3227,  -16.8226,  -15.6842, -132.3154,  -11.3158,  -19.1965,\n",
      "          -9.4262, -169.3802], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 49016.3203125\n",
      "Outputs tensor([   0.9304,   -1.2152,   -2.2083,  -34.7304,  -11.8494,   -9.0847,\n",
      "          -3.7970,  -61.3287, -246.9831,  -12.6587,   -4.3387,   -4.3843,\n",
      "         -14.5964,  -20.3864,    0.6224, -115.5333,   -1.5390,  -10.2473,\n",
      "         -13.8580, -368.6389], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34254.90625\n",
      "Epoch training loss: 38066.764322916664\n",
      "Epoch validation loss: tensor([[37656.5391]])\n",
      "\n",
      "On epoch: 81\n",
      "Outputs tensor([  -9.2366, -208.4679,    2.2187,    5.4588,   -2.4582, -388.3500,\n",
      "           1.6492,   -7.1156,   -4.8147,   -6.4652, -421.6154,    4.1584,\n",
      "        -124.2193,   -3.9244,   -8.0774, -146.0652,    0.9290,  -14.0845,\n",
      "          -7.7191,   -6.1009], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31187.77734375\n",
      "Outputs tensor([-1.4667e+01,  4.3708e-01, -4.6186e+00, -3.4243e+02, -5.8482e+00,\n",
      "        -2.8572e+00, -1.0236e+00,  1.8715e+00, -2.8597e+02,  9.5170e-01,\n",
      "        -1.0809e+00, -7.0118e+00, -4.3190e+02, -1.1536e+01,  1.4063e-01,\n",
      "        -3.1760e+02, -7.8429e+00, -1.4588e+02, -1.2221e+01, -1.4787e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33568.26953125\n",
      "Outputs tensor([-307.6188,   -9.1635,   -2.4025, -430.3545,   -8.3580, -214.7862,\n",
      "         -16.2509,    0.8892,  -15.3862,    4.8549,   -6.8695,   -1.5763,\n",
      "        -393.3218,    3.1742, -349.2047, -407.5885,  -11.3145,   -3.2747,\n",
      "          -7.4417, -470.7487], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28375.896484375\n",
      "Outputs tensor([  -3.3062, -533.2935, -552.2733,   -5.1669, -701.3057,  -23.0591,\n",
      "         -58.2135,   -9.5241, -495.2195,  -12.6008, -273.1967,   -7.4922,\n",
      "         -79.4735,  -11.9030,  -10.5079,  -11.2428,  -11.7094,  -24.8061,\n",
      "        -436.3973,   -5.6523], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32087.650390625\n",
      "Outputs tensor([-594.1386, -425.7501, -246.3153,  -13.4852,  -16.0016,   -9.8402,\n",
      "         -13.9383, -208.5183,  -12.9620,  -17.2027,  -11.7814,  -15.4531,\n",
      "        -163.0609,  -21.2089,  -12.4448, -643.8500, -595.7128,  -17.9887,\n",
      "         -15.7703, -513.0665], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37712.0859375\n",
      "Outputs tensor([ -12.1572,  -17.8207,  -23.7644, -369.4023,  -18.1482,  -81.4797,\n",
      "        -540.4814, -237.6386,  -19.9219,  -16.7040, -709.4075,  -37.2128,\n",
      "        -290.6296, -157.8915,  -13.0506,  -17.2990, -134.7108,  -29.8415,\n",
      "         -74.7545,  -83.4251], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 97603.25\n",
      "Epoch training loss: 43422.48828125\n",
      "Epoch validation loss: tensor([[47344.1836]])\n",
      "\n",
      "On epoch: 82\n",
      "Outputs tensor([-8.2887e+00, -4.0340e+00, -1.3502e+00, -1.8220e+00,  2.6834e-01,\n",
      "         3.7030e+00, -6.2034e+00,  1.2752e+00,  3.8957e+00, -4.0294e+00,\n",
      "        -3.3964e+02,  1.4976e+00, -3.7772e+02,  2.3354e+00,  4.1514e+00,\n",
      "        -3.7953e+02,  5.1410e-01, -2.9729e+00,  5.4499e-01, -6.7920e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44535.41796875\n",
      "Outputs tensor([  -1.2977,  -89.2129,    7.7266, -361.9586,   -0.7702,    6.8446,\n",
      "          -4.0325, -307.0572,   -4.2654,    6.7653,   -5.9520,   -1.4972,\n",
      "           5.3536,    5.7148,    8.4014, -400.2137,   -0.7943,   -4.1578,\n",
      "          -4.3402, -442.3005], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36005.609375\n",
      "Outputs tensor([-3.9528e+00, -4.2794e+00, -2.1311e+02, -8.6115e+00, -2.2619e+02,\n",
      "        -2.5355e+00, -4.8931e-01, -1.4190e+01, -5.5061e+00, -2.4904e+00,\n",
      "        -2.6387e+00, -6.1893e-02, -1.9269e+02,  1.2813e+00, -1.1696e+02,\n",
      "        -6.6688e+00, -3.4984e+00, -2.0470e+02, -1.9118e+00, -1.5628e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45159.35546875\n",
      "Outputs tensor([-8.8749e+00, -4.4797e+00, -1.3570e+01, -8.2516e-01, -4.9615e+02,\n",
      "        -1.3611e+01, -1.9784e+01, -1.2715e+01, -2.0468e+01, -1.3130e+01,\n",
      "        -5.9768e+02, -3.2906e+02, -1.3062e+01, -1.4736e+01, -2.3762e+00,\n",
      "        -4.2717e+02, -8.5079e+00, -3.5165e-01, -4.5509e+02, -4.4353e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27678.87890625\n",
      "Outputs tensor([  -0.5481, -317.3956, -106.1653,   -7.0364,   -2.6703,    2.9584,\n",
      "          -7.3218,  -13.3406,   -8.6409,    1.6596,   -2.6304,   -5.0988,\n",
      "        -358.8885, -535.2372, -131.5323,  -10.9904, -469.1063, -484.8610,\n",
      "        -447.9097,   -2.6940], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36380.2734375\n",
      "Outputs tensor([ -12.5906, -234.7519, -545.5504,  -14.7795,  -20.0844,  -56.2788,\n",
      "        -628.9741,  -19.1302,  -12.8512,  -16.4707,  -67.0852,  -11.7936,\n",
      "        -327.6142, -633.7359, -317.4101,  -16.0913,  -37.4271,   -7.8805,\n",
      "         -11.3009, -103.4505], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25901.8515625\n",
      "Epoch training loss: 35943.564453125\n",
      "Epoch validation loss: tensor([[30995.8145]])\n",
      "\n",
      "On epoch: 83\n",
      "Outputs tensor([ -11.7760, -547.3065,  -10.3507, -497.3339,  -13.1796,  -29.2413,\n",
      "        -680.0888,  -13.2050,  -19.4396,  -11.4450,  -19.2572,  -12.7449,\n",
      "          -8.0801,   -4.5277,   -7.6500, -695.1924,  -15.1088,  -36.4010,\n",
      "        -368.0319, -622.3149], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 20950.453125\n",
      "Outputs tensor([-383.7055,   -7.6830, -476.8790,  -15.4990,  -11.2778,   -9.6694,\n",
      "        -537.9260,  -12.2373, -331.2563,   -7.3867,  -16.2693,   -5.2993,\n",
      "         -21.4231,  -14.9953,  -12.9681,  -13.1042, -333.4456, -456.6139,\n",
      "        -499.8216,   -5.0602], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44481.19140625\n",
      "Outputs tensor([-595.3196,  -12.1094,  -13.0612,  -10.3372,  -10.1259,    1.4831,\n",
      "         -15.2881, -338.2722,   -4.7896,  -60.6335,   -3.3312,   -1.7996,\n",
      "        -332.4849,  -18.1374,  -57.1549,  -15.9702, -197.1735,  -16.7322,\n",
      "          -5.7238,  -13.7141], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40504.4140625\n",
      "Outputs tensor([ -15.9722, -528.9352,   -8.1339,  -85.4635,  -19.5479,   -9.1726,\n",
      "        -392.4738,  -10.3659,   -6.1145, -537.0821,  -11.5803,   -6.0175,\n",
      "          -9.0535, -321.4450,  -67.2651, -562.3975,  -12.6114,   -9.1878,\n",
      "          -8.1249,  -17.9949], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26237.859375\n",
      "Outputs tensor([-509.3228,  -15.3491,  -11.5409,   -9.4474, -196.8987, -512.8754,\n",
      "        -425.7959,  -16.3788,  -13.6984, -131.3480,   -7.9627,  -17.5075,\n",
      "         -40.5382,   -9.5299,  -20.6208, -324.9810, -415.0184,  -19.9935,\n",
      "        -379.5501,  -62.5877], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31629.359375\n",
      "Outputs tensor([-561.2977, -243.3095,  -13.0677,  -17.1541, -159.0079,  -11.1304,\n",
      "        -708.0295,  -13.6145,  -81.3798,  -20.0762,  -14.9250, -188.5928,\n",
      "         -12.5888, -149.0666, -651.2121, -277.8004,  -16.4948, -241.8497,\n",
      "         -13.0055,   -8.7843], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 75781.6328125\n",
      "Epoch training loss: 39930.818359375\n",
      "Epoch validation loss: tensor([[39390.9922]])\n",
      "\n",
      "On epoch: 84\n",
      "Outputs tensor([-5.4101e-01, -6.2548e+00, -1.4555e+00,  7.5187e-02, -3.3794e+02,\n",
      "         1.7252e+00, -4.9438e-01, -7.5588e+00, -3.5423e+02, -1.0026e+01,\n",
      "        -6.3164e+01, -3.1152e+00,  1.1445e+00, -5.5235e+00, -2.1054e+02,\n",
      "         2.1554e+00,  2.5444e-01, -4.0532e+00,  1.1994e+00, -6.2698e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35003.7421875\n",
      "Outputs tensor([-6.1423e+00, -2.7996e-02, -3.6470e+02,  4.3857e+00, -7.4702e+00,\n",
      "        -1.1663e+01, -4.1270e+00,  3.0360e+00, -5.6641e+00, -2.7426e+02,\n",
      "        -2.4759e+02, -5.6175e+00, -9.7074e+00, -8.4832e+01, -3.8283e+02,\n",
      "        -1.7825e+02,  6.0457e-01, -5.7152e+00,  2.1234e+00, -2.9747e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 53860.0703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ -11.8509,  -87.6284,  -10.5844, -588.0758,   -8.1475,  -18.9699,\n",
      "        -612.0756,  -29.3725,   -9.1997,  -11.1891,   -8.9606,  -44.9276,\n",
      "         -18.4697, -620.0595,  -31.3972,   -4.9462,   -8.5589,  -12.7593,\n",
      "         -30.0236,   -9.4557], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47496.30859375\n",
      "Outputs tensor([ -18.3914,  -13.6826,   -2.7806,  -10.5488, -143.6299, -251.8523,\n",
      "          -0.5401, -333.0010,  -17.4510,   -4.5308,  -23.4090, -271.6603,\n",
      "        -123.6572,    3.0597, -428.1378,   -8.7351,  -13.4903,  -12.4941,\n",
      "          -9.2135,  -14.0287], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24573.32421875\n",
      "Outputs tensor([ -13.3288,  -19.4099,  -11.9514,  -10.1598,  -13.1622,  -15.8252,\n",
      "        -561.1036, -619.0838,   -9.6942, -241.2465, -601.3409, -651.1241,\n",
      "         -10.4378,  -11.8832,   -9.5605,   -6.9520,  -16.3488,  -59.9689,\n",
      "         -19.4371,  -10.0810], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33785.32421875\n",
      "Outputs tensor([  -4.6880,   -6.8193,  -11.3868,   -7.7547,  -83.6658, -310.5520,\n",
      "         -11.3903,   -7.5061, -570.4526, -499.2266,  -21.4224,   -2.9650,\n",
      "        -286.2214, -503.1573,   -0.7790, -127.3200,  -20.4243,   -6.5754,\n",
      "         -15.9623,  -16.2000], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37859.34765625\n",
      "Epoch training loss: 38763.01953125\n",
      "Epoch validation loss: tensor([[34010.9805]])\n",
      "\n",
      "On epoch: 85\n",
      "Outputs tensor([ -17.0170, -102.0111, -517.2606,   -4.7094,   -7.4307,   -7.4711,\n",
      "          -5.8343,   -4.4971, -244.9131, -435.0646,  -16.4949,   -8.8932,\n",
      "         -18.6807, -496.0709, -249.0082,   -3.1953,   -3.1032, -561.3238,\n",
      "         -13.5244,   -4.4453], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33965.69921875\n",
      "Outputs tensor([-513.5371, -657.0883, -310.3952,  -14.9471,   -6.2581,  -15.2339,\n",
      "        -134.9498,   -3.6023,  -14.6229, -324.5231,   -7.5514,   -7.4358,\n",
      "        -593.5525,   -3.5130,  -17.6508, -257.4309,   -4.3392,  -17.9073,\n",
      "         -21.9233,   -7.5235], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34970.0546875\n",
      "Outputs tensor([-500.1496, -298.0066,  -17.1132, -498.3882,   -6.7540,   -5.6390,\n",
      "          -6.9492,  -14.4991,  -10.6783,   -9.7425,  -10.6959, -488.1277,\n",
      "         -12.1037,   -1.2879,   -6.6164, -453.6282,   -3.4319,   -7.2352,\n",
      "          -4.1822,   -5.9901], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30342.35546875\n",
      "Outputs tensor([ -10.7362,  -17.5858,   -4.2242, -340.3539, -346.1808,   -3.4540,\n",
      "        -533.2604,   -5.7793, -355.1672, -229.7507,   -4.7014,  -11.1497,\n",
      "        -233.6208,   -3.3668, -576.7892, -152.6320,   -6.4985,  -10.7333,\n",
      "          -1.5333,   -2.8517], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25006.736328125\n",
      "Outputs tensor([-473.7455,   -9.4196, -558.7800,   -7.4572,   -6.4095,  -32.2183,\n",
      "        -662.8948,  -74.7938,  -36.3339,   -8.5811,  -21.6461,  -18.7440,\n",
      "          -6.0561,  -19.6968,  -13.2416,  -13.7321, -280.1761,  -11.3671,\n",
      "        -649.9952, -130.5789], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29524.50390625\n",
      "Outputs tensor([ -17.5287, -506.0039,  -14.3359,  -27.1658,  -21.2648, -383.6930,\n",
      "         -23.1890,  -22.0542,  -24.2201,  -64.0846, -541.6649,  -16.3145,\n",
      "          -9.8787, -255.1813,  -20.2384,  -15.0424,  -83.6310,  -11.1110,\n",
      "         -19.3395, -123.4760], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47577.70703125\n",
      "Epoch training loss: 33564.509440104164\n",
      "Epoch validation loss: tensor([[37695.7227]])\n",
      "\n",
      "On epoch: 86\n",
      "Outputs tensor([-2.0842e+02, -9.6572e+01, -2.4659e+00, -9.5349e+00, -9.7781e+00,\n",
      "        -1.3749e+01, -3.9733e+00, -4.6124e+00, -3.6166e+00, -3.7262e+02,\n",
      "        -3.7997e+00, -3.7381e+02, -1.0324e+02, -1.0549e+00, -1.3490e-01,\n",
      "        -1.2450e+01, -5.7593e+00, -1.1802e+01, -3.6780e+00, -4.2404e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35549.22265625\n",
      "Outputs tensor([-1.2043e+01, -7.4908e+00, -1.8181e+01, -7.9856e+00, -3.0133e+02,\n",
      "        -6.9847e+00, -2.7896e-01, -1.6629e+00, -4.5916e-01, -1.2316e+01,\n",
      "        -3.1336e-01, -4.4070e+02, -6.0940e-01,  5.3055e-01, -7.3488e+01,\n",
      "         3.5002e-01, -4.2739e+02, -1.1931e+01, -4.2636e+02, -3.5629e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25661.869140625\n",
      "Outputs tensor([-365.0644,  -11.5210,  -20.5918,  -10.4364, -587.8791, -417.5806,\n",
      "         -11.2855,   -2.6592,   -9.4825, -151.3255,   -5.3557,   -9.6245,\n",
      "         -18.7703,  -29.9588,   -4.0410,   -4.5766, -482.6601,  -20.3858,\n",
      "         -12.3674, -620.0374], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34071.27734375\n",
      "Outputs tensor([ -14.5224,  -17.4387,  -14.9673, -228.0903, -623.4147, -555.6407,\n",
      "         -14.9257,  -13.8831,  -20.4066,  -10.3918, -486.1955,  -13.2355,\n",
      "        -634.2269,  -17.8471,  -24.1214,  -19.6634, -483.3499,  -20.1221,\n",
      "         -16.6122,  -14.4837], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32605.12890625\n",
      "Outputs tensor([ -11.8439,  -10.2436, -457.8260,  -17.1798, -528.1188,  -11.4693,\n",
      "         -14.6565,   -8.0757, -451.1035,  -18.0138,   -9.0059,  -51.1413,\n",
      "         -21.5381,   -5.6904, -472.0450,  -15.0449,  -45.5934,  -82.5573,\n",
      "          -7.5684, -537.8133], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38542.15234375\n",
      "Outputs tensor([  -8.0270, -344.4903,  -18.5658,  -34.9159,  -16.7155, -241.9802,\n",
      "         -14.7173,  -15.5789,  -23.5987,  -16.0215, -165.7175, -598.8845,\n",
      "         -12.6368,  -22.3636,  -74.0822, -658.3710,   -8.7051,  -16.4381,\n",
      "         -13.3362,  -60.6099], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 38225.5\n",
      "Epoch training loss: 34109.191731770836\n",
      "Epoch validation loss: tensor([[30769.3145]])\n",
      "\n",
      "On epoch: 87\n",
      "Outputs tensor([ -10.3309,   -1.0928,   -6.1255,  -12.6274,  -11.9962,  -11.2625,\n",
      "        -482.5047,  -10.9990,  -51.0283, -475.9958,   -3.8037,  -14.8446,\n",
      "        -113.3129,  -10.2385,  -10.5261, -388.7700, -317.3400,  -19.6006,\n",
      "          -9.9132,    1.3033], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35151.70703125\n",
      "Outputs tensor([ -17.0152, -439.5557,   -9.8298,   -3.4994,   -4.8760, -495.3191,\n",
      "        -529.8407,   -7.8521, -111.2066, -516.8630,   -9.9593,  -16.0563,\n",
      "        -655.9746,  -14.6402,   -4.7008,   -7.8092,   -3.2447,   -4.1320,\n",
      "        -455.5699, -409.2290], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 18927.109375\n",
      "Outputs tensor([  -6.6033, -393.3358,   -9.8692,   -7.2085,   -6.8604, -551.5504,\n",
      "        -505.5014,  -20.7571,  -42.0332, -498.8178,  -78.5288,  -13.3612,\n",
      "        -211.6644,   -7.6809, -606.3425,  -29.0459,   -4.0180,  -14.4301,\n",
      "         -14.1552,   -5.3363], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47763.234375\n",
      "Outputs tensor([  -1.6402,  -40.9998,   -2.0364,   -7.5389,  -11.3709,   -6.7572,\n",
      "          -2.5370,  -17.1471,   -9.3595,  -12.5998,  -12.0792, -157.4251,\n",
      "          -1.0505, -515.2527, -443.1964, -541.0560,   -7.8794,   -2.2764,\n",
      "          -4.7933, -432.6253], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47814.5234375\n",
      "Outputs tensor([  -7.3757,   -3.9333, -487.9967,  -38.6307,   -4.6390,  -25.0132,\n",
      "          -4.3893,  -17.6204,  -66.7917,   -5.0004,   -5.9789,   -6.8303,\n",
      "         -42.1666,   -7.8737, -164.4131, -550.5872,   -4.1378, -213.8162,\n",
      "          -7.9417,  -16.5333], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23194.77734375\n",
      "Outputs tensor([  -1.7062,  -15.8670,   -3.4388,   -4.7416, -538.4595,   -4.4133,\n",
      "        -320.1562,   -9.1181, -556.1503,   -9.3049,   -7.0425,  -12.9362,\n",
      "          -6.9910,   -7.7403, -231.7632,  -10.0865, -615.9393,   -6.1306,\n",
      "          -5.9444,  -84.6501], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34099.90625\n",
      "Epoch training loss: 34491.876302083336\n",
      "Epoch validation loss: tensor([[32821.1641]])\n",
      "\n",
      "On epoch: 88\n",
      "Outputs tensor([ -26.9968,  -31.1353,  -47.7905,  -12.5645,  -28.5450, -564.6785,\n",
      "        -123.4399, -326.4229,   -4.2455, -376.7596,  -17.3723,  -14.5910,\n",
      "        -267.3524,   -6.2709,  -96.4088,  -18.7352,  -56.5512,  -14.5751,\n",
      "          -9.2554,  -11.1971], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 55981.64453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([-457.6409, -299.8934,   -5.9564,    3.6682,   -4.8789,    6.2459,\n",
      "        -285.6126,   -5.0974,  -10.0404,   -3.8406, -389.7862,    4.0665,\n",
      "          -1.2229, -371.5650,    1.8375,  -12.5174,    2.4241,    5.2341,\n",
      "        -402.4168, -138.4644], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44049.0703125\n",
      "Outputs tensor([-5.3635e-01, -1.7561e+01, -5.9057e+02, -1.4616e+01, -5.5433e+02,\n",
      "        -1.5557e+00, -1.4940e+01, -1.2740e+02, -1.1258e+01, -4.3225e+00,\n",
      "        -3.5204e+00, -2.0697e+00, -6.6786e+02, -2.8854e+02, -2.4394e+01,\n",
      "        -8.0538e+00, -5.1303e+02, -6.3459e+00, -2.5949e+00, -4.7207e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32073.693359375\n",
      "Outputs tensor([-2.9370e+02, -1.2222e+00,  1.4918e+00, -4.2421e+00, -2.3650e+01,\n",
      "        -5.1139e+02, -4.2881e+02, -1.1361e+01,  1.9798e+00, -1.8672e+02,\n",
      "        -1.2112e+01, -4.7834e+02,  8.8842e-01, -3.8588e+02, -1.3683e+00,\n",
      "        -3.2666e-01, -4.9654e+00, -1.6680e+01, -1.1330e+01, -1.2068e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27251.40625\n",
      "Outputs tensor([ -11.7015, -580.4308,   -3.5621,   -5.2577,   -5.9554,  -15.8006,\n",
      "         -12.4757,   -7.6058, -132.1071,  -14.1702, -409.9052,  -10.7469,\n",
      "         -15.2161,   -3.2874, -583.7741,   -4.0860,   -2.3224,   -4.9097,\n",
      "         -49.7397, -486.8833], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33206.99609375\n",
      "Outputs tensor([  -8.8067,   -6.5956,   -8.8111, -227.0138,    0.7407, -437.2560,\n",
      "          -7.6844,  -46.3604,   -9.9334,   -4.5343, -450.1658,   -7.1105,\n",
      "        -368.8712,  -35.0039,  -17.4243, -616.0416, -225.7736,   -3.9865,\n",
      "        -436.5109,  -24.0211], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26462.453125\n",
      "Epoch training loss: 36504.210611979164\n",
      "Epoch validation loss: tensor([[30981.4902]])\n",
      "\n",
      "On epoch: 89\n",
      "Outputs tensor([ -18.2616, -308.9835,   -4.4967, -477.7852, -535.3810,   -4.7220,\n",
      "        -331.8071,   -8.9507,   -3.1681,   -6.8124,  -11.5255, -353.4541,\n",
      "         -14.1173,   -0.6438,  -17.7711,   -5.5397,   -4.2014,  -11.6416,\n",
      "          -3.9958, -529.5753], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27765.083984375\n",
      "Outputs tensor([-511.5307,   -8.4964,  -15.8451,  -10.1561,   -8.8717,  -17.1592,\n",
      "        -292.3472, -670.1926, -559.3374,   -6.2434,  -16.1653, -387.0769,\n",
      "         -10.7717,   -6.7202,  -15.7321,   -6.2126,  -13.3420,  -11.1980,\n",
      "        -404.1151,  -10.2809], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28492.212890625\n",
      "Outputs tensor([ -23.3929, -245.7535, -468.9749, -114.3832,  -13.1101,   -9.9379,\n",
      "         -14.3552,  -13.7091,  -13.3615,  -12.4015,  -17.5323,   -5.4878,\n",
      "          -3.2145,  -10.8437, -633.0863, -235.3855,  -13.4876, -318.9473,\n",
      "          -5.0649, -427.0052], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34371.0859375\n",
      "Outputs tensor([ -14.5953, -232.8532, -118.0751, -329.7189,  -14.5830,  -16.7460,\n",
      "         -60.7326,  -16.4033,  -63.4288,  -91.7442, -110.2135,  -25.5293,\n",
      "        -402.5036, -531.7723,  -19.9430, -418.6810,  -24.5592, -428.5823,\n",
      "         -77.7161,  -10.6675], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 68930.0859375\n",
      "Outputs tensor([ 3.0759e-01, -5.4104e+00, -2.7846e+02, -1.1516e+02, -3.0159e+02,\n",
      "        -4.4407e+02,  7.5171e-01, -5.1071e+00, -2.3283e+00,  2.7260e+00,\n",
      "        -1.4800e+01, -4.5309e+02,  5.9418e+00, -5.5392e+00,  3.8875e+00,\n",
      "         3.4228e+00,  8.6940e+00, -3.0388e+00, -5.8069e+00, -8.9412e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50877.15625\n",
      "Outputs tensor([  -6.9117,  -17.2278, -142.0479,   -6.5307, -565.1553,  -16.9727,\n",
      "          -9.4566,   -1.4831,   -8.5317, -483.2474, -524.0765,   -6.5487,\n",
      "          -5.2301,  -14.7281,   -3.6032,   -3.9960,   -2.3573,   -5.8396,\n",
      "         -10.2607,  -18.0946], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34621.984375\n",
      "Epoch training loss: 40842.934895833336\n",
      "Epoch validation loss: tensor([[36651.7617]])\n",
      "\n",
      "On epoch: 90\n",
      "Outputs tensor([ -15.4040, -182.2343,   -7.0306,   -6.0931,  -13.0819,    1.2052,\n",
      "           4.4577,   -5.5795,    5.1685, -495.9806,   -4.6852,   -3.6903,\n",
      "        -497.0437,   -9.2440,   -0.7428,   -4.2456,   -6.6142,    2.4676,\n",
      "        -486.0781,   -6.2420], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42912.23046875\n",
      "Outputs tensor([-3.6311e+02, -4.7084e+02, -1.9902e+00, -1.0507e+01, -5.8581e-01,\n",
      "        -1.0531e+01,  2.4878e+00,  2.8929e-01, -3.6296e+02, -3.5159e+02,\n",
      "         4.2324e+00, -4.1253e+00, -3.5877e+02, -5.3971e+00, -5.1929e+02,\n",
      "        -9.3362e+01, -3.1917e+02, -1.7941e+01,  2.2616e+00,  5.3829e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36839.0625\n",
      "Outputs tensor([ -16.8652,  -10.1714,  -15.0065,  -26.0532,  -22.6603, -571.2317,\n",
      "         -18.5682,  -16.0232,  -20.1251, -368.7934,  -17.2085, -447.8304,\n",
      "         -19.3956,  -13.6482,  -11.1194,  -16.3371,  -59.4572,  -17.0183,\n",
      "         -80.7572, -709.8932], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 56316.51171875\n",
      "Outputs tensor([-5.3564e+00, -1.0223e+01, -2.9014e+00, -3.4347e+02, -1.7500e+01,\n",
      "        -5.8164e+02, -1.9007e+01, -1.1360e+01, -1.4383e+01, -1.5150e+02,\n",
      "        -2.1568e+02, -1.1612e+01, -2.8644e+01,  1.7583e-01, -6.4050e+00,\n",
      "        -7.3622e+00, -8.5974e+00, -3.0336e+00, -1.1636e+01, -4.6503e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28897.05859375\n",
      "Outputs tensor([ 8.1290e-01, -4.3545e+02, -4.3270e-01, -1.4915e+01, -3.0478e+02,\n",
      "        -4.3829e+02, -9.1630e+00, -3.7803e+00, -2.8984e+02, -6.1591e+00,\n",
      "        -1.2036e+01, -9.4152e+00, -1.9017e+01, -6.9699e+00, -1.8553e+01,\n",
      "        -1.7443e+01, -3.3629e+00, -1.9452e+00, -3.4571e+02, -2.7134e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41963.765625\n",
      "Outputs tensor([-467.2708,   -5.7077,   -4.7934,  -16.3513,  -73.1106,  -20.1919,\n",
      "          -7.6992,   -7.3032,  -10.8041,   -9.7520,  -11.3599,  -18.9218,\n",
      "        -443.3130,   -9.9170, -716.7580,  -17.2848,  -18.1272,  -12.3886,\n",
      "         -18.2371, -584.2371], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39133.3046875\n",
      "Epoch training loss: 41010.322265625\n",
      "Epoch validation loss: tensor([[37198.2422]])\n",
      "\n",
      "On epoch: 91\n",
      "Outputs tensor([ -19.9897,   -9.9078,  -23.6128, -608.5797,   -6.6632,   -5.9686,\n",
      "         -18.0426, -257.9101,  -11.9190,  -15.8267,  -10.0076,  -13.8913,\n",
      "        -470.5983, -520.4799,  -18.2732,   -5.2088,   -5.5236,   -7.4281,\n",
      "          -6.3775,  -10.7617], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29556.693359375\n",
      "Outputs tensor([ -16.9778, -474.6807,   -8.2509,  -14.1519,   -4.6344, -458.4676,\n",
      "        -422.9230,   -5.8469,  -10.2504,   -7.1215, -379.0357, -287.0924,\n",
      "         -13.3673,   -3.7474,   -9.3839,   -7.4617, -125.7108,   -6.6783,\n",
      "        -301.2907,   -9.5688], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 40059.20703125\n",
      "Outputs tensor([-671.8030,   -5.8116, -432.6512,   -6.5629, -444.4059,  -96.5377,\n",
      "        -509.1839,   -5.8972,  -14.8437,   -5.0544,  -20.6785,   -6.8922,\n",
      "         -14.6601,   -5.6658,  -11.3656,   -6.2564, -563.4085,   -4.1630,\n",
      "          -4.6547, -544.4649], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24624.91015625\n",
      "Outputs tensor([-462.4056, -172.0679,   -2.7913, -419.4479,   -3.7076,   -9.7628,\n",
      "          -0.7588,    3.8360,  -13.6809,   -3.3211,   -3.1515,   -7.3012,\n",
      "          -9.6419,  -20.1521, -472.3402,  -14.4451, -435.8164, -557.2115,\n",
      "        -491.8900,    1.7926], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 44582.77734375\n",
      "Outputs tensor([ -22.6294,  -14.9820, -120.6088,  -22.4866,  -27.6116,  -12.8715,\n",
      "         -14.8301,  -14.7653,  -23.7817, -443.2909, -241.5235,   -9.3799,\n",
      "        -346.4835, -445.0315, -668.8916,  -11.9275, -419.6010,  -10.9707,\n",
      "          -7.7489, -445.6423], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 50387.18359375\n",
      "Outputs tensor([  -7.3134, -564.8056,  -20.1076,   -2.6238,  -32.3709,   -8.5779,\n",
      "          -6.4540,   -6.9846,  -11.2929,  -15.9897,   -6.6808,   -7.4558,\n",
      "         -81.4353, -382.5781,   -9.7538,   -6.8601, -366.6540,  -16.3571,\n",
      "         -90.6156,  -12.8621], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32862.7265625\n",
      "Epoch training loss: 37012.249674479164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch validation loss: tensor([[38088.5352]])\n",
      "\n",
      "On epoch: 92\n",
      "Outputs tensor([-3.5606e+00, -7.2669e+00, -3.6133e+02, -4.9858e+01, -1.1495e+01,\n",
      "         7.1268e-02, -3.4842e+00, -3.4856e+00, -1.5319e+00, -2.4058e+02,\n",
      "        -1.1162e+01, -4.4162e+02, -1.1086e+02, -8.2411e+00, -1.2168e+01,\n",
      "        -2.0129e+02, -9.7604e+00, -1.0819e+00,  3.5580e-02, -9.1076e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 58065.19921875\n",
      "Outputs tensor([ -14.2181, -582.4659,  -14.2015,  -24.8736,  -40.0302,  -19.3786,\n",
      "          -7.6444,   -9.2017,  -19.5583, -359.5028,   -9.7844,  -21.6053,\n",
      "         -72.4018,  -15.4820, -578.8223, -602.9250, -316.8872,   -8.4859,\n",
      "         -57.0816,  -11.0148], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24574.11328125\n",
      "Outputs tensor([ -23.3782, -180.0767,  -12.9581, -542.4366,  -45.9004, -288.6250,\n",
      "          -8.8164, -405.0290,  -16.2969, -564.0381, -325.5338,  -57.2025,\n",
      "         -12.1749,   -9.1964,  -12.2407, -481.7052,   -7.7681,  -23.0414,\n",
      "         -10.6447,  -28.7419], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 28953.228515625\n",
      "Outputs tensor([-550.3934,  -14.1211,   -8.3414, -222.2115,  -13.4075,  -12.4169,\n",
      "         -20.1687, -223.6814, -389.2667,  -17.0613,  -12.3316,  -12.2890,\n",
      "         -18.3977,  -23.6789,  -17.8998,   -8.2588,  -26.3698,  -16.0809,\n",
      "         -19.7462,  -35.6555], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37329.4609375\n",
      "Outputs tensor([ -14.4464,   -6.2699,   -8.3396,   -5.0586, -327.1289,   -7.5831,\n",
      "        -448.6835,  -14.6076,   -6.6148,   -8.9902,   -3.1914, -476.8182,\n",
      "        -444.0172,  -14.7748,  -11.0069,   -5.5849, -217.0139,   -4.0688,\n",
      "          -9.8983, -509.6805], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32230.98828125\n",
      "Outputs tensor([ -12.6847,  -20.1462,  -28.8887,  -15.0590,  -32.9910,   -3.1970,\n",
      "          -1.7305,   -9.8433,   -3.2395, -721.1256, -463.3769, -200.2884,\n",
      "         -10.8836,    2.4754,   -8.0895,   -9.6730, -435.0026,   -9.2865,\n",
      "        -260.8097,  -11.0757], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32659.390625\n",
      "Epoch training loss: 35635.396809895836\n",
      "Epoch validation loss: tensor([[31715.1484]])\n",
      "\n",
      "On epoch: 93\n",
      "Outputs tensor([-563.7189,   -5.7132,  -12.6808,  -14.4121,  -10.1255,   -8.8851,\n",
      "        -310.6522, -105.5663,   -7.0385,   -8.1569,  -10.0288,  -87.9149,\n",
      "         -17.9443,  -17.3265, -499.0265, -469.8610,   -9.0444,  -10.6240,\n",
      "        -432.0334,   -9.4412], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32912.98046875\n",
      "Outputs tensor([-3.8082e+02, -6.5746e+00, -6.4539e+00, -1.6534e+01, -2.1229e+00,\n",
      "        -4.7718e+02, -3.7979e+02, -8.9978e+00, -5.0569e+02, -9.4235e-01,\n",
      "        -1.5574e-01, -6.8256e+00, -1.0069e+01, -5.2473e+02, -1.4170e+01,\n",
      "        -1.5923e+01, -3.4139e+02, -3.2003e+02, -7.7982e+00, -1.2424e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25358.966796875\n",
      "Outputs tensor([ -10.0714,  -20.4541,  -65.4865,  -12.0989, -722.1397,   -5.3272,\n",
      "        -477.4081,   -3.7368,  -20.4170, -335.5829,   -8.2614,   -4.7094,\n",
      "          -7.3510, -570.2631,  -54.0652, -666.4831,   -7.4292,  -18.3706,\n",
      "         -19.6999,   -5.3195], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32496.037109375\n",
      "Outputs tensor([-302.4529, -283.6313,  -18.9625,  -10.9415,   -8.9655, -464.1849,\n",
      "        -438.8023, -116.3080,  -55.9290,  -11.8655,   -9.4960, -221.5566,\n",
      "        -110.6191,  -12.0935,  -46.1131, -110.4801, -267.3589,  -99.0953,\n",
      "         -47.1041, -700.6431], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 58986.80078125\n",
      "Outputs tensor([  -4.6348, -230.6351,   -6.0810, -408.7029,   -4.3650, -335.9080,\n",
      "          -2.1119,  -10.9346,   -0.4620, -381.3924,   -5.9353,   -3.4439,\n",
      "          -1.5727,   -8.9751,  -13.0721,   -6.0398,  -11.6795,   -5.4589,\n",
      "         -12.7884, -389.6323], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41172.65234375\n",
      "Outputs tensor([  -4.1163,  -13.8016,  -13.1468, -234.2413, -486.3912,  -16.5698,\n",
      "          -5.5010,   -6.5883,   -4.5455,   -4.4217,  -15.0936, -211.5266,\n",
      "          -8.9921, -527.4764,  -13.0132, -400.0143,   -6.4372,   -5.6470,\n",
      "         -11.9543, -158.2054], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 41585.27734375\n",
      "Epoch training loss: 38752.119140625\n",
      "Epoch validation loss: tensor([[36834.0078]])\n",
      "\n",
      "On epoch: 94\n",
      "Outputs tensor([ -47.0687,    1.3755,   -4.4365, -112.4405, -400.5032,   -5.1885,\n",
      "          -7.3310,   -2.4030, -328.7676, -428.2899,   -8.0844,  -16.0490,\n",
      "         -11.7134,  -15.8198,   -4.7634,   -5.7229,  -13.0032, -532.5012,\n",
      "        -432.8315,   -4.9612], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25813.392578125\n",
      "Outputs tensor([-495.1125,  -31.8828,  -12.5415, -549.2291,  -10.2870,  -12.7542,\n",
      "          -7.5537,  -15.6610,   -7.5312,  -13.5515,   -4.2376, -497.0107,\n",
      "        -393.8425, -548.3646,   -4.0697,  -12.1157,  -11.9563,   -9.0089,\n",
      "         -12.3479,   -7.7070], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 26660.875\n",
      "Outputs tensor([-581.1218,   -6.6597,  -24.3463,  -20.5605,  -13.8400,   -7.1545,\n",
      "          -6.9376,   -9.7704, -214.8441,  -14.8714,   -8.9952, -666.6373,\n",
      "         -13.6409,   -6.7765, -500.6560, -672.2942,  -12.3598,  -13.7416,\n",
      "          -6.6359,   -2.1418], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33858.0546875\n",
      "Outputs tensor([  -9.6930,  -54.5157,   -6.5978,  -36.6804,   -9.0970, -123.5387,\n",
      "        -440.9762,   -6.6914,  -17.4160,  -12.5866,   -9.8382,  -19.9891,\n",
      "        -455.1808,   -8.8723, -647.7980,  -12.2270,  -15.4876, -155.0581,\n",
      "         -15.6965,   -8.6353], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 34650.7734375\n",
      "Outputs tensor([-246.0908,   -3.0887,   -7.2323,   -9.3647, -436.5169,  -13.0721,\n",
      "         -11.8048,   -7.5517, -364.5284,   -4.0306,   -3.6105,  -21.3008,\n",
      "        -232.6566,   -9.6375,   -5.7903,  -10.2246,   -5.2870,   -7.4190,\n",
      "        -345.6322, -235.0333], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37961.73828125\n",
      "Outputs tensor([-182.0053, -216.7226, -496.5269,   -7.8996,  -19.6810,   -7.0580,\n",
      "         -16.5583,  -10.1994,  -20.0899, -191.6916, -162.6343,   -7.5540,\n",
      "         -10.8932,   -5.8241, -126.9870,   -7.6434, -562.5347,   -3.4474,\n",
      "        -380.5271,   -5.5546], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 55321.8515625\n",
      "Epoch training loss: 35711.1142578125\n",
      "Epoch validation loss: tensor([[30213.2090]])\n",
      "\n",
      "On epoch: 95\n",
      "Outputs tensor([  -4.5500,  -18.6036,  -35.9457,   -7.8651, -519.6337,   -4.1775,\n",
      "          -2.5829,   -4.2564,  -10.5889,  -14.6329,   -6.0995, -511.8594,\n",
      "         -19.8234,   -9.7033, -507.1344,  -10.8196,   -7.2360,   -9.7251,\n",
      "        -465.9163, -495.5192], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 21063.462890625\n",
      "Outputs tensor([  -5.7497, -447.5638, -558.4018,  -17.9031, -510.7231,  -25.7307,\n",
      "         -10.5534, -349.8859,   -9.9944, -522.3632, -210.0543,  -15.2970,\n",
      "          -5.9463,  -11.3766,   -7.5626,  -45.1318,   -9.1014,   -7.0850,\n",
      "          -7.2457,   -5.9678], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37109.21875\n",
      "Outputs tensor([ -15.5968,   -4.5316,  -94.1149,   -6.1677, -506.2040,  -20.1840,\n",
      "         -10.4575,   -2.7252, -650.2352, -510.7315,   -2.3058, -229.9465,\n",
      "         -13.5712, -293.1415,   -4.8068,  -10.4552,   -3.5166,  -10.0657,\n",
      "          -2.7747,  -16.9107], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27440.677734375\n",
      "Outputs tensor([ -29.8169,   -8.4092,   -7.1782,   -7.5583,   -9.7053, -173.0067,\n",
      "         -61.2629,  -25.0407, -646.3812,  -96.4603,   -8.7880, -379.3812,\n",
      "         -14.3877,   -9.2941,  -15.6917,  -39.7155,  -13.7270, -358.7622,\n",
      "          -6.2396,  -11.4348], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39692.37890625\n",
      "Outputs tensor([ -19.3198, -683.5242, -563.1624, -481.2201,  -18.0090,  -96.9400,\n",
      "          -9.9929, -592.3615,  -13.4187, -511.4866,  -17.3056,  -15.7520,\n",
      "         -10.7787,   -4.5397, -576.2270, -128.0828,  -12.6848,  -21.2528,\n",
      "         -10.3604,  -11.7509], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30181.58203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ -14.4181,  -20.2429,  -98.2420, -381.2557,  -10.1395,  -10.4861,\n",
      "        -546.9937,  -20.3020, -734.3074, -555.3887,  -15.7143,  -17.4608,\n",
      "         -12.3170,  -52.7166,  -40.2205,  -12.5629,  -12.0424,  -13.7131,\n",
      "        -198.0040,  -10.8737], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 36324.1328125\n",
      "Epoch training loss: 31968.575520833332\n",
      "Epoch validation loss: tensor([[37174.0273]])\n",
      "\n",
      "On epoch: 96\n",
      "Outputs tensor([-310.4468,  -19.3008,   -7.6207,   -8.0755, -403.7179, -391.2567,\n",
      "          -6.6707,  -11.0460,  -17.7113, -391.0448,  -12.4871,  -13.9782,\n",
      "         -10.2741,   -6.7063,   -5.9610,   -3.8281,   -6.7522,   -4.7431,\n",
      "        -364.6257, -377.8505], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 37847.8515625\n",
      "Outputs tensor([-404.1747,  -15.1962, -434.3787,  -10.1639,   -9.4416,  -23.3110,\n",
      "          -9.9176,  -29.7034,  -36.0713,  -21.9454, -486.1861, -593.5092,\n",
      "          -9.6355,  -10.1306, -425.1119,  -14.1400, -571.3459,   -8.8988,\n",
      "         -13.4583, -249.9291], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 25437.126953125\n",
      "Outputs tensor([  -6.2239, -583.6263,   -5.4488, -586.5959,   -4.9194,  -17.7720,\n",
      "          -7.5515,  -99.3055,  -15.4147,   -3.6444,   -8.7928,  -68.4939,\n",
      "          -6.3619,   -3.7309, -442.0881, -265.0011,  -14.2469, -105.7578,\n",
      "          -7.4905,  -11.1445], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39933.59765625\n",
      "Outputs tensor([-246.7901,  -11.0395,  -25.9718, -537.3738,  -12.0735,  -52.9621,\n",
      "         -14.0718, -500.2713, -674.6656,   -3.8979,   -6.2518,   -4.2213,\n",
      "        -411.8025, -469.5970,  -11.7752,   -3.2316,   -5.9180,   -1.9224,\n",
      "         -13.3676,   -5.4975], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 27296.943359375\n",
      "Outputs tensor([-4.2984e+00, -3.1055e+00, -1.1567e+01, -1.3933e+01, -4.4029e+02,\n",
      "        -4.7283e+02, -8.7831e+00, -4.5824e+00, -1.7341e+00, -9.2840e+01,\n",
      "        -3.8659e+02, -1.1586e-01, -4.9437e+02, -6.2232e+00, -9.6402e+00,\n",
      "        -1.0054e+01, -9.1891e+00, -6.6392e+02, -2.5330e+00, -1.3704e+01],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 33636.5859375\n",
      "Outputs tensor([-2.0183e+01, -4.8247e+00, -2.8961e+01, -2.7060e+02, -2.7556e+01,\n",
      "        -7.5560e+00, -3.7014e+00, -1.0968e+01, -5.4852e+00, -4.7058e+01,\n",
      "        -9.6138e-01, -3.2232e+01, -1.5196e+02, -7.9165e+00, -3.8639e-02,\n",
      "        -5.3048e+02, -5.6277e+00, -8.2776e+00, -5.3695e+02, -1.3661e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23934.689453125\n",
      "Epoch training loss: 31347.799153645832\n",
      "Epoch validation loss: tensor([[33843.6406]])\n",
      "\n",
      "On epoch: 97\n",
      "Outputs tensor([  -6.1845, -431.4787,  -10.4517,   -5.5701,   -7.7270,   -5.4488,\n",
      "         -16.9373,   -8.5478,  -25.9292,   -8.9288,  -14.3506,   -4.6591,\n",
      "        -507.1389,   -3.8530, -506.4756,   -5.5609,  -12.6946,   -1.4643,\n",
      "          -6.0722, -189.2010], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 30303.775390625\n",
      "Outputs tensor([ -14.6059,   -9.6178,  -15.5483, -384.6549,   -5.8769, -150.8880,\n",
      "         -31.0842, -355.0729,   -3.6934, -359.5138,  -13.4601,   -5.0790,\n",
      "          -9.1877,  -19.0312,   -8.4610,  -12.2175, -477.8407,   -4.9819,\n",
      "          -7.7004,  -12.1020], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45450.2265625\n",
      "Outputs tensor([ -92.1467,   -8.3543,   -8.0385,  -30.2926,   -9.3200,   -8.4073,\n",
      "         -17.0847,  -14.3200, -685.0823,  -27.2966, -463.3684,  -11.7129,\n",
      "         -20.7146,  -16.3330, -108.4425,   -9.1754, -381.9026,  -12.1205,\n",
      "        -380.2364,   -4.0106], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 42232.609375\n",
      "Outputs tensor([-2.9973e+00, -8.3041e+00,  2.6930e+00, -6.0641e+00, -4.7609e+02,\n",
      "        -8.5114e+00, -1.4558e+01, -2.5229e+01, -5.0828e+00, -3.0700e+02,\n",
      "        -3.5052e+02, -3.0088e+02, -5.7485e+00, -7.4699e+00, -1.4566e+01,\n",
      "         3.6431e-01, -7.2535e+00, -1.2009e+01, -6.2993e+00, -3.0475e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 32423.61328125\n",
      "Outputs tensor([ 2.1643e-01, -2.0193e-01, -4.8245e+02, -5.3105e+02,  5.1933e-02,\n",
      "        -1.5589e+00, -1.0198e+01, -2.6873e+02, -1.0306e+01, -6.2681e+02,\n",
      "        -9.5646e+00, -6.6926e+00, -2.2582e+00, -8.1907e+00, -1.9208e+00,\n",
      "        -1.1737e+01, -5.6316e+02, -3.5445e+02, -5.6257e+02, -4.0937e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24303.921875\n",
      "Outputs tensor([-255.9924, -380.8861,  -85.6936,   -4.4686,  -18.4763,  -13.5069,\n",
      "        -302.9544,  -76.2134,   -7.5934,   -7.0843,   -4.0570, -641.1292,\n",
      "          -7.7573, -256.5473,   -6.3814, -224.5254,  -11.3043, -245.2435,\n",
      "        -374.1020,  -18.3218], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 57907.23046875\n",
      "Epoch training loss: 38770.2294921875\n",
      "Epoch validation loss: tensor([[33226.7266]])\n",
      "\n",
      "On epoch: 98\n",
      "Outputs tensor([  -5.3680,  -11.6808,  -19.1512,  -17.7060,   -6.5766,  -35.9200,\n",
      "         -19.6989, -202.9491, -411.1469,  -10.6339,   -9.7413,   -7.1472,\n",
      "         -32.6552,   -5.8761,  -54.9306,  -18.0779,  -55.6444, -353.3352,\n",
      "         -15.3947,   -8.5166], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 45582.09765625\n",
      "Outputs tensor([   3.3468,   -1.0774,    4.7778,    3.1811,    2.0270,    5.9426,\n",
      "          -3.7651,    5.6410, -307.1584, -329.7182,   -9.4479,    1.5139,\n",
      "          -6.3033, -402.6657,   -9.4977, -414.9893,  -45.6534, -245.1072,\n",
      "           3.5023,  -11.2947], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 74128.203125\n",
      "Outputs tensor([ -15.4353,  -16.6615,  -39.8714,  -13.4496,  -31.4888,  -14.2261,\n",
      "         -15.5844, -595.3369,  -19.6625,  -70.2143,  -24.2034, -116.9337,\n",
      "         -12.4020,  -20.4617,  -13.1755,  -15.6266,  -22.4233, -644.3935,\n",
      "         -17.3661,  -20.2397], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 24568.20703125\n",
      "Outputs tensor([ -14.3211,  -14.1862,  -14.5892,  -20.1261,  -48.4263,  -13.7191,\n",
      "         -14.4176,   -9.2880,  -23.8463, -371.3540, -339.8882,  -29.8963,\n",
      "         -17.8692,  -28.5905, -564.9811, -100.0291, -237.9565,  -25.2153,\n",
      "        -461.2773,  -13.3740], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 29021.349609375\n",
      "Outputs tensor([ -28.1835, -494.5175, -508.0750,  -36.6139, -384.4973, -555.3568,\n",
      "         -60.8134,  -19.1363, -423.1804,  -18.1862,  -20.5087,  -27.2601,\n",
      "         -13.4050,  -14.4464,  -16.3102, -199.1003,  -16.7765,  -22.1195,\n",
      "        -176.1528, -606.9408], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23557.39453125\n",
      "Outputs tensor([ -38.4423,  -16.6952, -629.4866,  -12.4959,  -13.7878,  -10.5494,\n",
      "        -150.6674, -376.7198,  -16.6659,  -10.8420, -238.9494, -692.1440,\n",
      "        -584.4280,  -79.4879,  -17.4238,  -16.3283,  -20.8133,  -22.6468,\n",
      "        -487.8231,   -9.0264], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 35344.671875\n",
      "Epoch training loss: 38700.320638020836\n",
      "Epoch validation loss: tensor([[33366.4688]])\n",
      "\n",
      "On epoch: 99\n",
      "Outputs tensor([-1.7131e+02, -1.3472e+02, -1.9262e+01, -1.4572e+01,  3.9778e-01,\n",
      "        -1.5842e+01, -5.6525e+02, -1.9141e+01, -3.1261e+02, -8.5192e+00,\n",
      "        -6.6665e+00, -3.1060e+00, -1.5898e+01, -9.2629e+00, -5.4902e+02,\n",
      "        -7.3556e+00, -5.6778e+02, -7.2177e+00, -9.6524e+00, -5.2007e+02],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47823.6328125\n",
      "Outputs tensor([ -26.0055,  -11.3825,  -19.1071, -291.2596,  -23.4179,  -12.8272,\n",
      "         -12.9812, -240.6341, -527.2717, -631.7478,  -16.0376,  -12.8550,\n",
      "         -10.7272,  -64.5722, -537.2546,  -13.9645, -533.9275, -687.0156,\n",
      "         -10.6972,  -18.7273], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 47585.2109375\n",
      "Outputs tensor([ -10.7656,   -5.6329,  -12.5452,   -6.9908, -520.5381,   -5.5851,\n",
      "          -4.1926,   -3.6800,  -94.6564,  -17.1873,  -11.1383,   -8.9968,\n",
      "          -9.6493, -308.5170,  -27.4493,  -10.1080,  -15.3447, -452.1245,\n",
      "        -408.2524,  -12.5672], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 23046.853515625\n",
      "Outputs tensor([-352.2751,   -2.7439,  -14.1028,   -6.9541,   -8.8448, -183.5380,\n",
      "          -5.7022,   -6.2717,   -4.6331,   -8.6001, -311.7101,  -10.5974,\n",
      "          -2.0714,  -44.6195, -405.9078, -469.5805,  -16.5184, -438.8043,\n",
      "         -19.1234, -223.0091], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 43827.98046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor([ -12.1465, -568.1345,  -17.6258,  -11.6241,  -14.1585,  -12.6436,\n",
      "         -36.0855,  -67.7314,  -15.5839, -127.8419,  -15.2917,  -72.8548,\n",
      "          -7.3263, -342.1699,  -20.3834,  -30.1604,  -59.5917,  -11.2740,\n",
      "        -286.8084, -578.5343], grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 39872.0625\n",
      "Outputs tensor([-1.7319e+00, -9.3398e+00, -1.0478e+01, -5.4206e-01, -9.8374e+00,\n",
      "        -4.9045e+02, -2.8164e+01, -2.5409e+02, -1.0812e+01, -7.9066e+00,\n",
      "        -1.9128e+01, -2.0946e+01,  3.3273e-01, -4.3500e+00, -1.1233e-01,\n",
      "        -5.5376e+02, -1.3493e+01, -1.1510e+01, -1.8014e+00, -9.2833e+00],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Batch size: 20\n",
      "Batch average loss 31269.375\n",
      "Epoch training loss: 38904.185872395836\n",
      "Epoch validation loss: tensor([[32595.1152]])\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# For early stopping, use blocks of epochs (say size 3), and a window (say size 10) of how far to look in the past.\n",
    "# Then, compare the average loss on the current block to the block in the past. Another idea would be compare the variances.\n",
    "# If the fraction is not sufficiently small, halt.\n",
    "\n",
    "max_epochs = 100\n",
    "# Should be stated as a fraction of the previous error\n",
    "max_frac = 0.999\n",
    "window = 10 # How many epochs in the past to compare to\n",
    "block = 5 # What size block of epochs to use\n",
    "\n",
    "epoch_training_loss = []\n",
    "epoch_val_loss = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"\\nOn epoch: \" + str(epoch))\n",
    "    \n",
    "    running_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    net.train()\n",
    "    for inputs, labels in training_generator:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(torch.squeeze(outputs), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Outputs', torch.squeeze(outputs))\n",
    "        print('Batch size: ' + str(len(inputs)))\n",
    "        running_loss += loss.item() * len(inputs) # Multiply by the batch size\n",
    "        print('Batch average loss ' + str(loss.item()))\n",
    "    training_loss = running_loss / training_set.__len__()\n",
    "    print(\"Epoch training loss: \" + str(training_loss))\n",
    "    epoch_training_loss.append(training_loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        net.eval()\n",
    "        for ind in validation_set.ids:\n",
    "            X, y = validation_set.__getitem__(ind)\n",
    "            X = X.unsqueeze(0) # Add fake batch dimension\n",
    "            yHat = net(X.float())\n",
    "            val_loss += (y - yHat)**2\n",
    "        val_loss /= validation_set.__len__()\n",
    "        epoch_val_loss.append(val_loss)\n",
    "    print(\"Epoch validation loss: \" + str(val_loss))\n",
    "    \n",
    "    if len(epoch_val_loss) >= window + block + 1:\n",
    "        latestBlock = np.mean(epoch_val_loss[-1:-1-block])\n",
    "        earlierBlock = np.mean(epoch_val_loss[-1-window:-1-window-block])\n",
    "        \n",
    "        # latestBlock must be sufficiently smaller than earlierBlock\n",
    "        if latestBlock / earlierBlock > max_frac:\n",
    "            print('Converged')\n",
    "            pdb.set_trace()\n",
    "            break\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Results\n",
      "Sample: 114\n",
      "y: -457\n",
      "yHat: [[-553.08344]]\n",
      "Sample: 62\n",
      "y: 259\n",
      "yHat: [[-81.724304]]\n",
      "Sample: 33\n",
      "y: -159\n",
      "yHat: [[-2.3037307]]\n",
      "Sample: 107\n",
      "y: -278\n",
      "yHat: [[-6.502552]]\n",
      "Sample: 7\n",
      "y: 45\n",
      "yHat: [[-8.704995]]\n",
      "Sample: 100\n",
      "y: 290\n",
      "yHat: [[-18.260115]]\n",
      "Sample: 40\n",
      "y: 17\n",
      "yHat: [[-12.659433]]\n",
      "Sample: 86\n",
      "y: 288\n",
      "yHat: [[-113.88098]]\n",
      "Sample: 76\n",
      "y: -132\n",
      "yHat: [[-3.7797906]]\n",
      "Sample: 71\n",
      "y: -611\n",
      "yHat: [[-511.12024]]\n",
      "Sample: 134\n",
      "y: 84\n",
      "yHat: [[-11.00695]]\n",
      "Sample: 51\n",
      "y: -465\n",
      "yHat: [[-373.11786]]\n",
      "Sample: 73\n",
      "y: -361\n",
      "yHat: [[-12.095773]]\n",
      "Sample: 54\n",
      "y: 70\n",
      "yHat: [[-5.6661673]]\n",
      "Sample: 63\n",
      "y: -61\n",
      "yHat: [[-13.47722]]\n",
      "Sample: 37\n",
      "y: -538\n",
      "yHat: [[-508.8604]]\n",
      "Sample: 78\n",
      "y: -361\n",
      "yHat: [[-277.37064]]\n",
      "Sample: 90\n",
      "y: -343\n",
      "yHat: [[-378.21066]]\n",
      "Sample: 45\n",
      "y: 300\n",
      "yHat: [[-38.641758]]\n",
      "Sample: 16\n",
      "y: -525\n",
      "yHat: [[-437.4524]]\n",
      "Sample: 121\n",
      "y: -622\n",
      "yHat: [[-586.09863]]\n",
      "Sample: 66\n",
      "y: -259\n",
      "yHat: [[-0.9929946]]\n",
      "Sample: 24\n",
      "y: -604\n",
      "yHat: [[-537.6856]]\n",
      "Sample: 8\n",
      "y: -362\n",
      "yHat: [[-159.48254]]\n",
      "Sample: 126\n",
      "y: 14\n",
      "yHat: [[-15.990319]]\n",
      "Sample: 22\n",
      "y: 180\n",
      "yHat: [[-3.4756973]]\n",
      "Sample: 44\n",
      "y: -239\n",
      "yHat: [[-1.7332823]]\n",
      "Sample: 97\n",
      "y: -60\n",
      "yHat: [[-8.567288]]\n",
      "Sample: 93\n",
      "y: -37\n",
      "yHat: [[-20.09338]]\n",
      "Sample: 26\n",
      "y: -33\n",
      "yHat: [[-11.730168]]\n",
      "Sample: 137\n",
      "y: 197\n",
      "yHat: [[-0.25115377]]\n",
      "Sample: 84\n",
      "y: -563\n",
      "yHat: [[-546.51434]]\n",
      "Sample: 27\n",
      "y: -264\n",
      "yHat: [[-5.9769297]]\n",
      "Sample: 127\n",
      "y: -591\n",
      "yHat: [[-646.0859]]\n",
      "Sample: 132\n",
      "y: -330\n",
      "yHat: [[-142.26465]]\n",
      "Sample: 59\n",
      "y: -499\n",
      "yHat: [[-12.970435]]\n",
      "Sample: 18\n",
      "y: 76\n",
      "yHat: [[-8.398411]]\n",
      "Sample: 83\n",
      "y: -253\n",
      "yHat: [[-4.4403124]]\n",
      "Sample: 61\n",
      "y: -329\n",
      "yHat: [[-7.5736294]]\n",
      "Sample: 92\n",
      "y: -381\n",
      "yHat: [[-278.58466]]\n",
      "Sample: 112\n",
      "y: -369\n",
      "yHat: [[-182.2543]]\n",
      "Sample: 2\n",
      "y: -90\n",
      "yHat: [[-9.458862]]\n",
      "Sample: 141\n",
      "y: -65\n",
      "yHat: [[-11.429803]]\n",
      "Sample: 43\n",
      "y: -276\n",
      "yHat: [[-4.804361]]\n",
      "Sample: 10\n",
      "y: -333\n",
      "yHat: [[-199.58372]]\n",
      "Sample: 60\n",
      "y: 160\n",
      "yHat: [[-1.5949581]]\n",
      "Sample: 116\n",
      "y: 278\n",
      "yHat: [[-9.893861]]\n",
      "Sample: 144\n",
      "y: -402\n",
      "yHat: [[-188.51999]]\n",
      "Sample: 119\n",
      "y: 215\n",
      "yHat: [[-6.6704683]]\n",
      "Sample: 108\n",
      "y: 247\n",
      "yHat: [[-5.2986016]]\n",
      "Sample: 69\n",
      "y: 91\n",
      "yHat: [[-7.961196]]\n",
      "Sample: 135\n",
      "y: -634\n",
      "yHat: [[-675.4217]]\n",
      "Sample: 56\n",
      "y: -106\n",
      "yHat: [[-10.331355]]\n",
      "Sample: 80\n",
      "y: 217\n",
      "yHat: [[-1.0702994]]\n",
      "Sample: 123\n",
      "y: -433\n",
      "yHat: [[-300.87494]]\n",
      "Sample: 133\n",
      "y: 322\n",
      "yHat: [[-53.16555]]\n",
      "Sample: 106\n",
      "y: -532\n",
      "yHat: [[-406.5354]]\n",
      "Sample: 146\n",
      "y: 195\n",
      "yHat: [[-1.4921424]]\n",
      "Sample: 50\n",
      "y: -178\n",
      "yHat: [[-6.199148]]\n",
      "Sample: 147\n",
      "y: -262\n",
      "yHat: [[-11.668845]]\n",
      "Sample: 85\n",
      "y: -2\n",
      "yHat: [[-15.482706]]\n",
      "Sample: 30\n",
      "y: 274\n",
      "yHat: [[-2.818341]]\n",
      "Sample: 101\n",
      "y: -55\n",
      "yHat: [[-11.855568]]\n",
      "Sample: 94\n",
      "y: -532\n",
      "yHat: [[-525.65247]]\n",
      "Sample: 64\n",
      "y: -247\n",
      "yHat: [[-2.3923643]]\n",
      "Sample: 89\n",
      "y: 285\n",
      "yHat: [[-11.746841]]\n",
      "Sample: 91\n",
      "y: -169\n",
      "yHat: [[-2.469344]]\n",
      "Sample: 125\n",
      "y: -33\n",
      "yHat: [[-13.92436]]\n",
      "Sample: 48\n",
      "y: -209\n",
      "yHat: [[-3.2438872]]\n",
      "Sample: 13\n",
      "y: -305\n",
      "yHat: [[-13.248829]]\n",
      "Sample: 111\n",
      "y: -425\n",
      "yHat: [[-357.6162]]\n",
      "Sample: 95\n",
      "y: -38\n",
      "yHat: [[-9.989749]]\n",
      "Sample: 20\n",
      "y: -497\n",
      "yHat: [[-436.07428]]\n",
      "Sample: 15\n",
      "y: 19\n",
      "yHat: [[-16.733677]]\n",
      "Sample: 52\n",
      "y: 21\n",
      "yHat: [[-13.021911]]\n",
      "Sample: 3\n",
      "y: -267\n",
      "yHat: [[-6.2570004]]\n",
      "Sample: 149\n",
      "y: -218\n",
      "yHat: [[-2.704004]]\n",
      "Sample: 98\n",
      "y: -38\n",
      "yHat: [[-12.280146]]\n",
      "Sample: 6\n",
      "y: 272\n",
      "yHat: [[-12.673964]]\n",
      "Sample: 68\n",
      "y: -399\n",
      "yHat: [[-171.24695]]\n",
      "Sample: 109\n",
      "y: -27\n",
      "yHat: [[-53.317055]]\n",
      "Sample: 96\n",
      "y: 154\n",
      "yHat: [[-4.1544228]]\n",
      "Sample: 12\n",
      "y: 276\n",
      "yHat: [[-5.7086897]]\n",
      "Sample: 102\n",
      "y: 62\n",
      "yHat: [[-11.043716]]\n",
      "Sample: 120\n",
      "y: -309\n",
      "yHat: [[-19.60098]]\n",
      "Sample: 104\n",
      "y: -634\n",
      "yHat: [[-525.4031]]\n",
      "Sample: 128\n",
      "y: -257\n",
      "yHat: [[-5.576553]]\n",
      "Sample: 46\n",
      "y: -580\n",
      "yHat: [[-498.69388]]\n",
      "Sample: 11\n",
      "y: -226\n",
      "yHat: [[-5.9961596]]\n",
      "Sample: 110\n",
      "y: -10\n",
      "yHat: [[-18.242409]]\n",
      "Sample: 124\n",
      "y: 173\n",
      "yHat: [[-4.626799]]\n",
      "Sample: 41\n",
      "y: -228\n",
      "yHat: [[-3.6973774]]\n",
      "Sample: 148\n",
      "y: 269\n",
      "yHat: [[-16.754221]]\n",
      "Sample: 1\n",
      "y: 265\n",
      "yHat: [[-5.3908973]]\n",
      "Sample: 113\n",
      "y: 279\n",
      "yHat: [[-12.572362]]\n",
      "Sample: 139\n",
      "y: 38\n",
      "yHat: [[-9.423455]]\n",
      "Sample: 42\n",
      "y: 190\n",
      "yHat: [[-4.9494267]]\n",
      "Sample: 4\n",
      "y: 50\n",
      "yHat: [[-9.791854]]\n",
      "Sample: 129\n",
      "y: 211\n",
      "yHat: [[-4.9409165]]\n",
      "Sample: 17\n",
      "y: 124\n",
      "yHat: [[-6.232856]]\n",
      "Sample: 38\n",
      "y: -28\n",
      "yHat: [[-11.723222]]\n",
      "Sample: 5\n",
      "y: 109\n",
      "yHat: [[-7.8222966]]\n",
      "Sample: 53\n",
      "y: -103\n",
      "yHat: [[-7.311444]]\n",
      "Sample: 143\n",
      "y: -262\n",
      "yHat: [[-8.573943]]\n",
      "Sample: 105\n",
      "y: -356\n",
      "yHat: [[-251.80576]]\n",
      "Sample: 0\n",
      "y: -486\n",
      "yHat: [[-449.0539]]\n",
      "Sample: 34\n",
      "y: -139\n",
      "yHat: [[-3.4425719]]\n",
      "Sample: 28\n",
      "y: -51\n",
      "yHat: [[-11.017063]]\n",
      "Sample: 55\n",
      "y: 249\n",
      "yHat: [[-5.1776986]]\n",
      "Sample: 75\n",
      "y: -442\n",
      "yHat: [[-467.13092]]\n",
      "Sample: 35\n",
      "y: 42\n",
      "yHat: [[-9.681241]]\n",
      "Sample: 23\n",
      "y: -179\n",
      "yHat: [[-5.326562]]\n",
      "Sample: 74\n",
      "y: -252\n",
      "yHat: [[-2.2998192]]\n",
      "Sample: 31\n",
      "y: -129\n",
      "yHat: [[-6.2413383]]\n",
      "Sample: 118\n",
      "y: -329\n",
      "yHat: [[-64.85469]]\n",
      "Sample: 57\n",
      "y: 133\n",
      "yHat: [[-6.7718167]]\n",
      "Sample: 131\n",
      "y: -199\n",
      "yHat: [[-3.6366403]]\n",
      "Sample: 65\n",
      "y: -232\n",
      "yHat: [[-5.4949727]]\n",
      "Sample: 32\n",
      "y: 240\n",
      "yHat: [[-6.5385394]]\n",
      "Sample: 138\n",
      "y: -147\n",
      "yHat: [[-3.7497027]]\n",
      "RMSE on dataset: [[188.9707]]\n",
      "Avg Abs Dev on dataset: [[157.00372]]\n",
      "\n",
      "Validation Set Results\n",
      "Sample: 14\n",
      "y: -149\n",
      "yHat: [[-6.390817]]\n",
      "Sample: 122\n",
      "y: 243\n",
      "yHat: [[-2.491883]]\n",
      "Sample: 19\n",
      "y: 87\n",
      "yHat: [[-11.308266]]\n",
      "Sample: 29\n",
      "y: -597\n",
      "yHat: [[-486.616]]\n",
      "Sample: 130\n",
      "y: -246\n",
      "yHat: [[-5.7027774]]\n",
      "Sample: 49\n",
      "y: -41\n",
      "yHat: [[-13.6351185]]\n",
      "Sample: 136\n",
      "y: 133\n",
      "yHat: [[-5.7609587]]\n",
      "Sample: 99\n",
      "y: -133\n",
      "yHat: [[-6.0811014]]\n",
      "Sample: 82\n",
      "y: -13\n",
      "yHat: [[-15.8635]]\n",
      "Sample: 79\n",
      "y: 279\n",
      "yHat: [[-76.66525]]\n",
      "Sample: 115\n",
      "y: -438\n",
      "yHat: [[-354.85324]]\n",
      "Sample: 145\n",
      "y: -268\n",
      "yHat: [[-12.464134]]\n",
      "Sample: 72\n",
      "y: 160\n",
      "yHat: [[-0.84874946]]\n",
      "Sample: 77\n",
      "y: -320\n",
      "yHat: [[-63.52202]]\n",
      "Sample: 25\n",
      "y: -606\n",
      "yHat: [[-576.8732]]\n",
      "Sample: 81\n",
      "y: -19\n",
      "yHat: [[-12.990358]]\n",
      "Sample: 140\n",
      "y: 264\n",
      "yHat: [[-4.656426]]\n",
      "Sample: 142\n",
      "y: 234\n",
      "yHat: [[-4.0187182]]\n",
      "Sample: 39\n",
      "y: -496\n",
      "yHat: [[-448.012]]\n",
      "Sample: 58\n",
      "y: 281\n",
      "yHat: [[-160.89304]]\n",
      "Sample: 88\n",
      "y: -457\n",
      "yHat: [[-456.1652]]\n",
      "Sample: 70\n",
      "y: -253\n",
      "yHat: [[-5.889807]]\n",
      "Sample: 87\n",
      "y: 267\n",
      "yHat: [[-4.90388]]\n",
      "Sample: 36\n",
      "y: 1\n",
      "yHat: [[-16.15051]]\n",
      "Sample: 21\n",
      "y: -527\n",
      "yHat: [[-517.41656]]\n",
      "Sample: 9\n",
      "y: -389\n",
      "yHat: [[-262.40732]]\n",
      "Sample: 103\n",
      "y: 243\n",
      "yHat: [[-7.709994]]\n",
      "Sample: 67\n",
      "y: 134\n",
      "yHat: [[-4.9767475]]\n",
      "Sample: 117\n",
      "y: -210\n",
      "yHat: [[-3.071092]]\n",
      "Sample: 47\n",
      "y: -169\n",
      "yHat: [[-2.747877]]\n",
      "RMSE on dataset: [[193.04588]]\n",
      "Avg Abs Dev on dataset: [[157.08029]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, dataset in enumerate([training_set, validation_set]):\n",
    "        if i == 0:\n",
    "            print('\\nTraining Set Results')\n",
    "        else:\n",
    "            print('\\nValidation Set Results')\n",
    "        MSE = 0\n",
    "        avgAbsDev = 0\n",
    "        for sample in dataset.ids:\n",
    "            X, y = dataset.__getitem__(sample)\n",
    "            X = X.unsqueeze(0) # Add fake batch dimension\n",
    "            net.eval()\n",
    "            yHat = net(X.float()).numpy()\n",
    "            MSE += (yHat - y)**2\n",
    "            avgAbsDev += np.abs(yHat - y)\n",
    "\n",
    "            print('Sample: ' + str(sample))\n",
    "            print('y: ' + str(y))\n",
    "            print('yHat: ' + str(yHat))\n",
    "        MSE /= len(dataset.ids)\n",
    "        avgAbsDev /= len(dataset.ids)\n",
    "        print('RMSE on dataset: ' + str(np.sqrt(MSE)))\n",
    "        print('Avg Abs Dev on dataset: ' + str(avgAbsDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "plt.figure()\n",
    "plt.plot(range(max_epochs), epoch_training_loss, range(max_epochs), epoch_val_loss)\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in resnet18.state_dict():\n",
    "    print(param_tensor, \"\\t\", resnet18.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
